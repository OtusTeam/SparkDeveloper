[
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\nimport org.apache.log4j.{Level, Logger}\nLogger.getLogger(\"org\").setLevel(Level.OFF)",
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\nimport org.apache.log4j.{Level, Logger}\nLogger.getLogger(\"org\").setLevel(Level.OFF)",
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\nimport org.apache.log4j.{Level, Logger}\nLogger.getLogger(\"org\").setLevel(Level.OFF)",
    "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport java.sql.Timestamp\nimport java.time._",
    "val spark = SparkSession\n                .builder()\n                .master(\"spark://master:7077\")\n                .appName(\"Hello Spark\")\n                .getOrCreate()\n\nimport spark.implicits._",
    "val spark = SparkSession\n                .builder()\n                .master(\"spark://master:7077\")\n                .appName(\"Hello Spark\")\n                .getOrCreate()\n\nimport spark.implicits._",
    "println(s\"spark.version == ${spark.version}\")",
    "val df = List(\n      (1L, 2.0, \"string1\", LocalDate.of(2000, 1, 1), Timestamp.valueOf(\"2023-01-01 12:00:00\")),\n      (2L, 3.0, \"string2\", LocalDate.of(2000, 2, 1), Timestamp.valueOf(\"2023-02-01 12:00:00\")),\n      (3L, 4.0, \"string3\", LocalDate.of(2000, 3, 1), Timestamp.valueOf(\"2023-03-01 12:00:00\"))\n    ).toDF(\"a\", \"b\", \"c\", \"d\", \"e\")",
    "df.printSchema()",
    "df.show()",
    "df.withColumn(\"upper_c\", upper($\"c\")).show()",
    "df.select(col(\"c\")).show()",
    "df.filter($\"a\" === 1L).show()",
    "val df0 = List((1L, \"100000\"), (2L, \"2000\"), (3L, \"3000\")).toDF(\"id\", \"exampleCol\")\nval df = df0.select($\"id\", col(\"exampleCol\").cast(\"decimal\").alias(\"exampleColDecimal\"))",
    "df.printSchema",
    "df.filter($\"exampleCol\" === 100000).show",
    "df.filter(col(\"exampleCol\") === 100000).show",
    "df.filter(col(\"exampleCol\") === 100000).explain(\"extended\")",
    "val df2 = df0.withColumn(\"exampleColDecimal\", col(\"exampleCol\").cast(\"decimal\")).drop(\"exampleCol\")",
    "df2.printSchema",
    "df2.filter($\"exampleCol\" === 100000).explain(\"extended\")",
    "val people = List((1, \"Vasya\"), (2, \"Petya\")).toDF(\"id\", \"name\")",
    "people.printSchema",
    "people.show()",
    "people.select(\"name\").show()",
    "people.filter($\"id\" > 1).show()",
    "people.createOrReplaceTempView(\"people\")",
    "spark.sql(\"SELECT * FROM people\").show()",
    "val sqlDF1 = spark.sql(\"SELECT * FROM people\")",
    "sqlDF1.show()",
    "val sqlDF2 = spark.sql(\"SELECT * FROM people where id='2'\")",
    "sqlDF2.show()"
]