{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6568ca62-c9d6-4893-96b5-9e8025fa9147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.5`\n",
    "import $ivy.`org.postgresql:postgresql:42.7.2`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a25d85-2ecc-41a9-877c-ba49f3056892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866438e",
   "metadata": {},
   "source": [
    "Создаём SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e0ef85-6403-40fe-99c3-a308d9699219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/13 16:48:13 INFO SparkContext: Running Spark version 3.5.5\n",
      "25/03/13 16:48:13 INFO SparkContext: OS info Mac OS X, 15.3.2, aarch64\n",
      "25/03/13 16:48:13 INFO SparkContext: Java version 11.0.26\n",
      "25/03/13 16:48:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"WARN\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@56fdc2be\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"JDBC Data Source\")\n",
    "                .config(\"spark.driver.memory\", \"8g\")\n",
    "                .config(\"spark.log.level\", \"WARN\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4fcab",
   "metadata": {},
   "source": [
    "Задаём свойства подключения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe90f98-4664-4c71-8774-680c686bb180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriver\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"org.postgresql.Driver\"\u001b[39m\n",
       "\u001b[36murl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"jdbc:postgresql://localhost:5432/spark\"\u001b[39m\n",
       "\u001b[36muser\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"postgres\"\u001b[39m\n",
       "\u001b[36mpassword\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"postgres\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driver = \"org.postgresql.Driver\"\n",
    "val url = \"jdbc:postgresql://localhost:5432/spark\"\n",
    "val user = \"postgres\"\n",
    "val password = \"postgres\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64f6a1",
   "metadata": {},
   "source": [
    "## Чтение таблицы целиком"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8566ec",
   "metadata": {},
   "source": [
    "### Вариант 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9495e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36memployees_df\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]\n",
       "\u001b[36mres5_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m30003L\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees_df = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees\")\n",
    "    .load()\n",
    "\n",
    "employees_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a45443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11de3968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "| 10030|1958-07-14|     Elvis|    Demeyer|     M|1994-02-17|\n",
      "| 10040|1959-09-13|     Weiyi|    Meriste|     F|1993-02-14|\n",
      "| 10050|1958-05-21|   Yinghua|     Dredge|     M|1990-12-25|\n",
      "| 10060|1961-10-15|  Breannda|Billingsley|     M|1987-11-02|\n",
      "| 10070|1955-08-20|    Reuven| Garigliano|     M|1985-10-14|\n",
      "| 10080|1957-12-03|    Premal|       Baek|     M|1985-11-19|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "| 10100|1953-04-21|  Hironobu|  Haraldson|     F|1987-09-21|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c89bc1",
   "metadata": {},
   "source": [
    "### Вариант 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef276680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\u001b[39m\n",
       "\u001b[36mconnectionProperties\u001b[39m: \u001b[32mProperties\u001b[39m = {password=postgres, user=postgres}\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m\n",
       "\u001b[36mres8_3\u001b[39m: \u001b[32mObject\u001b[39m = \u001b[32mnull\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Properties\n",
    "\n",
    "val connectionProperties = new Properties()\n",
    "connectionProperties.put(\"user\", user)\n",
    "connectionProperties.put(\"password\", password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4711cbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]\n",
       "\u001b[36mres9_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m30003L\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.jdbc(url, \"public.employees\", connectionProperties)\n",
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b39509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9db107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "| 10030|1958-07-14|     Elvis|    Demeyer|     M|1994-02-17|\n",
      "| 10040|1959-09-13|     Weiyi|    Meriste|     F|1993-02-14|\n",
      "| 10050|1958-05-21|   Yinghua|     Dredge|     M|1990-12-25|\n",
      "| 10060|1961-10-15|  Breannda|Billingsley|     M|1987-11-02|\n",
      "| 10070|1955-08-20|    Reuven| Garigliano|     M|1985-10-14|\n",
      "| 10080|1957-12-03|    Premal|       Baek|     M|1985-11-19|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "| 10100|1953-04-21|  Hironobu|  Haraldson|     F|1987-09-21|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b7406",
   "metadata": {},
   "source": [
    "Проверим количество партиций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e51f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476cedd",
   "metadata": {},
   "source": [
    "## Как распараллелить чтение?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2b635-d31f-4902-b88b-9b4b594e3781",
   "metadata": {},
   "source": [
    "### Партиционирование по столбцам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3cbe3",
   "metadata": {},
   "source": [
    "Добавим количество партиций к параметрам чтения таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc55fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 30003\n",
      "num partitions = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf101\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df101 = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees\")\n",
    "    .option(\"numPartitions\", \"10\")\n",
    "    .load()\n",
    "\n",
    "println(s\"count = ${df101.count}\")\n",
    "println(s\"num partitions = ${df101.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d847e01",
   "metadata": {},
   "source": [
    "Количество партиций не изменилось."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd6535",
   "metadata": {},
   "source": [
    "Узнаем минимальное и максимальное значения столбца *emp_no*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0404d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(emp_no)|max(emp_no)|\n",
      "+-----------+-----------+\n",
      "|      10010|     499990|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(min(col(\"emp_no\")), max(col(\"emp_no\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7081ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 10010\n",
      "max = 499990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmin_emp_no\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"10010\"\u001b[39m\n",
       "\u001b[36mmax_emp_no\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"499990\"\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val min_emp_no = df.agg(min(col(\"emp_no\"))).collect()(0)(0).toString\n",
    "val max_emp_no = df.agg(max(col(\"emp_no\"))).collect()(0)(0).toString\n",
    "\n",
    "println(s\"min = $min_emp_no\\nmax = $max_emp_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a44c76-8669-43ca-a7f3-044edd1f26db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf102\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df102 = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees\")\n",
    "    .option(\"partitionColumn\", \"emp_no\")\n",
    "    .option(\"lowerBound\", min_emp_no)\n",
    "    .option(\"upperBound\", max_emp_no)\n",
    "    .option(\"numPartitions\", \"10\")\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c891d7-9e95-428c-bc43-1f50023c0915",
   "metadata": {},
   "source": [
    "Number of partitions: 10\n",
    "\n",
    "WHERE clauses of these partitions: \n",
    "- \"emp_no\" < 59008 or \"emp_no\" is null\n",
    "- \"emp_no\" >= 59008 AND \"emp_no\" < 108006\n",
    "- \"emp_no\" >= 108006 AND \"emp_no\" < 157004\n",
    "- \"emp_no\" >= 157004 AND \"emp_no\" < 206002\n",
    "- \"emp_no\" >= 206002 AND \"emp_no\" < 255000\n",
    "- \"emp_no\" >= 255000 AND \"emp_no\" < 303998\n",
    "- \"emp_no\" >= 303998 AND \"emp_no\" < 352996\n",
    "- \"emp_no\" >= 352996 AND \"emp_no\" < 401994\n",
    "- \"emp_no\" >= 401994 AND \"emp_no\" < 450992\n",
    "- \"emp_no\" >= 450992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d925c6c-14e6-4236-b1ef-117dc286a987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 30003\n",
      "num partitions = 10\n"
     ]
    }
   ],
   "source": [
    "println(s\"count = ${df102.count()}\\nnum partitions = ${df102.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b86cd66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "| 10030|1958-07-14|     Elvis|    Demeyer|     M|1994-02-17|\n",
      "| 10040|1959-09-13|     Weiyi|    Meriste|     F|1993-02-14|\n",
      "| 10050|1958-05-21|   Yinghua|     Dredge|     M|1990-12-25|\n",
      "| 10060|1961-10-15|  Breannda|Billingsley|     M|1987-11-02|\n",
      "| 10070|1955-08-20|    Reuven| Garigliano|     M|1985-10-14|\n",
      "| 10080|1957-12-03|    Premal|       Baek|     M|1985-11-19|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "| 10100|1953-04-21|  Hironobu|  Haraldson|     F|1987-09-21|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df102.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdbbd4",
   "metadata": {},
   "source": [
    "Посмотрим сколько записей попало в каждую партицию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count = 0\n",
      "Partition count = 203\n",
      "Partition count = 200\n",
      "Partition count = 601\n",
      "Partition count = 4500\n",
      "Partition count = 4900\n",
      "Partition count = 4900\n",
      "Partition count = 4899\n",
      "Partition count = 4900\n",
      "Partition count = 4900\n"
     ]
    }
   ],
   "source": [
    "df102.rdd.foreachPartition(p => println(s\"Partition count = ${p.length}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c138b0",
   "metadata": {},
   "source": [
    "Это также можно получить другим способом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81f8bd49-d668-476a-a9b5-3861cca86ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m4900\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m4900\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m203\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m601\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m4899\u001b[39m),\n",
       "  (\u001b[32m5\u001b[39m, \u001b[32m4500\u001b[39m),\n",
       "  (\u001b[32m6\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m7\u001b[39m, \u001b[32m200\u001b[39m),\n",
       "  (\u001b[32m8\u001b[39m, \u001b[32m4900\u001b[39m),\n",
       "  (\u001b[32m9\u001b[39m, \u001b[32m4900\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df102.rdd.mapPartitionsWithIndex{(p,i) => List((p, i.length)).iterator}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8190f-732f-419c-bf8c-9cfd853500ff",
   "metadata": {},
   "source": [
    "Зададим в качестве *lowerBound* и *upperBound* произвольные значения (не min и max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efcb1c35-2404-4e60-847b-13f28f4797ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf103\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df103 = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees\")\n",
    "    .option(\"partitionColumn\", \"emp_no\")\n",
    "    .option(\"lowerBound\", \"20000\")\n",
    "    .option(\"upperBound\", \"50000\")\n",
    "    .option(\"numPartitions\", \"10\")\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0090f-1776-4727-a8d1-73a7a72764f0",
   "metadata": {},
   "source": [
    "Number of partitions: 10\n",
    "\n",
    "WHERE clauses of these partitions:\n",
    "- \"emp_no\" < 23000 or \"emp_no\" is null\n",
    "- \"emp_no\" >= 23000 AND \"emp_no\" < 26000\n",
    "- \"emp_no\" >= 26000 AND \"emp_no\" < 29000\n",
    "- \"emp_no\" >= 29000 AND \"emp_no\" < 32000\n",
    "- \"emp_no\" >= 32000 AND \"emp_no\" < 35000\n",
    "- \"emp_no\" >= 35000 AND \"emp_no\" < 38000\n",
    "- \"emp_no\" >= 38000 AND \"emp_no\" < 41000\n",
    "- \"emp_no\" >= 41000 AND \"emp_no\" < 44000\n",
    "- \"emp_no\" >= 44000 AND \"emp_no\" < 47000\n",
    "- \"emp_no\" >= 47000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8bfe2",
   "metadata": {},
   "source": [
    "Посмотрим сколько теперь записей попало в каждую партицию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8396c33-6ee1-4052-84af-88389572467e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 30003\n",
      "num partitions = 10\n"
     ]
    }
   ],
   "source": [
    "println(s\"count = ${df103.count()}\\nnum partitions = ${df103.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8363c0e-331f-466c-8871-a2816bdcff7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m1299\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m5\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m6\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m7\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m8\u001b[39m, \u001b[32m300\u001b[39m),\n",
       "  (\u001b[32m9\u001b[39m, \u001b[32m26304\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df103.rdd.mapPartitionsWithIndex{(p,i) => List((p, i.length)).iterator}.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af1dba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "| 10030|1958-07-14|     Elvis|    Demeyer|     M|1994-02-17|\n",
      "| 10040|1959-09-13|     Weiyi|    Meriste|     F|1993-02-14|\n",
      "| 10050|1958-05-21|   Yinghua|     Dredge|     M|1990-12-25|\n",
      "| 10060|1961-10-15|  Breannda|Billingsley|     M|1987-11-02|\n",
      "| 10070|1955-08-20|    Reuven| Garigliano|     M|1985-10-14|\n",
      "| 10080|1957-12-03|    Premal|       Baek|     M|1985-11-19|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "| 10100|1953-04-21|  Hironobu|  Haraldson|     F|1987-09-21|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df103.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e7118",
   "metadata": {},
   "source": [
    "### Партиционирование по предикатам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eead0",
   "metadata": {},
   "source": [
    "### Пример 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac468144",
   "metadata": {},
   "source": [
    "Опредилим **два** предиката по значению столбца *gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dde1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 30003\n",
      "num partitions = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"gender = 'M'\"\u001b[39m, \u001b[32m\"gender = 'F'\"\u001b[39m)\n",
       "\u001b[36mdf_pred\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates = Array(\"gender = 'M'\", \"gender = 'F'\")\n",
    "\n",
    "val df_pred = spark.read.jdbc(url, \"public.employees\", predicates, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred.count}\")\n",
    "println(s\"num partitions = ${df_pred.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a37b1",
   "metadata": {},
   "source": [
    "Опредилим **один** предиката по одному значению столбца *gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c871cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 12091\n",
      "num partitions = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"gender = 'F'\"\u001b[39m)\n",
       "\u001b[36mdf_pred1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates1 = Array(\"gender = 'F'\")\n",
    "\n",
    "val df_pred1 = spark.read.jdbc(url, \"public.employees\", predicates1, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred1.count}\")\n",
    "println(s\"num partitions = ${df_pred1.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b33cb4",
   "metadata": {},
   "source": [
    "Опредилим **три** предиката по значению столбца *gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cb58f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 47915\n",
      "num partitions = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"gender = 'M'\"\u001b[39m,\n",
       "  \u001b[32m\"gender = 'F'\"\u001b[39m,\n",
       "  \u001b[32m\"gender = 'M'\"\u001b[39m\n",
       ")\n",
       "\u001b[36mdf_pred3\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates3 = Array(\"gender = 'M'\", \"gender = 'F'\", \"gender = 'M'\")\n",
    "\n",
    "val df_pred3 = spark.read.jdbc(url, \"public.employees\", predicates3, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred3.count}\")\n",
    "println(s\"num partitions = ${df_pred3.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dcd52",
   "metadata": {},
   "source": [
    "Опредилим **четыре** предиката по значению столбца *gender*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0465c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 60006\n",
      "num partitions = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates4\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"gender = 'M'\"\u001b[39m,\n",
       "  \u001b[32m\"gender = 'F'\"\u001b[39m,\n",
       "  \u001b[32m\"gender = 'M'\"\u001b[39m,\n",
       "  \u001b[32m\"gender = 'F'\"\u001b[39m\n",
       ")\n",
       "\u001b[36mdf_pred4\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates4 = Array(\"gender = 'M'\", \"gender = 'F'\", \"gender = 'M'\", \"gender = 'F'\")\n",
    "\n",
    "val df_pred4 = spark.read.jdbc(url, \"public.employees\", predicates4, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred4.count}\")\n",
    "println(s\"num partitions = ${df_pred4.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31474c51",
   "metadata": {},
   "source": [
    "Посмотрим сколько записей для каждого значения столбца *gender* было в исходной таблице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbad35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|count(emp_no)|\n",
      "+------+-------------+\n",
      "|     F|        12091|\n",
      "|     M|        17912|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"gender\")).agg(count(col(\"emp_no\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6087b5",
   "metadata": {},
   "source": [
    "Сравним с количеством записей при применении трёх и четырёх предикатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4fb8447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|count(emp_no)|\n",
      "+------+-------------+\n",
      "|     M|        35824|\n",
      "|     F|        12091|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred3.groupBy(col(\"gender\")).agg(count(col(\"emp_no\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04174cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|gender|count(emp_no)|\n",
      "+------+-------------+\n",
      "|     M|        35824|\n",
      "|     F|        24182|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred4.groupBy(col(\"gender\")).agg(count(col(\"emp_no\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2feda7",
   "metadata": {},
   "source": [
    "### Пример 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba03ed8",
   "metadata": {},
   "source": [
    "Определим **два** предиката по условиям на значения столбца *emp_no* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba552cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 8001\n",
      "num partitions = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"emp_no > 20000 and emp_no <= 50000\"\u001b[39m,\n",
       "  \u001b[32m\"emp_no >= 50000 and emp_no <= 100000\"\u001b[39m\n",
       ")\n",
       "\u001b[36mdf_pred2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates2 = Array(\"emp_no > 20000 and emp_no <= 50000\", \"emp_no >= 50000 and emp_no <= 100000\")\n",
    "\n",
    "val df_pred2 = spark.read.jdbc(url, \"public.employees\", predicates2, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred2.count}\")\n",
    "println(s\"num partitions = ${df_pred2.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "144f4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 20010|1961-01-26|    Saniya|     Veccia|     M|1997-06-16|\n",
      "| 20020|1962-07-22|     Akeel|     Covnot|     F|1996-03-02|\n",
      "| 20030|1962-05-09|    Nitsan|Hoppenstand|     F|1988-11-18|\n",
      "| 20040|1962-04-16|   Youjian|    Vingron|     M|1987-01-21|\n",
      "| 20050|1959-02-27|  Guoxiang|   Greibach|     F|1991-03-18|\n",
      "| 20060|1954-07-18|    Chrisa|Attimonelli|     F|1985-10-16|\n",
      "| 20070|1959-04-13|       Tse|    Bellone|     M|1992-07-08|\n",
      "| 20080|1962-03-22|   Odoardo|  Heiserman|     F|1991-07-01|\n",
      "| 20090|1957-05-20| Serenella|   Kaltofen|     F|1986-06-24|\n",
      "| 20100|1965-01-18|       Utz|     Heuter|     F|1986-06-15|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4e46d",
   "metadata": {},
   "source": [
    "Определим **один** предикат по условию на значения столбца *emp_no* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b4ffbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 3000\n",
      "num partitions = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredicates22\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"emp_no > 20000 and emp_no <= 50000\"\u001b[39m)\n",
       "\u001b[36mdf_pred22\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates22 = Array(\"emp_no > 20000 and emp_no <= 50000\")\n",
    "\n",
    "val df_pred22 = spark.read.jdbc(url, \"public.employees\", predicates22, connectionProperties)\n",
    "\n",
    "println(s\"count = ${df_pred22.count}\")\n",
    "println(s\"num partitions = ${df_pred22.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26008249",
   "metadata": {},
   "source": [
    "## Фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b94fa",
   "metadata": {},
   "source": [
    "Выполним запрос к базе на выборку значений из таблицы с условием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81bf6a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mq\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"select * from public.employees where emp_no > 20000 and emp_no <= 50000\"\u001b[39m\n",
       "\u001b[36mdfq\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 4 more fields]\n",
       "\u001b[36mres35_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3000L\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q = \"\"\"select * from public.employees where emp_no > 20000 and emp_no <= 50000\"\"\"\n",
    "\n",
    "val dfq = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"query\", q)\n",
    "    .load()\n",
    "\n",
    "dfq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c34b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 20010|1961-01-26|    Saniya|     Veccia|     M|1997-06-16|\n",
      "| 20020|1962-07-22|     Akeel|     Covnot|     F|1996-03-02|\n",
      "| 20030|1962-05-09|    Nitsan|Hoppenstand|     F|1988-11-18|\n",
      "| 20040|1962-04-16|   Youjian|    Vingron|     M|1987-01-21|\n",
      "| 20050|1959-02-27|  Guoxiang|   Greibach|     F|1991-03-18|\n",
      "| 20060|1954-07-18|    Chrisa|Attimonelli|     F|1985-10-16|\n",
      "| 20070|1959-04-13|       Tse|    Bellone|     M|1992-07-08|\n",
      "| 20080|1962-03-22|   Odoardo|  Heiserman|     F|1991-07-01|\n",
      "| 20090|1957-05-20| Serenella|   Kaltofen|     F|1986-06-24|\n",
      "| 20100|1965-01-18|       Utz|     Heuter|     F|1986-06-15|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfq.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b74c",
   "metadata": {},
   "source": [
    "## Соединения в базе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978db1e",
   "metadata": {},
   "source": [
    "Выполним запрос к базе на выборку значений из соединения таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e47cfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mqj\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"select e.emp_no, birth_date, first_name, last_name, gender, hire_date, salary, from_date, to_date from employees e join salaries s on e.emp_no = s.emp_no\"\u001b[39m\n",
       "\u001b[36mdfj\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 7 more fields]\n",
       "\u001b[36mres37_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m283827L\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val qj = \"\"\"select e.emp_no, birth_date, first_name, last_name, gender, hire_date, salary, from_date, to_date from employees e join salaries s on e.emp_no = s.emp_no\"\"\"\n",
    "\n",
    "val dfj = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"query\", qj)\n",
    "    .load()\n",
    "\n",
    "dfj.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b170af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|salary| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+----------+----------+\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 72488|1996-11-24|1997-11-24|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 74347|1997-11-24|1998-11-24|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 75405|1998-11-24|1999-11-24|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 78194|1999-11-24|2000-11-23|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 79580|2000-11-23|2001-11-23|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24| 80324|2001-11-23|9999-01-01|\n",
      "| 10020|1952-12-24|    Mayuko|  Warwick|     M|1991-01-26| 40000|1997-12-30|1998-12-30|\n",
      "| 10020|1952-12-24|    Mayuko|  Warwick|     M|1991-01-26| 40647|1998-12-30|1999-12-30|\n",
      "| 10020|1952-12-24|    Mayuko|  Warwick|     M|1991-01-26| 43800|1999-12-30|2000-12-29|\n",
      "| 10020|1952-12-24|    Mayuko|  Warwick|     M|1991-01-26| 44927|2000-12-29|2001-12-29|\n",
      "| 10020|1952-12-24|    Mayuko|  Warwick|     M|1991-01-26| 47017|2001-12-29|9999-01-01|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 66956|1994-02-17|1995-02-17|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 68393|1995-02-17|1996-02-17|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 72795|1996-02-17|1997-02-16|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 76453|1997-02-16|1998-02-16|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 79290|1998-02-16|1999-02-16|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 83327|1999-02-16|2000-02-16|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 86634|2000-02-16|2001-02-15|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 87027|2001-02-15|2002-02-15|\n",
      "| 10030|1958-07-14|     Elvis|  Demeyer|     M|1994-02-17| 88806|2002-02-15|9999-01-01|\n",
      "+------+----------+----------+---------+------+----------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc09ee3",
   "metadata": {},
   "source": [
    "## Запись в таблицу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5cd8a",
   "metadata": {},
   "source": [
    "Посмотрим на таблицу *employees*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71a4818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "| 10030|1958-07-14|     Elvis|    Demeyer|     M|1994-02-17|\n",
      "| 10040|1959-09-13|     Weiyi|    Meriste|     F|1993-02-14|\n",
      "| 10050|1958-05-21|   Yinghua|     Dredge|     M|1990-12-25|\n",
      "| 10060|1961-10-15|  Breannda|Billingsley|     M|1987-11-02|\n",
      "| 10070|1955-08-20|    Reuven| Garigliano|     M|1985-10-14|\n",
      "| 10080|1957-12-03|    Premal|       Baek|     M|1985-11-19|\n",
      "| 10090|1961-05-30|    Kendra|    Hofting|     M|1986-03-14|\n",
      "| 10100|1953-04-21|  Hironobu|  Haraldson|     F|1987-09-21|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdfab9",
   "metadata": {},
   "source": [
    "Загрузим таблицу *salaries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12f51761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msalaries_df\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, salary: int ... 2 more fields]\n",
       "\u001b[36mres40_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m283827L\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salaries_df = spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.salaries\")\n",
    "    .load()\n",
    "\n",
    "salaries_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdb3bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10010| 72488|1996-11-24|1997-11-24|\n",
      "| 10010| 74347|1997-11-24|1998-11-24|\n",
      "| 10010| 75405|1998-11-24|1999-11-24|\n",
      "| 10010| 78194|1999-11-24|2000-11-23|\n",
      "| 10010| 79580|2000-11-23|2001-11-23|\n",
      "| 10010| 80324|2001-11-23|9999-01-01|\n",
      "| 10020| 40000|1997-12-30|1998-12-30|\n",
      "| 10020| 40647|1998-12-30|1999-12-30|\n",
      "| 10020| 43800|1999-12-30|2000-12-29|\n",
      "| 10020| 44927|2000-12-29|2001-12-29|\n",
      "+------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaries_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629595c",
   "metadata": {},
   "source": [
    "Сделаем группировку по колонке *emp_no* и найдём максимальное значение колонки *salary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5275c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|emp_no|max_salary|\n",
      "+------+----------+\n",
      "| 12940|     85425|\n",
      "| 13840|     41453|\n",
      "| 14450|     75524|\n",
      "| 14570|     72506|\n",
      "| 15790|     79009|\n",
      "| 17420|     97003|\n",
      "| 18800|     88342|\n",
      "| 19530|     47864|\n",
      "| 21220|     86177|\n",
      "| 21700|     68115|\n",
      "+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msalaries_grouped\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, max_salary: int]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salaries_grouped = salaries_df.groupBy(col(\"emp_no\")).agg(max(col(\"salary\")).alias(\"max_salary\"))\n",
    "\n",
    "salaries_grouped.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104e41a",
   "metadata": {},
   "source": [
    "Создадим новый Dataframe как результат соединения *employees_df* и агрегированного *salaries_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e22ecee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|max_salary|\n",
      "+------+----------+----------+---------+------+----------+----------+\n",
      "| 12940|1953-10-25|  Odinaldo|   Farrar|     F|1987-12-12|     85425|\n",
      "| 13840|1954-11-13|     Remco|    Demke|     M|1992-06-09|     41453|\n",
      "| 14450|1963-08-01|  Fumitaka|Prochazka|     F|1985-04-26|     75524|\n",
      "| 14570|1963-07-26|    Chinho|     Bala|     F|1994-11-17|     72506|\n",
      "| 15790|1960-03-20|     Kokou| Schnabel|     M|1991-04-03|     79009|\n",
      "| 17420|1964-05-22|     Jinpo|Stamatiou|     F|1987-07-31|     97003|\n",
      "| 18800|1962-05-21|    Baruch|  Rosiles|     F|1990-07-06|     88342|\n",
      "| 19530|1953-12-26|     Barry|   Dratva|     M|1997-09-02|     47864|\n",
      "| 21220|1956-05-11|      Mana|  Murtagh|     M|1991-10-29|     86177|\n",
      "| 21700|1963-07-12|       Urs|  Plesums|     F|1992-03-07|     68115|\n",
      "| 27760|1959-12-05|    Zhigen|  Schrift|     M|1991-04-03|     63106|\n",
      "| 28170|1964-05-19|     Jiong|  Bamford|     M|1997-05-03|     56862|\n",
      "| 30970|1956-02-16|  Fumitake| Schoegge|     F|1986-12-01|     69833|\n",
      "| 32460|1957-10-08|   Kousuke|    Rotem|     M|1987-10-17|     73387|\n",
      "| 35820|1953-02-26|  Michaela|   Brizzi|     F|1990-07-22|     87087|\n",
      "| 38220|1952-07-05|      Maya|   Terkki|     M|1986-08-31|     79664|\n",
      "| 41890|1958-05-19|      Kirk|   Pettit|     M|1995-12-02|     58692|\n",
      "| 48510|1959-09-12|      Shih| Savasere|     F|1987-11-20|     59676|\n",
      "| 54190|1960-08-01|    Doowon|   Ashish|     F|1990-12-24|     72968|\n",
      "| 56110|1953-06-13|     Jouni|   Reeken|     F|1988-08-16|     47841|\n",
      "+------+----------+----------+---------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36memployees_salaries_df\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_no: int, birth_date: date ... 5 more fields]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees_salaries_df = employees_df.join(salaries_grouped, \"emp_no\")\n",
    "\n",
    "employees_salaries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfce94",
   "metadata": {},
   "source": [
    "Сохраним новый Dataframe в таблицу в базе. Таблицы с таким именем в базе не было. Она будет создана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72740355",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_salaries_df.write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees_salaries\")\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44283bc9",
   "metadata": {},
   "source": [
    "Если таблица с таким именем существовала в базе, то при сохранении надо использовать режим *overwrite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9dca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_salaries_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees_salaries\")\n",
    "    .option(\"truncate\", \"true\")\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d80221a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres46\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m30003L\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees_salaries\")\n",
    "    .load()\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99ea46",
   "metadata": {},
   "source": [
    "Если использовать режим *append* содержимое Dataframe будет добавлено в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78f6bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_salaries_df.write\n",
    "    .mode(\"append\")\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees_salaries\")\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b9111c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres48\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m60006L\u001b[39m"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"url\", url)\n",
    "    .option(\"user\", user)\n",
    "    .option(\"password\", password)\n",
    "    .option(\"dbtable\", \"public.employees_salaries\")\n",
    "    .load()\n",
    "    .count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
