{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d22977-ebe9-44f2-be27-e847b10bd2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c205ccf0-1f60-499b-86bf-b0f1a5352037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92a7ca3-7597-4737-9d72-a050430d0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fc6b17-f21f-48a8-a9ae-249ff2e7d789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/03/13 16:28:22 INFO SparkContext: Running Spark version 3.5.5\n",
      "25/03/13 16:28:22 INFO SparkContext: OS info Mac OS X, 15.3.2, aarch64\n",
      "25/03/13 16:28:22 INFO SparkContext: Java version 11.0.26\n",
      "25/03/13 16:28:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"WARN\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7ec03e09\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Spark SQL\")\n",
    "                .config(\"spark.log.level\", \"WARN\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d77452-a4f3-4795-a898-211247296a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.5\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5567071e-689f-4e21-b0bd-bef018305386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDF = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "customerDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0ee3cc-f323-4c82-bd63-995e29f45550",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0972a67-b860-4b7d-9ba9-ee8a629bca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Birthdate: string, Country: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, Country from customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078f44ea-b558-4745-a5a4-55d1a72adb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|Birthdate          |Country       |\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|Iceland       |\n",
      "|1974-11-26 15:30:20|Finland       |\n",
      "|1977-05-06 23:57:35|Italy         |\n",
      "|1996-09-13 19:14:27|Norway        |\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, Country from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f82b4d-3432-47ab-831b-6144aae62bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|Date               |Country       |\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|Iceland       |\n",
      "|1974-11-26 15:30:20|Finland       |\n",
      "|1977-05-06 23:57:35|Italy         |\n",
      "|1996-09-13 19:14:27|Norway        |\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4fa6db-bd77-4f41-b7d9-47c5b7298a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----+\n",
      "|Date               |Country       |Flag|\n",
      "+-------------------+--------------+----+\n",
      "|1994-02-20 00:46:27|United Kingdom|true|\n",
      "|1988-06-21 00:15:34|Iceland       |true|\n",
      "|1974-11-26 15:30:20|Finland       |true|\n",
      "|1977-05-06 23:57:35|Italy         |true|\n",
      "|1996-09-13 19:14:27|Norway        |true|\n",
      "+-------------------+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country, true as Flag from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa4178a-4f61-4403-acbb-5b33f7806c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Flag: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country, true as Flag from customer\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a665459b-edb1-433f-a231-92e7dadb44a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "|Address                                           |Birthdate          |Country|CustomerID|Email                    |Name          |Username       |\n",
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017            |1996-09-13 19:14:27|Norway |12350     |amyholland@yahoo.com     |Natalie Ford  |gregoryharrison|\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106|1969-06-21 03:39:20|Norway |12352     |vcarter@hotmail.com      |Dana Clarke   |hmyers         |\n",
      "|0297 Jacob Ranch Apt. 019\\nNorth Judith, NV 27455 |1990-06-01 14:49:52|Norway |12381     |douglaschavez@hotmail.com|Matthew Jones |stephanie68    |\n",
      "|3102 Hopkins Walk\\nAndrechester, MD 54461         |1976-06-19 08:10:24|Norway |12430     |crystalromero@hotmail.com|Lisa Tate     |pgilbert       |\n",
      "|637 Philip Lock Suite 286\\nJohnsmouth, RI 96778   |1984-06-06 09:36:14|Norway |12432     |jessica87@hotmail.com    |Cheryl Herring|mathewsnicholas|\n",
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer where Country = 'Norway'\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2d62a0-3279-4c3a-8ef9-84e2ba1edf4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                           |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                  |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                  |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165         |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017            |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106|1969-06-21 03:39:20|Norway        |12352     |vcarter@hotmail.com     |Dana Clarke    |hmyers          |\n",
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer where Country <> 'Iceland'\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1009b655-c60c-4ab1-a45a-c4061edbcbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "|Address                                             |Birthdate          |Country       |CustomerID|Email                  |Name                |Username         |\n",
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "|6942 Connie Skyway\\nPatrickville, WA 16551          |1973-10-24 00:52:10|United Kingdom|12989     |amber97@hotmail.com    |Brandon Contreras   |ecasey           |\n",
      "|79375 David Neck\\nWest Matthewton, NJ 92863         |1971-05-04 22:20:10|United Kingdom|12988     |erica98@gmail.com      |Gabriel Romero      |qknight          |\n",
      "|00881 West Flat\\nNorth Emily, IL 32130              |1997-03-05 19:20:57|United Kingdom|12987     |vkeith@yahoo.com       |Christopher Lawrence|smcintyre        |\n",
      "|499 Jonathan Streets Apt. 890\\nEast Ashley, MD 76825|1987-10-24 20:05:15|United Kingdom|12985     |fredsmith@yahoo.com    |Xavier Myers        |stricklandjeffery|\n",
      "|9505 Melissa Streets\\nSouth Frankville, NJ 91189    |1975-09-22 15:21:58|United Kingdom|12984     |scottjonathan@yahoo.com|Brandy Huang        |amandawilliams   |\n",
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer order by CustomerID desc\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892882e0-4621-4c37-aeff-0ba555e03efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|Birthdate          |Date      |\n",
      "+-------------------+----------+\n",
      "|1994-02-20 00:46:27|1994-02-20|\n",
      "|1988-06-21 00:15:34|1988-06-21|\n",
      "|1974-11-26 15:30:20|1974-11-26|\n",
      "|1977-05-06 23:57:35|1977-05-06|\n",
      "|1996-09-13 19:14:27|1996-09-13|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, date_format(Birthdate, 'yyyy-MM-dd') as Date from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76529542-c892-4968-b75c-0dc0da799bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------------------+\n",
      "|Name           |Username        |Identity                         |\n",
      "+---------------+----------------+---------------------------------+\n",
      "|Lindsay Cowan  |valenciajennifer|[Lindsay Cowan, valenciajennifer]|\n",
      "|Katherine David|hillrachel      |[Katherine David, hillrachel]    |\n",
      "|Leslie Martinez|serranobrian    |[Leslie Martinez, serranobrian]  |\n",
      "|Brad Cardenas  |charleshudson   |[Brad Cardenas, charleshudson]   |\n",
      "|Natalie Ford   |gregoryharrison |[Natalie Ford, gregoryharrison]  |\n",
      "+---------------+----------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Name, Username, array(Name, Username) as Identity from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a354ca-f39f-4aba-8138-081597ebe1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Country       |count|\n",
      "+--------------+-----+\n",
      "|United Kingdom|124  |\n",
      "|Germany       |86   |\n",
      "|France        |86   |\n",
      "|Spain         |30   |\n",
      "|Belgium       |25   |\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Country, count(*) as count from customer group by Country order by count desc\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8d5c3b-05a9-4cb9-97ed-85fdf66d8f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+---------------+\n",
      "|Country  |bd        |min(CustomerID)|max(CustomerID)|\n",
      "+---------+----------+---------------+---------------+\n",
      "|Australia|1966-09-17|12422          |12422          |\n",
      "|Australia|1967-11-29|12424          |12424          |\n",
      "|Australia|1985-12-30|12386          |12386          |\n",
      "|Australia|1986-08-28|12434          |12434          |\n",
      "|Australia|1987-02-26|12393          |12393          |\n",
      "+---------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "  Country\n",
    "  , date_format(Birthdate, 'yyyy-MM-dd') as bd\n",
    "  , min(CustomerID)\n",
    "  , max(CustomerID)\n",
    "from customer\n",
    "group by Country, bd\n",
    "order by Country\"\"\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f30413-3569-4183-b47f-74878e88c7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     507|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from customer\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9600af-8959-4b6b-9943-034aa5e3d2f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1014|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from (\n",
    "  select * from customer\n",
    "  union all\n",
    "  select * from customer\n",
    ")\"\"\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03ceb9c-63d7-4c02-932d-efbd1073edd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mretailDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Description: string ... 5 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retailDF = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "693813b9-7df9-4c0b-a4d8-a2a04080e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailDF.createOrReplaceTempView(\"retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89952bc5-8dee-4caf-ac5f-3798dca1dd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 12 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jDF = spark.sql(\"\"\"\n",
    "select c.*, r.*\n",
    "from customer c\n",
    "join retail r\n",
    "on c.CustomerID == r.CustomerID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2eecf71-fdc1-41b4-b452-22f0b54bbe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae63d3f0-b6e2-4ae2-8381-7f03667aec0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |CustomerID|Description                      |InvoiceDate    |InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|12346     |MEDIUM CERAMIC TOP STORAGE JAR   |1/18/2011 10:01|541431   |74215   |23166    |1.04     |\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|12346     |MEDIUM CERAMIC TOP STORAGE JAR   |1/18/2011 10:17|C541433  |-74215  |23166    |1.04     |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |BLACK CANDELABRA T-LIGHT HOLDER  |12/7/2010 14:57|537626   |12      |85116    |2.1      |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |AIRLINE BAG VINTAGE JET SET BROWN|12/7/2010 14:57|537626   |4       |22375    |4.25     |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |COLOUR GLASS. STAR T-LIGHT HOLDER|12/7/2010 14:57|537626   |12      |71477    |3.25     |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf6c161-de26-4439-92d7-c7274e6846f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `CustomerID` is ambiguous, could be: [`c`.`CustomerID`, `r`.`CustomerID`].",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `CustomerID` is ambiguous, could be: [`c`.`CustomerID`, `r`.`CustomerID`].\u001b[39m",
      "  org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(\u001b[32mQueryCompilationErrors.scala\u001b[39m:\u001b[32m1937\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(\u001b[32mpackage.scala\u001b[39m:\u001b[32m377\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m146\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m364\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m157\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.package$.withPosition(\u001b[32mpackage.scala\u001b[39m:\u001b[32m100\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m164\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m135\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m194\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m371\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m357\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.resolveExpressionByPlanChildren(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1520\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.$anonfun$applyOrElse$110(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1658\u001b[39m)",
      "  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m286\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)",
      "  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)",
      "  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m286\u001b[39m)",
      "  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m279\u001b[39m)",
      "  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1658\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1545\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m323\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m134\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m130\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m111\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m110\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1545\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1520\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  scala.collection.LinearSeqOptimized.foldLeft(\u001b[32mLinearSeqOptimized.scala\u001b[39m:\u001b[32m126\u001b[39m)",
      "  scala.collection.LinearSeqOptimized.foldLeft$(\u001b[32mLinearSeqOptimized.scala\u001b[39m:\u001b[32m122\u001b[39m)",
      "  scala.collection.immutable.List.foldLeft(\u001b[32mList.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m211\u001b[39m)",
      "  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m431\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m211\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m240\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m236\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m187\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.execute(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m236\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.execute(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m202\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m182\u001b[39m)",
      "  org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m89\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m182\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m223\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m330\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$.withInternalError(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m546\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.executePhase(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m218\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m74\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m66\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m89\u001b[39m)",
      "  org.apache.spark.sql.Dataset.withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m4352\u001b[39m)",
      "  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1542\u001b[39m)",
      "  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1559\u001b[39m)",
      "  ammonite.$sess.cmd27$Helper.<init>(\u001b[32mcmd27.sc\u001b[39m:\u001b[32m1\u001b[39m)",
      "  ammonite.$sess.cmd27$.<init>(\u001b[32mcmd27.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "  ammonite.$sess.cmd27$.<clinit>(\u001b[32mcmd27.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "jDF.select(\"CustomerID\", \"Name\", \"Birthdate\", \"InvoiceDate\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d2aa62b-4e34-4450-8163-f1ceb7add173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjDF2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Name: string ... 2 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jDF2 = spark.sql(\"\"\"\n",
    "select c.CustomerID, c.Name, c.Birthdate, r.InvoiceDate\n",
    "from customer c\n",
    "join retail r\n",
    "on c.CustomerID == r.CustomerID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d5b6a43-739f-4d7d-a9d1-a16c247164c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+---------------+\n",
      "|CustomerID|Name           |Birthdate          |InvoiceDate    |\n",
      "+----------+---------------+-------------------+---------------+\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:01|\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:17|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "+----------+---------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF2.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ed9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+---------------+\n",
      "|CustomerID|Name           |Birthdate          |InvoiceDate    |\n",
      "+----------+---------------+-------------------+---------------+\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:01|\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:17|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "+----------+---------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF2.select(\"CustomerID\", \"Name\", \"Birthdate\", \"InvoiceDate\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b09568-457c-4a66-a8d5-866e1a627c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
