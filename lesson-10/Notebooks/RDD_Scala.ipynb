{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc99a5fd-2141-4ffe-afb1-4b0a595a75af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.HashSet\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import scala.collection.immutable.HashSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c2d19-92b7-420f-b212-0fe0f37bd5ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/08/29 15:22:54 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/08/29 15:22:54 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/08/29 15:22:54 INFO SparkContext: Java version 11.0.24\n",
      "24/08/29 15:22:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/29 15:22:54 INFO ResourceUtils: ==============================================================\n",
      "24/08/29 15:22:54 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/08/29 15:22:54 INFO ResourceUtils: ==============================================================\n",
      "24/08/29 15:22:54 INFO SparkContext: Submitted application: HelloRDD\n",
      "24/08/29 15:22:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/08/29 15:22:54 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/08/29 15:22:54 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/08/29 15:22:54 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/08/29 15:22:54 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/08/29 15:22:54 INFO SecurityManager: Changing view acls groups to: \n",
      "24/08/29 15:22:54 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/08/29 15:22:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/08/29 15:22:55 INFO Utils: Successfully started service 'sparkDriver' on port 44617.\n",
      "24/08/29 15:22:55 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/08/29 15:22:55 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/08/29 15:22:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/08/29 15:22:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/08/29 15:22:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/08/29 15:22:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a794ca44-52cf-40cc-b48e-9c4e26cbf92a\n",
      "24/08/29 15:22:55 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/08/29 15:22:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/08/29 15:22:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/08/29 15:22:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/08/29 15:22:55 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/08/29 15:22:55 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/08/29 15:22:55 INFO Executor: Java version 11.0.24\n",
      "24/08/29 15:22:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/08/29 15:22:55 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@43b930a7 for default.\n",
      "24/08/29 15:22:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35545.\n",
      "24/08/29 15:22:55 INFO NettyBlockTransferService: Server created on ubuntu:35545\n",
      "24/08/29 15:22:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/08/29 15:22:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 35545, None)\n",
      "24/08/29 15:22:55 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:35545 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 35545, None)\n",
      "24/08/29 15:22:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 35545, None)\n",
      "24/08/29 15:22:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 35545, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@3daee502\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@38477c10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"RDD\").setMaster(\"local[*]\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${sc.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d92288-07f5-4da1-9bd5-ee9025b5632a",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566d2761-a067-4bc6-8296-b1858a5a7fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cell6.sc:1\n",
       "\u001b[36mmappedRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[1] at map at cell6.sc:2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val mappedRDD = rdd.map(_.toUpperCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea27869-6091-4c89-87a0-bcdb480c958c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:57 INFO SparkContext: Starting job: collect at cell7.sc:1\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Got job 0 (collect at cell7.sc:1) with 3 output partitions\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Final stage: ResultStage 0 (collect at cell7.sc:1)\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at cell6.sc:2), which has no missing parents\n",
      "24/08/29 15:22:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:22:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at cell6.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:22:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:22:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:57 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/08/29 15:22:57 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "24/08/29 15:22:57 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "24/08/29 15:22:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver\n",
      "24/08/29 15:22:57 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 904 bytes result sent to driver\n",
      "24/08/29 15:22:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 904 bytes result sent to driver\n",
      "24/08/29 15:22:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 109 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:22:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 136 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:22:57 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 109 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:22:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:22:57 INFO DAGScheduler: ResultStage 0 (collect at cell7.sc:1) finished in 0,300 s\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:22:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/08/29 15:22:57 INFO DAGScheduler: Job 0 finished: collect at cell7.sc:1, took 0,355595 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"A\"\u001b[39m, \u001b[32m\"B\"\u001b[39m, \u001b[32m\"C\"\u001b[39m, \u001b[32m\"D\"\u001b[39m, \u001b[32m\"E\"\u001b[39m, \u001b[32m\"F\"\u001b[39m, \u001b[32m\"X\"\u001b[39m, \u001b[32m\"Y\"\u001b[39m, \u001b[32m\"Z\"\u001b[39m)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappedRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600ccad-960f-4fca-a5ab-abced92098d3",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9838e4a4-96e6-4290-954b-8d265a6f92ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[2] at parallelize at cell8.sc:1\n",
       "\u001b[36mflatmappedRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[3] at flatMap at cell8.sc:2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"some text here\", \"other text over there\", \"and text\", \"short\", \"and a longer one\"), 3)\n",
    "val flatmappedRDD = rdd.flatMap(_.split(\"\\\\s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe379bc-b118-43a9-b2f2-b6b3a7b444c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:58 INFO SparkContext: Starting job: collect at cell9.sc:1\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Got job 1 (collect at cell9.sc:1) with 3 output partitions\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Final stage: ResultStage 1 (collect at cell9.sc:1)\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at flatMap at cell8.sc:2), which has no missing parents\n",
      "24/08/29 15:22:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:22:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at flatMap at cell8.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:22:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:22:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7628 bytes) \n",
      "24/08/29 15:22:58 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7646 bytes) \n",
      "24/08/29 15:22:58 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7638 bytes) \n",
      "24/08/29 15:22:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
      "24/08/29 15:22:58 INFO Executor: Running task 2.0 in stage 1.0 (TID 5)\n",
      "24/08/29 15:22:58 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)\n",
      "24/08/29 15:22:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 870 bytes result sent to driver\n",
      "24/08/29 15:22:58 INFO Executor: Finished task 2.0 in stage 1.0 (TID 5). 882 bytes result sent to driver\n",
      "24/08/29 15:22:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 19 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:22:58 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 18 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:22:58 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 892 bytes result sent to driver\n",
      "24/08/29 15:22:58 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 23 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:22:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:22:58 INFO DAGScheduler: ResultStage 1 (collect at cell9.sc:1) finished in 0,034 s\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:22:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/08/29 15:22:58 INFO DAGScheduler: Job 1 finished: collect at cell9.sc:1, took 0,042530 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"some\"\u001b[39m,\n",
       "  \u001b[32m\"text\"\u001b[39m,\n",
       "  \u001b[32m\"here\"\u001b[39m,\n",
       "  \u001b[32m\"other\"\u001b[39m,\n",
       "  \u001b[32m\"text\"\u001b[39m,\n",
       "  \u001b[32m\"over\"\u001b[39m,\n",
       "  \u001b[32m\"there\"\u001b[39m,\n",
       "  \u001b[32m\"and\"\u001b[39m,\n",
       "  \u001b[32m\"text\"\u001b[39m,\n",
       "  \u001b[32m\"short\"\u001b[39m,\n",
       "  \u001b[32m\"and\"\u001b[39m,\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"longer\"\u001b[39m,\n",
       "  \u001b[32m\"one\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmappedRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c083f1-3410-4fcf-9492-a4f7c65291e8",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102a4d28-da4e-451a-8213-786a785d42eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[4] at parallelize at cell10.sc:1\n",
       "\u001b[36mfilteredRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[5] at filter at cell10.sc:2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val filteredRDD = rdd.filter(_  < \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff88346b-421a-4b43-8526-760bc9b934b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:59 INFO SparkContext: Starting job: collect at cell11.sc:1\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Got job 2 (collect at cell11.sc:1) with 3 output partitions\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Final stage: ResultStage 2 (collect at cell11.sc:1)\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at filter at cell10.sc:2), which has no missing parents\n",
      "24/08/29 15:22:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at filter at cell10.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO Executor: Running task 1.0 in stage 2.0 (TID 7)\n",
      "24/08/29 15:22:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 6)\n",
      "24/08/29 15:22:59 INFO Executor: Running task 2.0 in stage 2.0 (TID 8)\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 1.0 in stage 2.0 (TID 7). 857 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 6). 861 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 2.0 in stage 2.0 (TID 8). 892 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 17 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 18 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 20 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:22:59 INFO DAGScheduler: ResultStage 2 (collect at cell11.sc:1) finished in 0,034 s\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Job 2 finished: collect at cell11.sc:1, took 0,041985 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"a\"\u001b[39m, \u001b[32m\"b\"\u001b[39m, \u001b[32m\"c\"\u001b[39m, \u001b[32m\"d\"\u001b[39m, \u001b[32m\"e\"\u001b[39m)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112e1b2-7749-4344-961a-efa36e1a46e2",
   "metadata": {},
   "source": [
    "## mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e7dbd6-ea63-43e6-93b2-732812ac2c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[6] at parallelize at cell12.sc:1\n",
       "\u001b[36mmapPartitionsRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[7] at mapPartitions at cell12.sc:2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val mapPartitionsRDD = rdd.mapPartitions(p => (Array(\"Hello\").iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9712febf-3797-463a-9db0-08c23f05ef8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:22:59 INFO SparkContext: Starting job: collect at cell13.sc:1\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Got job 3 (collect at cell13.sc:1) with 3 output partitions\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Final stage: ResultStage 3 (collect at cell13.sc:1)\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at mapPartitions at cell12.sc:2), which has no missing parents\n",
      "24/08/29 15:22:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:22:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at mapPartitions at cell12.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:22:59 INFO Executor: Running task 1.0 in stage 3.0 (TID 10)\n",
      "24/08/29 15:22:59 INFO Executor: Running task 2.0 in stage 3.0 (TID 11)\n",
      "24/08/29 15:22:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 857 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 1.0 in stage 3.0 (TID 10). 857 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 14 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 17 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:22:59 INFO Executor: Finished task 2.0 in stage 3.0 (TID 11). 900 bytes result sent to driver\n",
      "24/08/29 15:22:59 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 18 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:22:59 INFO DAGScheduler: ResultStage 3 (collect at cell13.sc:1) finished in 0,034 s\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:22:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/08/29 15:22:59 INFO DAGScheduler: Job 3 finished: collect at cell13.sc:1, took 0,044499 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"Hello\"\u001b[39m, \u001b[32m\"Hello\"\u001b[39m, \u001b[32m\"Hello\"\u001b[39m)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapPartitionsRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1b529-9b31-4733-95fb-4162295a2d3a",
   "metadata": {},
   "source": [
    "## mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3207095-1bb5-43aa-b23c-e77ae35e54a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[8] at parallelize at cell14.sc:1\n",
       "\u001b[36mmapPartitionsRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[9] at mapPartitionsWithIndex at cell14.sc:2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val mapPartitionsRDD = rdd.mapPartitionsWithIndex((i, p) => (Array(i).iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed59450-7255-4bc8-9fd8-0f50893902fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:00 INFO SparkContext: Starting job: collect at cell15.sc:1\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Got job 4 (collect at cell15.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Final stage: ResultStage 4 (collect at cell15.sc:1)\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at mapPartitionsWithIndex at cell14.sc:2), which has no missing parents\n",
      "24/08/29 15:23:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.0 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:35545 (size: 2.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at mapPartitionsWithIndex at cell14.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:00 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 13) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:00 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 14) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:00 INFO Executor: Running task 1.0 in stage 4.0 (TID 13)\n",
      "24/08/29 15:23:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 12)\n",
      "24/08/29 15:23:00 INFO Executor: Running task 2.0 in stage 4.0 (TID 14)\n",
      "24/08/29 15:23:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 12). 836 bytes result sent to driver\n",
      "24/08/29 15:23:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 15 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:00 INFO Executor: Finished task 2.0 in stage 4.0 (TID 14). 836 bytes result sent to driver\n",
      "24/08/29 15:23:00 INFO Executor: Finished task 1.0 in stage 4.0 (TID 13). 836 bytes result sent to driver\n",
      "24/08/29 15:23:00 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 14) in 15 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:00 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 13) in 17 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:00 INFO DAGScheduler: ResultStage 4 (collect at cell15.sc:1) finished in 0,029 s\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/08/29 15:23:00 INFO DAGScheduler: Job 4 finished: collect at cell15.sc:1, took 0,035158 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapPartitionsRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d272dc-0d5c-45fe-b2ec-95035fa10e5e",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0119a6e7-79aa-4d3b-99de-c7ddf11e574c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[10] at parallelize at cell16.sc:1\n",
       "\u001b[36msampleRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = PartitionwiseSampledRDD[11] at sample at cell16.sc:2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val sampleRDD = rdd.sample(false, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395c975a-441b-47d4-b81a-b3381a62c1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:01 INFO SparkContext: Starting job: collect at cell17.sc:1\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Got job 5 (collect at cell17.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Final stage: ResultStage 5 (collect at cell17.sc:1)\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Submitting ResultStage 5 (PartitionwiseSampledRDD[11] at sample at cell16.sc:2), which has no missing parents\n",
      "24/08/29 15:23:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.6 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (PartitionwiseSampledRDD[11] at sample at cell16.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO Executor: Running task 2.0 in stage 5.0 (TID 17)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 15)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 1.0 in stage 5.0 (TID 16)\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 1.0 in stage 5.0 (TID 16). 849 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 15). 853 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 2.0 in stage 5.0 (TID 17). 849 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 24 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 41 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 30 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:01 INFO DAGScheduler: ResultStage 5 (collect at cell17.sc:1) finished in 0,054 s\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Job 5 finished: collect at cell17.sc:1, took 0,060501 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres17\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"b\"\u001b[39m)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e98fe6-66a1-40d3-a71c-e30e420d7d6a",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "348ecc77-e6aa-4729-8a7d-2739cfb32025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[12] at parallelize at cell18.sc:1\n",
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[13] at parallelize at cell18.sc:2\n",
       "\u001b[36munionRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = UnionRDD[14] at union at cell18.sc:3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val rdd2 = sc.parallelize(List(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"X\", \"Y\", \"Z\"), 3)\n",
    "val unionRDD = rdd.union(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3311c7ab-e11a-4e40-86cc-149392768118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ubuntu:35545 in memory (size: 2.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO SparkContext: Starting job: collect at cell19.sc:1\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Got job 6 (collect at cell19.sc:1) with 6 output partitions\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Final stage: ResultStage 6 (collect at cell19.sc:1)\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Submitting ResultStage 6 (UnionRDD[14] at union at cell18.sc:3), which has no missing parents\n",
      "24/08/29 15:23:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu:35545 (size: 2.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:01 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 6 (UnionRDD[14] at union at cell18.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 6 tasks resource profile 0\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 19) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 20) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 21) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 22) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 23) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7732 bytes) \n",
      "24/08/29 15:23:01 INFO Executor: Running task 1.0 in stage 6.0 (TID 19)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 2.0 in stage 6.0 (TID 20)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 18)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 3.0 in stage 6.0 (TID 21)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 4.0 in stage 6.0 (TID 22)\n",
      "24/08/29 15:23:01 INFO Executor: Running task 5.0 in stage 6.0 (TID 23)\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 3.0 in stage 6.0 (TID 21). 861 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 18). 861 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 1.0 in stage 6.0 (TID 19). 861 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 2.0 in stage 6.0 (TID 20). 861 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 21) in 22 ms on ubuntu (executor driver) (1/6)\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 19) in 29 ms on ubuntu (executor driver) (2/6)\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 4.0 in stage 6.0 (TID 22). 904 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 32 ms on ubuntu (executor driver) (3/6)\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 20) in 30 ms on ubuntu (executor driver) (4/6)\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 22) in 29 ms on ubuntu (executor driver) (5/6)\n",
      "24/08/29 15:23:01 INFO Executor: Finished task 5.0 in stage 6.0 (TID 23). 861 bytes result sent to driver\n",
      "24/08/29 15:23:01 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 23) in 29 ms on ubuntu (executor driver) (6/6)\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:01 INFO DAGScheduler: ResultStage 6 (collect at cell19.sc:1) finished in 0,060 s\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/08/29 15:23:01 INFO DAGScheduler: Job 6 finished: collect at cell19.sc:1, took 0,067853 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres19\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"b\"\u001b[39m,\n",
       "  \u001b[32m\"c\"\u001b[39m,\n",
       "  \u001b[32m\"d\"\u001b[39m,\n",
       "  \u001b[32m\"e\"\u001b[39m,\n",
       "  \u001b[32m\"f\"\u001b[39m,\n",
       "  \u001b[32m\"x\"\u001b[39m,\n",
       "  \u001b[32m\"y\"\u001b[39m,\n",
       "  \u001b[32m\"z\"\u001b[39m,\n",
       "  \u001b[32m\"A\"\u001b[39m,\n",
       "  \u001b[32m\"B\"\u001b[39m,\n",
       "  \u001b[32m\"C\"\u001b[39m,\n",
       "  \u001b[32m\"D\"\u001b[39m,\n",
       "  \u001b[32m\"E\"\u001b[39m,\n",
       "  \u001b[32m\"F\"\u001b[39m,\n",
       "  \u001b[32m\"X\"\u001b[39m,\n",
       "  \u001b[32m\"Y\"\u001b[39m,\n",
       "  \u001b[32m\"Z\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be4636-89a4-47c6-889a-702eda3e00bc",
   "metadata": {},
   "source": [
    "## Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eadeb378-2b1f-417a-87f9-e01c90b7a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[15] at parallelize at cell20.sc:1\n",
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[16] at parallelize at cell20.sc:2\n",
       "\u001b[36mintersectionRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[22] at intersection at cell20.sc:3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"), 3)\n",
    "val rdd2 = sc.parallelize(List(\"d\", \"e\", \"f\", \"x\", \"y\", \"z\"), 3)\n",
    "val intersectionRDD = rdd.intersection(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1e2cea-f357-4a1a-8603-c0fed596a6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:02 INFO SparkContext: Starting job: collect at cell21.sc:1\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Registering RDD 18 (intersection at cell20.sc:3) as input to shuffle 1\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Registering RDD 17 (intersection at cell20.sc:3) as input to shuffle 0\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Got job 7 (collect at cell21.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Final stage: ResultStage 9 (collect at cell21.sc:1)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7, ShuffleMapStage 8)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7, ShuffleMapStage 8)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[18] at intersection at cell20.sc:3), which has no missing parents\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu:35545 (size: 2.6 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[18] at intersection at cell20.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[17] at intersection at cell20.sc:3), which has no missing parents\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 24) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 25) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 26) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO Executor: Running task 0.0 in stage 7.0 (TID 24)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 2.0 in stage 7.0 (TID 26)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 1.0 in stage 7.0 (TID 25)\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ubuntu:35545 (size: 2.6 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[17] at intersection at cell20.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 27) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 28) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 29) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:02 INFO Executor: Running task 0.0 in stage 8.0 (TID 27)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 2.0 in stage 8.0 (TID 29)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 1.0 in stage 8.0 (TID 28)\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 0.0 in stage 7.0 (TID 24). 1083 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 2.0 in stage 8.0 (TID 29). 1083 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 1.0 in stage 8.0 (TID 28). 1083 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 1.0 in stage 7.0 (TID 25). 1083 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 2.0 in stage 7.0 (TID 26). 1083 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 24) in 59 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 0.0 in stage 8.0 (TID 27). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 25) in 58 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 28) in 40 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 29) in 42 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 26) in 65 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 27) in 49 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: ShuffleMapStage 7 (intersection at cell20.sc:3) finished in 0,083 s\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:02 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:02 INFO DAGScheduler: running: Set(ShuffleMapStage 8)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:02 INFO DAGScheduler: ShuffleMapStage 8 (intersection at cell20.sc:3) finished in 0,076 s\n",
      "24/08/29 15:23:02 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:02 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:02 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "24/08/29 15:23:02 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at intersection at cell20.sc:3), which has no missing parents\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.6 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ubuntu:35545 (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at intersection at cell20.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Adding task set 9.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 30) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 31) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:02 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 32) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 30)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 1.0 in stage 9.0 (TID 31)\n",
      "24/08/29 15:23:02 INFO Executor: Running task 2.0 in stage 9.0 (TID 32)\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 1.0 in stage 9.0 (TID 31). 1713 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 0.0 in stage 9.0 (TID 30). 1713 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO Executor: Finished task 2.0 in stage 9.0 (TID 32). 1713 bytes result sent to driver\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 32) in 93 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 30) in 98 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:02 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 31) in 96 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:02 INFO DAGScheduler: ResultStage 9 (collect at cell21.sc:1) finished in 0,116 s\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/08/29 15:23:02 INFO DAGScheduler: Job 7 finished: collect at cell21.sc:1, took 0,235869 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"f\"\u001b[39m, \u001b[32m\"d\"\u001b[39m, \u001b[32m\"e\"\u001b[39m)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersectionRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7900dc-f654-4963-9975-1192bdba78f1",
   "metadata": {},
   "source": [
    "## Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4930a7d3-6089-4b65-96d4-b2e722f07d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[23] at parallelize at cell22.sc:1\n",
       "\u001b[36mdistinctRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[26] at distinct at cell22.sc:2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"c\", \"c\", \"b\", \"b\", \"a\"), 3)\n",
    "val distinctRDD = rdd.distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67651626-4872-4d36-85d7-3e9a8acda203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:03 INFO SparkContext: Starting job: collect at cell23.sc:1\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Registering RDD 24 (distinct at cell22.sc:2) as input to shuffle 2\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Got job 8 (collect at cell23.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Final stage: ResultStage 11 (collect at cell23.sc:1)\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[24] at distinct at cell22.sc:2), which has no missing parents\n",
      "24/08/29 15:23:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ubuntu:35545 (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[24] at distinct at cell22.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:03 INFO TaskSchedulerImpl: Adding task set 10.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 33) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7608 bytes) \n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 34) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7614 bytes) \n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 35) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7613 bytes) \n",
      "24/08/29 15:23:03 INFO Executor: Running task 1.0 in stage 10.0 (TID 34)\n",
      "24/08/29 15:23:03 INFO Executor: Running task 2.0 in stage 10.0 (TID 35)\n",
      "24/08/29 15:23:03 INFO Executor: Running task 0.0 in stage 10.0 (TID 33)\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 0.0 in stage 10.0 (TID 33). 1212 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 2.0 in stage 10.0 (TID 35). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 1.0 in stage 10.0 (TID 34). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 33) in 48 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 35) in 48 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 34) in 52 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:03 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:03 INFO DAGScheduler: ShuffleMapStage 10 (distinct at cell22.sc:2) finished in 0,066 s\n",
      "24/08/29 15:23:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:03 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:03 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
      "24/08/29 15:23:03 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[26] at distinct at cell22.sc:2), which has no missing parents\n",
      "24/08/29 15:23:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ubuntu:35545 (size: 3.4 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at distinct at cell22.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 36) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 37) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:03 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 38) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:03 INFO Executor: Running task 1.0 in stage 11.0 (TID 37)\n",
      "24/08/29 15:23:03 INFO Executor: Running task 0.0 in stage 11.0 (TID 36)\n",
      "24/08/29 15:23:03 INFO Executor: Running task 2.0 in stage 11.0 (TID 38)\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 2 (98.0 B) non-empty blocks including 2 (98.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 2 (98.0 B) non-empty blocks including 2 (98.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/29 15:23:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 0.0 in stage 11.0 (TID 36). 1756 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 2.0 in stage 11.0 (TID 38). 1713 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 36) in 33 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:03 INFO Executor: Finished task 1.0 in stage 11.0 (TID 37). 1756 bytes result sent to driver\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 38) in 30 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:03 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 37) in 38 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:03 INFO DAGScheduler: ResultStage 11 (collect at cell23.sc:1) finished in 0,058 s\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/08/29 15:23:03 INFO DAGScheduler: Job 8 finished: collect at cell23.sc:1, took 0,310280 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"c\"\u001b[39m, \u001b[32m\"a\"\u001b[39m, \u001b[32m\"b\"\u001b[39m)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bda78-d7b6-4629-a775-fc5bf492be7f",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0765d94c-2e45-4de8-9d7a-987fb95d2e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ubuntu:35545 in memory (size: 3.4 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ubuntu:35545 in memory (size: 2.6 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ubuntu:35545 in memory (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ubuntu:35545 in memory (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ubuntu:35545 in memory (size: 2.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ubuntu:35545 in memory (size: 2.6 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[27] at parallelize at cell24.sc:1\n",
       "\u001b[36mgroupbyRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mIterable\u001b[39m[\u001b[32mString\u001b[39m])] = ShuffledRDD[28] at groupByKey at cell24.sc:2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1, \"a\"), (2, \"b\"), (3, \"c\"), (1, \"d\"), (2, \"e\")), 3)\n",
    "val groupbyRDD = rdd.groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2cede8-6790-46b5-b776-255aa68075fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:04 INFO SparkContext: Starting job: collect at cell25.sc:1\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Registering RDD 27 (parallelize at cell24.sc:1) as input to shuffle 3\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Got job 9 (collect at cell25.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Final stage: ResultStage 13 (collect at cell25.sc:1)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting ShuffleMapStage 12 (ParallelCollectionRDD[27] at parallelize at cell24.sc:1), which has no missing parents\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 5.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu:35545 (size: 2.9 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 12 (ParallelCollectionRDD[27] at parallelize at cell24.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 39) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 40) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 41) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:04 INFO Executor: Running task 1.0 in stage 12.0 (TID 40)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 39)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 2.0 in stage 12.0 (TID 41)\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 39). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 39) in 24 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 2.0 in stage 12.0 (TID 41). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 1.0 in stage 12.0 (TID 40). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 40) in 31 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 41) in 31 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:04 INFO DAGScheduler: ShuffleMapStage 12 (parallelize at cell24.sc:1) finished in 0,044 s\n",
      "24/08/29 15:23:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:04 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:04 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting ResultStage 13 (ShuffledRDD[28] at groupByKey at cell24.sc:2), which has no missing parents\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 6.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu:35545 (size: 3.4 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (ShuffledRDD[28] at groupByKey at cell24.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 42) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 43) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 44) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 42)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 2.0 in stage 13.0 (TID 44)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 1.0 in stage 13.0 (TID 43)\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 1.0 in stage 13.0 (TID 43). 2200 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 42). 2197 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 43) in 22 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 42) in 22 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 2.0 in stage 13.0 (TID 44). 2200 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 44) in 25 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:04 INFO DAGScheduler: ResultStage 13 (collect at cell25.sc:1) finished in 0,035 s\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Job 9 finished: collect at cell25.sc:1, took 0,093188 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres25\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mIterable\u001b[39m[\u001b[32mString\u001b[39m])] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m3\u001b[39m, \u001b[33mCompactBuffer\u001b[39m(\u001b[32m\"c\"\u001b[39m)),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[33mCompactBuffer\u001b[39m(\u001b[32m\"a\"\u001b[39m, \u001b[32m\"d\"\u001b[39m)),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[33mCompactBuffer\u001b[39m(\u001b[32m\"b\"\u001b[39m, \u001b[32m\"e\"\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupbyRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560a06e-54b2-44be-bf73-23e9f543ec2a",
   "metadata": {},
   "source": [
    "## ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a8d93b0-bb1a-4e2a-8a08-c4b3ed9a0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ParallelCollectionRDD[29] at parallelize at cell26.sc:1\n",
       "\u001b[36mreducebyRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ShuffledRDD[30] at reduceByKey at cell26.sc:2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 4), (\"b\", 5)), 3)\n",
    "val reducebyRDD = rdd.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59c3f13-a685-4bb7-b0c4-6f01833fb799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:04 INFO SparkContext: Starting job: collect at cell27.sc:1\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Registering RDD 29 (parallelize at cell26.sc:1) as input to shuffle 4\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Got job 10 (collect at cell27.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Final stage: ResultStage 15 (collect at cell27.sc:1)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting ShuffleMapStage 14 (ParallelCollectionRDD[29] at parallelize at cell26.sc:1), which has no missing parents\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu:35545 (size: 2.7 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 14 (ParallelCollectionRDD[29] at parallelize at cell26.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Adding task set 14.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 45) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 46) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 47) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:04 INFO Executor: Running task 2.0 in stage 14.0 (TID 47)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 1.0 in stage 14.0 (TID 46)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 0.0 in stage 14.0 (TID 45)\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 0.0 in stage 14.0 (TID 45). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 2.0 in stage 14.0 (TID 47). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 1.0 in stage 14.0 (TID 46). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 47) in 19 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 45) in 22 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 46) in 27 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:04 INFO DAGScheduler: ShuffleMapStage 14 (parallelize at cell26.sc:1) finished in 0,036 s\n",
      "24/08/29 15:23:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:04 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:04 INFO DAGScheduler: waiting: Set(ResultStage 15)\n",
      "24/08/29 15:23:04 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting ResultStage 15 (ShuffledRDD[30] at reduceByKey at cell26.sc:2), which has no missing parents\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu:35545 (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 15 (ShuffledRDD[30] at reduceByKey at cell26.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Adding task set 15.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 48) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 49) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 50) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:04 INFO Executor: Running task 0.0 in stage 15.0 (TID 48)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 2.0 in stage 15.0 (TID 50)\n",
      "24/08/29 15:23:04 INFO Executor: Running task 1.0 in stage 15.0 (TID 49)\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "24/08/29 15:23:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 48). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 48) in 19 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 1.0 in stage 15.0 (TID 49). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO Executor: Finished task 2.0 in stage 15.0 (TID 50). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 49) in 24 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:04 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 50) in 24 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:04 INFO DAGScheduler: ResultStage 15 (collect at cell27.sc:1) finished in 0,037 s\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/08/29 15:23:04 INFO DAGScheduler: Job 10 finished: collect at cell27.sc:1, took 0,084135 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres27\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m\"c\"\u001b[39m, \u001b[32m3\u001b[39m), (\u001b[32m\"a\"\u001b[39m, \u001b[32m5\u001b[39m), (\u001b[32m\"b\"\u001b[39m, \u001b[32m7\u001b[39m))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducebyRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f91f9-f0aa-4b84-bf49-37f75e5f12aa",
   "metadata": {},
   "source": [
    "## AggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d512279c-4988-4604-b0c3-f665c034c308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ParallelCollectionRDD[31] at parallelize at cell28.sc:1\n",
       "\u001b[36maggregateByRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ShuffledRDD[32] at aggregateByKey at cell28.sc:2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 4), (\"b\", 5)), 3)\n",
    "val aggregateByRDD = rdd.aggregateByKey(0)(_+_, _+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f860ad18-a7d9-4aa2-b3d3-b8a86f98d485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:05 INFO SparkContext: Starting job: collect at cell29.sc:1\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Registering RDD 31 (parallelize at cell28.sc:1) as input to shuffle 5\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Got job 11 (collect at cell29.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Final stage: ResultStage 17 (collect at cell29.sc:1)\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting ShuffleMapStage 16 (ParallelCollectionRDD[31] at parallelize at cell28.sc:1), which has no missing parents\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 5.0 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu:35545 (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 16 (ParallelCollectionRDD[31] at parallelize at cell28.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Adding task set 16.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 51) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 52) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 53) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:05 INFO Executor: Running task 0.0 in stage 16.0 (TID 51)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 2.0 in stage 16.0 (TID 53)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 1.0 in stage 16.0 (TID 52)\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 1.0 in stage 16.0 (TID 52). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 2.0 in stage 16.0 (TID 53). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 53) in 22 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 0.0 in stage 16.0 (TID 51). 1212 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 52) in 26 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 51) in 27 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:05 INFO DAGScheduler: ShuffleMapStage 16 (parallelize at cell28.sc:1) finished in 0,040 s\n",
      "24/08/29 15:23:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:05 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:05 INFO DAGScheduler: waiting: Set(ResultStage 17)\n",
      "24/08/29 15:23:05 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRDD[32] at aggregateByKey at cell28.sc:2), which has no missing parents\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu:35545 (size: 3.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (ShuffledRDD[32] at aggregateByKey at cell28.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Adding task set 17.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 54) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 55) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 56) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:05 INFO Executor: Running task 2.0 in stage 17.0 (TID 56)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 1.0 in stage 17.0 (TID 55)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 0.0 in stage 17.0 (TID 54)\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 2.0 in stage 17.0 (TID 56). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 0.0 in stage 17.0 (TID 54). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 56) in 16 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 1.0 in stage 17.0 (TID 55). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 54) in 20 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 55) in 22 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:05 INFO DAGScheduler: ResultStage 17 (collect at cell29.sc:1) finished in 0,032 s\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Job 11 finished: collect at cell29.sc:1, took 0,080652 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres29\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m\"c\"\u001b[39m, \u001b[32m3\u001b[39m), (\u001b[32m\"a\"\u001b[39m, \u001b[32m5\u001b[39m), (\u001b[32m\"b\"\u001b[39m, \u001b[32m7\u001b[39m))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregateByRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d6e12-7284-4fd8-848a-eeb9ae6ffd05",
   "metadata": {},
   "source": [
    "## SortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1fd6451-045f-4369-8400-17a1aa3bcd8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ubuntu:35545 in memory (size: 3.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ubuntu:35545 in memory (size: 2.7 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ubuntu:35545 in memory (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ubuntu:35545 in memory (size: 2.9 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on ubuntu:35545 in memory (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ubuntu:35545 in memory (size: 3.4 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO SparkContext: Starting job: sortByKey at cell30.sc:2\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Got job 12 (sortByKey at cell30.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Final stage: ResultStage 18 (sortByKey at cell30.sc:2)\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[35] at sortByKey at cell30.sc:2), which has no missing parents\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.7 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ubuntu:35545 (size: 2.5 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:05 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 18 (MapPartitionsRDD[35] at sortByKey at cell30.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Adding task set 18.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 57) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7670 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 58) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7690 bytes) \n",
      "24/08/29 15:23:05 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 59) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7690 bytes) \n",
      "24/08/29 15:23:05 INFO Executor: Running task 2.0 in stage 18.0 (TID 59)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 0.0 in stage 18.0 (TID 57)\n",
      "24/08/29 15:23:05 INFO Executor: Running task 1.0 in stage 18.0 (TID 58)\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 2.0 in stage 18.0 (TID 59). 1142 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 0.0 in stage 18.0 (TID 57). 1095 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO Executor: Finished task 1.0 in stage 18.0 (TID 58). 1099 bytes result sent to driver\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 59) in 24 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 58) in 26 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:05 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 57) in 28 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:05 INFO DAGScheduler: ResultStage 18 (sortByKey at cell30.sc:2) finished in 0,043 s\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/08/29 15:23:05 INFO DAGScheduler: Job 12 finished: sortByKey at cell30.sc:2, took 0,049191 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ParallelCollectionRDD[33] at parallelize at cell30.sc:1\n",
       "\u001b[36msortedRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ShuffledRDD[36] at sortByKey at cell30.sc:2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 4), (\"b\", 5)), 3)\n",
    "val sortedRDD = rdd.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "254ebe2e-f911-457f-9ef8-8d2423f97584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:06 INFO SparkContext: Starting job: collect at cell31.sc:1\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Registering RDD 33 (parallelize at cell30.sc:1) as input to shuffle 6\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Got job 13 (collect at cell31.sc:1) with 3 output partitions\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Final stage: ResultStage 20 (collect at cell31.sc:1)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting ShuffleMapStage 19 (ParallelCollectionRDD[33] at parallelize at cell30.sc:1), which has no missing parents\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 4.4 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ubuntu:35545 (size: 2.7 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 19 (ParallelCollectionRDD[33] at parallelize at cell30.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Adding task set 19.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 60) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 61) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 2.0 in stage 19.0 (TID 62) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 1.0 in stage 19.0 (TID 61)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 0.0 in stage 19.0 (TID 60)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 2.0 in stage 19.0 (TID 62)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 0.0 in stage 19.0 (TID 60). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 2.0 in stage 19.0 (TID 62). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 60) in 31 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 1.0 in stage 19.0 (TID 61). 1040 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 2.0 in stage 19.0 (TID 62) in 33 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 61) in 35 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: ShuffleMapStage 19 (parallelize at cell30.sc:1) finished in 0,046 s\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:06 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:06 INFO DAGScheduler: waiting: Set(ResultStage 20)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting ResultStage 20 (ShuffledRDD[36] at sortByKey at cell30.sc:2), which has no missing parents\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ubuntu:35545 (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 20 (ShuffledRDD[36] at sortByKey at cell30.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Adding task set 20.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 63) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 64) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 65) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 2.0 in stage 20.0 (TID 65)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 1.0 in stage 20.0 (TID 64)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 0.0 in stage 20.0 (TID 63)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 2 (108.0 B) non-empty blocks including 2 (108.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 1.0 in stage 20.0 (TID 64). 1871 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 0.0 in stage 20.0 (TID 63). 1871 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 2.0 in stage 20.0 (TID 65). 1851 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 63) in 25 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 65) in 26 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 64) in 29 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:06 INFO DAGScheduler: ResultStage 20 (collect at cell31.sc:1) finished in 0,039 s\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Job 13 finished: collect at cell31.sc:1, took 0,092518 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres31\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"a\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"a\"\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m\"b\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"b\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"c\"\u001b[39m, \u001b[32m3\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbd1dc-bd35-4276-a26a-3e3cee37a828",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7364e748-79d0-449e-97ab-261f4789ff42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[37] at parallelize at cell32.sc:1\n",
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[38] at parallelize at cell32.sc:2\n",
       "\u001b[36mjoinedRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mInt\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = MapPartitionsRDD[41] at join at cell32.sc:3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1, \"a\"), (2, \"b\"), (3, \"c\")))\n",
    "val rdd2 = sc.parallelize(List((2, \"B\"), (1, \"A\"), (3, \"C\")))\n",
    "val joinedRDD = rdd.join(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcdc78c0-0a2c-447d-b120-c71ae66de99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:06 INFO SparkContext: Starting job: collect at cell33.sc:1\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Registering RDD 37 (parallelize at cell32.sc:1) as input to shuffle 7\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Registering RDD 38 (parallelize at cell32.sc:2) as input to shuffle 8\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Got job 14 (collect at cell33.sc:1) with 8 output partitions\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Final stage: ResultStage 23 (collect at cell33.sc:1)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21, ShuffleMapStage 22)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 21, ShuffleMapStage 22)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting ShuffleMapStage 21 (ParallelCollectionRDD[37] at parallelize at cell32.sc:1), which has no missing parents\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1964.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ubuntu:35545 (size: 1964.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 21 (ParallelCollectionRDD[37] at parallelize at cell32.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Adding task set 21.0 with 8 tasks resource profile 0\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting ShuffleMapStage 22 (ParallelCollectionRDD[38] at parallelize at cell32.sc:2), which has no missing parents\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 66) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 67) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 68) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 3.0 in stage 21.0 (TID 69) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 4.0 in stage 21.0 (TID 70) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1961.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 5.0 in stage 21.0 (TID 71) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 6.0 in stage 21.0 (TID 72) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:35545 (size: 1961.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 7.0 in stage 21.0 (TID 73) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 0.0 in stage 21.0 (TID 66)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 2.0 in stage 21.0 (TID 68)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 4.0 in stage 21.0 (TID 70)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 3.0 in stage 21.0 (TID 69)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 5.0 in stage 21.0 (TID 71)\n",
      "24/08/29 15:23:06 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:06 INFO Executor: Running task 7.0 in stage 21.0 (TID 73)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 6.0 in stage 21.0 (TID 72)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 3.0 in stage 21.0 (TID 69). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 0.0 in stage 21.0 (TID 66). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Running task 1.0 in stage 21.0 (TID 67)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 2.0 in stage 21.0 (TID 68). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 4.0 in stage 21.0 (TID 70). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 6.0 in stage 21.0 (TID 72). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 5.0 in stage 21.0 (TID 71). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 22 (ParallelCollectionRDD[38] at parallelize at cell32.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 66) in 31 ms on ubuntu (executor driver) (1/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 7.0 in stage 21.0 (TID 73). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Adding task set 22.0 with 8 tasks resource profile 0\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 3.0 in stage 21.0 (TID 69) in 30 ms on ubuntu (executor driver) (2/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 68) in 34 ms on ubuntu (executor driver) (3/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 1.0 in stage 21.0 (TID 67). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 4.0 in stage 21.0 (TID 70) in 32 ms on ubuntu (executor driver) (4/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 6.0 in stage 21.0 (TID 72) in 33 ms on ubuntu (executor driver) (5/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 74) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 75) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 2.0 in stage 22.0 (TID 76) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 3.0 in stage 22.0 (TID 77) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 4.0 in stage 22.0 (TID 78) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 5.0 in stage 22.0 (TID 79) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 1.0 in stage 22.0 (TID 75)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 5.0 in stage 21.0 (TID 71) in 44 ms on ubuntu (executor driver) (6/8)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 2.0 in stage 22.0 (TID 76)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 0.0 in stage 22.0 (TID 74)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 4.0 in stage 22.0 (TID 78)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 6.0 in stage 22.0 (TID 80) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7596 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 5.0 in stage 22.0 (TID 79)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 3.0 in stage 22.0 (TID 77)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 7.0 in stage 21.0 (TID 73) in 46 ms on ubuntu (executor driver) (7/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 1.0 in stage 22.0 (TID 75). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Running task 6.0 in stage 22.0 (TID 80)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 67) in 60 ms on ubuntu (executor driver) (8/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 4.0 in stage 22.0 (TID 78). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 0.0 in stage 22.0 (TID 74). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO DAGScheduler: ShuffleMapStage 21 (parallelize at cell32.sc:1) finished in 0,079 s\n",
      "24/08/29 15:23:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:06 INFO DAGScheduler: running: Set(ShuffleMapStage 22)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: waiting: Set(ResultStage 23)\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:06 INFO Executor: Finished task 3.0 in stage 22.0 (TID 77). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 7.0 in stage 22.0 (TID 81) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 7.0 in stage 22.0 (TID 81)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 75) in 29 ms on ubuntu (executor driver) (1/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 4.0 in stage 22.0 (TID 78) in 24 ms on ubuntu (executor driver) (2/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 3.0 in stage 22.0 (TID 77) in 27 ms on ubuntu (executor driver) (3/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 74) in 32 ms on ubuntu (executor driver) (4/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 2.0 in stage 22.0 (TID 76). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 2.0 in stage 22.0 (TID 76) in 30 ms on ubuntu (executor driver) (5/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 5.0 in stage 22.0 (TID 79). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 5.0 in stage 22.0 (TID 79) in 28 ms on ubuntu (executor driver) (6/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 7.0 in stage 22.0 (TID 81). 1045 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 6.0 in stage 22.0 (TID 80). 916 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 6.0 in stage 22.0 (TID 80) in 27 ms on ubuntu (executor driver) (7/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 7.0 in stage 22.0 (TID 81) in 10 ms on ubuntu (executor driver) (8/8)\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:06 INFO DAGScheduler: ShuffleMapStage 22 (parallelize at cell32.sc:2) finished in 0,080 s\n",
      "24/08/29 15:23:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:06 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:06 INFO DAGScheduler: waiting: Set(ResultStage 23)\n",
      "24/08/29 15:23:06 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[41] at join at cell32.sc:3), which has no missing parents\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 5.5 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:35545 (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:06 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 23 (MapPartitionsRDD[41] at join at cell32.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Adding task set 23.0 with 8 tasks resource profile 0\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 82) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 83) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 84) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 85) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 4.0 in stage 23.0 (TID 86) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 5.0 in stage 23.0 (TID 87) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 6.0 in stage 23.0 (TID 88) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO TaskSetManager: Starting task 7.0 in stage 23.0 (TID 89) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7496 bytes) \n",
      "24/08/29 15:23:06 INFO Executor: Running task 1.0 in stage 23.0 (TID 83)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 0.0 in stage 23.0 (TID 82)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 2.0 in stage 23.0 (TID 84)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 4.0 in stage 23.0 (TID 86)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 7.0 in stage 23.0 (TID 89)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO Executor: Running task 5.0 in stage 23.0 (TID 87)\n",
      "24/08/29 15:23:06 INFO Executor: Running task 3.0 in stage 23.0 (TID 85)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO Executor: Running task 6.0 in stage 23.0 (TID 88)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 7.0 in stage 23.0 (TID 89). 1705 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 1.0 in stage 23.0 (TID 83). 1861 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 7.0 in stage 23.0 (TID 89) in 21 ms on ubuntu (executor driver) (1/8)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 0.0 in stage 23.0 (TID 82). 1705 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 83) in 29 ms on ubuntu (executor driver) (2/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 4.0 in stage 23.0 (TID 86). 1748 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 82) in 36 ms on ubuntu (executor driver) (3/8)\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 6.0 in stage 23.0 (TID 88). 1705 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 2.0 in stage 23.0 (TID 84). 1861 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 4.0 in stage 23.0 (TID 86) in 37 ms on ubuntu (executor driver) (4/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 5.0 in stage 23.0 (TID 87). 1705 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 6.0 in stage 23.0 (TID 88) in 38 ms on ubuntu (executor driver) (5/8)\n",
      "24/08/29 15:23:06 INFO Executor: Finished task 3.0 in stage 23.0 (TID 85). 1861 bytes result sent to driver\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 5.0 in stage 23.0 (TID 87) in 40 ms on ubuntu (executor driver) (6/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 85) in 42 ms on ubuntu (executor driver) (7/8)\n",
      "24/08/29 15:23:06 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 84) in 43 ms on ubuntu (executor driver) (8/8)\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:06 INFO DAGScheduler: ResultStage 23 (collect at cell33.sc:1) finished in 0,059 s\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/08/29 15:23:06 INFO DAGScheduler: Job 14 finished: collect at cell33.sc:1, took 0,168647 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mInt\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, (\u001b[32m\"a\"\u001b[39m, \u001b[32m\"A\"\u001b[39m)),\n",
       "  (\u001b[32m2\u001b[39m, (\u001b[32m\"b\"\u001b[39m, \u001b[32m\"B\"\u001b[39m)),\n",
       "  (\u001b[32m3\u001b[39m, (\u001b[32m\"c\"\u001b[39m, \u001b[32m\"C\"\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ba2ac-1e52-4202-88c4-bec4fc48ba66",
   "metadata": {},
   "source": [
    "## Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30c6452f-0acb-430d-810a-b7463be3cd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[42] at parallelize at cell34.sc:1\n",
       "\u001b[36mcoalesceRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = CoalescedRDD[43] at coalesce at cell34.sc:2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"), 4)\n",
    "val coalesceRDD = rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf9e6685-9433-4e1d-b47c-1fb0574a97fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres35_0\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m\n",
       "\u001b[36mres35_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.partitions.size\n",
    "coalesceRDD.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d266b-86dc-405f-bc79-4e9bb9206c8d",
   "metadata": {},
   "source": [
    "## Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dee2214-1d13-41ab-baf5-21b6e8f26b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ubuntu:35545 in memory (size: 2.7 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_23_piece0 on ubuntu:35545 in memory (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ubuntu:35545 in memory (size: 1964.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ubuntu:35545 in memory (size: 2.5 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_20_piece0 on ubuntu:35545 in memory (size: 3.1 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:07 INFO BlockManagerInfo: Removed broadcast_22_piece0 on ubuntu:35545 in memory (size: 1961.0 B, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[44] at parallelize at cell36.sc:1\n",
       "\u001b[36mrepartitionRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[48] at repartition at cell36.sc:2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"), 2)\n",
    "val repartitionRDD = rdd.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8f780c7-ff8e-491f-9d8e-04cd2cbae9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres37_0\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m\n",
       "\u001b[36mres37_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.partitions.size\n",
    "repartitionRDD.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb179f2-babb-42b3-80e2-2e7bdb311bbc",
   "metadata": {},
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02f69b96-d344-4eef-87cb-587881fd6739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:08 INFO SparkContext: Starting job: reduce at cell38.sc:2\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Got job 15 (reduce at cell38.sc:2) with 8 output partitions\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Final stage: ResultStage 24 (reduce at cell38.sc:2)\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting ResultStage 24 (ParallelCollectionRDD[49] at parallelize at cell38.sc:1), which has no missing parents\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1948.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ubuntu:35545 (size: 1948.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 24 (ParallelCollectionRDD[49] at parallelize at cell38.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Adding task set 24.0 with 8 tasks resource profile 0\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 90) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 1.0 in stage 24.0 (TID 91) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 2.0 in stage 24.0 (TID 92) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 3.0 in stage 24.0 (TID 93) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 4.0 in stage 24.0 (TID 94) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 5.0 in stage 24.0 (TID 95) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 6.0 in stage 24.0 (TID 96) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7568 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 7.0 in stage 24.0 (TID 97) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7572 bytes) \n",
      "24/08/29 15:23:08 INFO Executor: Running task 1.0 in stage 24.0 (TID 91)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 0.0 in stage 24.0 (TID 90)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 4.0 in stage 24.0 (TID 94)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 3.0 in stage 24.0 (TID 93)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 6.0 in stage 24.0 (TID 96)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 7.0 in stage 24.0 (TID 97)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 5.0 in stage 24.0 (TID 95)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 6.0 in stage 24.0 (TID 96). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 0.0 in stage 24.0 (TID 90). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Running task 2.0 in stage 24.0 (TID 92)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 5.0 in stage 24.0 (TID 95). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 7.0 in stage 24.0 (TID 97). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 2.0 in stage 24.0 (TID 92). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 4.0 in stage 24.0 (TID 94). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 1.0 in stage 24.0 (TID 91). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 6.0 in stage 24.0 (TID 96) in 19 ms on ubuntu (executor driver) (1/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 90) in 29 ms on ubuntu (executor driver) (2/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 2.0 in stage 24.0 (TID 92) in 29 ms on ubuntu (executor driver) (3/8)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 3.0 in stage 24.0 (TID 93). 926 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 1.0 in stage 24.0 (TID 91) in 30 ms on ubuntu (executor driver) (4/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 5.0 in stage 24.0 (TID 95) in 28 ms on ubuntu (executor driver) (5/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 4.0 in stage 24.0 (TID 94) in 30 ms on ubuntu (executor driver) (6/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 3.0 in stage 24.0 (TID 93) in 31 ms on ubuntu (executor driver) (7/8)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 7.0 in stage 24.0 (TID 97) in 31 ms on ubuntu (executor driver) (8/8)\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:08 INFO DAGScheduler: ResultStage 24 (reduce at cell38.sc:2) finished in 0,044 s\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 15 finished: reduce at cell38.sc:2, took 0,053475 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[49] at parallelize at cell38.sc:1\n",
       "\u001b[36msumOfItems\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m45\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
    "val sumOfItems = rdd.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e50a9-fb62-4707-b51d-54649465c67f",
   "metadata": {},
   "source": [
    "## Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "816add1a-d4a5-4d78-8af2-e92d447fe643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:08 INFO SparkContext: Starting job: collect at cell39.sc:2\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Got job 16 (collect at cell39.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Final stage: ResultStage 25 (collect at cell39.sc:2)\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting ResultStage 25 (ParallelCollectionRDD[50] at parallelize at cell39.sc:1), which has no missing parents\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 2.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1769.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ubuntu:35545 (size: 1769.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 25 (ParallelCollectionRDD[50] at parallelize at cell39.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Adding task set 25.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 98) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 99) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 2.0 in stage 25.0 (TID 100) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO Executor: Running task 1.0 in stage 25.0 (TID 99)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 2.0 in stage 25.0 (TID 100)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 2.0 in stage 25.0 (TID 100). 818 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 1.0 in stage 25.0 (TID 99). 818 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Running task 0.0 in stage 25.0 (TID 98)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 2.0 in stage 25.0 (TID 100) in 7 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 0.0 in stage 25.0 (TID 98). 818 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 99) in 9 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 98) in 14 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:08 INFO DAGScheduler: ResultStage 25 (collect at cell39.sc:2) finished in 0,021 s\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 16 finished: collect at cell39.sc:2, took 0,024729 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[50] at parallelize at cell39.sc:1\n",
       "\u001b[36mcollectResult\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"b\"\u001b[39m,\n",
       "  \u001b[32m\"c\"\u001b[39m,\n",
       "  \u001b[32m\"d\"\u001b[39m,\n",
       "  \u001b[32m\"e\"\u001b[39m,\n",
       "  \u001b[32m\"f\"\u001b[39m,\n",
       "  \u001b[32m\"g\"\u001b[39m,\n",
       "  \u001b[32m\"h\"\u001b[39m,\n",
       "  \u001b[32m\"i\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"), 3)\n",
    "val collectResult = rdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61db812-3954-4752-a262-9ef1280d4ddc",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a006cd35-62f5-4bb8-9fa2-4c351d492580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:08 INFO SparkContext: Starting job: count at cell40.sc:2\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Got job 17 (count at cell40.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Final stage: ResultStage 26 (count at cell40.sc:2)\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting ResultStage 26 (ParallelCollectionRDD[51] at parallelize at cell40.sc:1), which has no missing parents\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 2.8 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 1769.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ubuntu:35545 (size: 1769.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:08 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 26 (ParallelCollectionRDD[51] at parallelize at cell40.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Adding task set 26.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 101) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 102) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO TaskSetManager: Starting task 2.0 in stage 26.0 (TID 103) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:08 INFO Executor: Running task 1.0 in stage 26.0 (TID 102)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 2.0 in stage 26.0 (TID 103)\n",
      "24/08/29 15:23:08 INFO Executor: Running task 0.0 in stage 26.0 (TID 101)\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 0.0 in stage 26.0 (TID 101). 844 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 2.0 in stage 26.0 (TID 103). 844 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO Executor: Finished task 1.0 in stage 26.0 (TID 102). 844 bytes result sent to driver\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 101) in 7 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 102) in 8 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:08 INFO TaskSetManager: Finished task 2.0 in stage 26.0 (TID 103) in 10 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:08 INFO DAGScheduler: ResultStage 26 (count at cell40.sc:2) finished in 0,019 s\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "24/08/29 15:23:08 INFO DAGScheduler: Job 17 finished: count at cell40.sc:2, took 0,022240 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[51] at parallelize at cell40.sc:1\n",
       "\u001b[36mcountResult\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m9L\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"), 3)\n",
    "val countResult = rdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c02b21-896c-4bf9-bbc7-46b4965a372d",
   "metadata": {},
   "source": [
    "## CountByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63d66c51-0808-4ff0-bc65-5776d1a7f9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:09 INFO SparkContext: Starting job: countByKey at cell41.sc:2\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Registering RDD 53 (countByKey at cell41.sc:2) as input to shuffle 10\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Got job 18 (countByKey at cell41.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Final stage: ResultStage 28 (countByKey at cell41.sc:2)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 27)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[53] at countByKey at cell41.sc:2), which has no missing parents\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ubuntu:35545 (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[53] at countByKey at cell41.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Adding task set 27.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 104) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7659 bytes) \n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 105) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 106) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7679 bytes) \n",
      "24/08/29 15:23:09 INFO Executor: Running task 0.0 in stage 27.0 (TID 104)\n",
      "24/08/29 15:23:09 INFO Executor: Running task 1.0 in stage 27.0 (TID 105)\n",
      "24/08/29 15:23:09 INFO Executor: Running task 2.0 in stage 27.0 (TID 106)\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 0.0 in stage 27.0 (TID 104). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 104) in 20 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 1.0 in stage 27.0 (TID 105). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 2.0 in stage 27.0 (TID 106). 1169 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 105) in 22 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 106) in 23 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:09 INFO DAGScheduler: ShuffleMapStage 27 (countByKey at cell41.sc:2) finished in 0,032 s\n",
      "24/08/29 15:23:09 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/29 15:23:09 INFO DAGScheduler: running: Set()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: waiting: Set(ResultStage 28)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: failed: Set()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[54] at countByKey at cell41.sc:2), which has no missing parents\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ubuntu:35545 (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 28 (ShuffledRDD[54] at countByKey at cell41.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Adding task set 28.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 107) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 108) (ubuntu, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 109) (ubuntu, executor driver, partition 2, NODE_LOCAL, 7433 bytes) \n",
      "24/08/29 15:23:09 INFO Executor: Running task 1.0 in stage 28.0 (TID 108)\n",
      "24/08/29 15:23:09 INFO Executor: Running task 0.0 in stage 28.0 (TID 107)\n",
      "24/08/29 15:23:09 INFO Executor: Running task 2.0 in stage 28.0 (TID 109)\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "24/08/29 15:23:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 1.0 in stage 28.0 (TID 108). 1852 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 108) in 17 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 2.0 in stage 28.0 (TID 109). 1852 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 0.0 in stage 28.0 (TID 107). 1852 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 109) in 25 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 107) in 26 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:09 INFO DAGScheduler: ResultStage 28 (countByKey at cell41.sc:2) finished in 0,034 s\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 18 finished: countByKey at cell41.sc:2, took 0,074083 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = ParallelCollectionRDD[52] at parallelize at cell41.sc:1\n",
       "\u001b[36mcountResult\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m] = \u001b[33mMap\u001b[39m(\u001b[32m\"c\"\u001b[39m -> \u001b[32m1L\u001b[39m, \u001b[32m\"a\"\u001b[39m -> \u001b[32m2L\u001b[39m, \u001b[32m\"b\"\u001b[39m -> \u001b[32m2L\u001b[39m)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 4), (\"b\", 5)), 3)\n",
    "val countResult = rdd.countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd8a17-31b6-47c1-9abd-2aa499bb3e6b",
   "metadata": {},
   "source": [
    "## First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "432d1e77-2625-4e13-a3b8-774d4444422b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:09 INFO BlockManagerInfo: Removed broadcast_27_piece0 on ubuntu:35545 in memory (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Removed broadcast_28_piece0 on ubuntu:35545 in memory (size: 3.0 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Removed broadcast_25_piece0 on ubuntu:35545 in memory (size: 1769.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Removed broadcast_26_piece0 on ubuntu:35545 in memory (size: 1769.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Removed broadcast_24_piece0 on ubuntu:35545 in memory (size: 1948.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO SparkContext: Starting job: first at cell42.sc:2\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Got job 19 (first at cell42.sc:2) with 1 output partitions\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Final stage: ResultStage 29 (first at cell42.sc:2)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting ResultStage 29 (ParallelCollectionRDD[55] at parallelize at cell42.sc:1), which has no missing parents\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 2.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 1777.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ubuntu:35545 (size: 1777.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (ParallelCollectionRDD[55] at parallelize at cell42.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 110) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7623 bytes) \n",
      "24/08/29 15:23:09 INFO Executor: Running task 0.0 in stage 29.0 (TID 110)\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 0.0 in stage 29.0 (TID 110). 810 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 110) in 10 ms on ubuntu (executor driver) (1/1)\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:09 INFO DAGScheduler: ResultStage 29 (first at cell42.sc:2) finished in 0,017 s\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 19 finished: first at cell42.sc:2, took 0,022588 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[55] at parallelize at cell42.sc:1\n",
       "\u001b[36mfirstResult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"a\"\u001b[39m"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"), 3)\n",
    "val firstResult = rdd.first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea482e-f869-4159-88b9-e0b06f78babf",
   "metadata": {},
   "source": [
    "## Take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20d9da58-5df0-45f6-a256-0e5eb26fd656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:09 INFO SparkContext: Starting job: take at cell43.sc:2\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Got job 20 (take at cell43.sc:2) with 1 output partitions\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Final stage: ResultStage 30 (take at cell43.sc:2)\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting ResultStage 30 (ParallelCollectionRDD[56] at parallelize at cell43.sc:1), which has no missing parents\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 2.9 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1794.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ubuntu:35545 (size: 1794.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:09 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (ParallelCollectionRDD[56] at parallelize at cell43.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 111) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:09 INFO Executor: Running task 0.0 in stage 30.0 (TID 111)\n",
      "24/08/29 15:23:09 INFO Executor: Finished task 0.0 in stage 30.0 (TID 111). 801 bytes result sent to driver\n",
      "24/08/29 15:23:09 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 111) in 5 ms on ubuntu (executor driver) (1/1)\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:09 INFO DAGScheduler: ResultStage 30 (take at cell43.sc:2) finished in 0,011 s\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "24/08/29 15:23:09 INFO DAGScheduler: Job 20 finished: take at cell43.sc:2, took 0,015203 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[56] at parallelize at cell43.sc:1\n",
       "\u001b[36mtakeResult\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\n",
    "val takeResult = rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18f8cd-99c4-408e-8f89-8a598ce80786",
   "metadata": {},
   "source": [
    "## TakeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f8176f4-4d08-41ee-96d4-ba774cc6a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO SparkContext: Starting job: takeSample at cell44.sc:2\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Got job 21 (takeSample at cell44.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Final stage: ResultStage 31 (takeSample at cell44.sc:2)\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting ResultStage 31 (ParallelCollectionRDD[57] at parallelize at cell44.sc:1), which has no missing parents\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 2.8 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1783.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ubuntu:35545 (size: 1783.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 31 (ParallelCollectionRDD[57] at parallelize at cell44.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Adding task set 31.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 112) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 113) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 114) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO Executor: Running task 1.0 in stage 31.0 (TID 113)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 0.0 in stage 31.0 (TID 112)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 2.0 in stage 31.0 (TID 114)\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 1.0 in stage 31.0 (TID 113). 844 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 2.0 in stage 31.0 (TID 114). 844 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 0.0 in stage 31.0 (TID 112). 844 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 113) in 8 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 114) in 10 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 112) in 13 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:10 INFO DAGScheduler: ResultStage 31 (takeSample at cell44.sc:2) finished in 0,023 s\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 21 finished: takeSample at cell44.sc:2, took 0,035686 s\n",
      "24/08/29 15:23:10 INFO SparkContext: Starting job: takeSample at cell44.sc:2\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Got job 22 (takeSample at cell44.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Final stage: ResultStage 32 (takeSample at cell44.sc:2)\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting ResultStage 32 (PartitionwiseSampledRDD[58] at takeSample at cell44.sc:2), which has no missing parents\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 3.6 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ubuntu:35545 (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 32 (PartitionwiseSampledRDD[58] at takeSample at cell44.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Adding task set 32.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 115) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7685 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 116) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7685 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 2.0 in stage 32.0 (TID 117) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7685 bytes) \n",
      "24/08/29 15:23:10 INFO Executor: Running task 2.0 in stage 32.0 (TID 117)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 0.0 in stage 32.0 (TID 115)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 1.0 in stage 32.0 (TID 116)\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 0.0 in stage 32.0 (TID 115). 801 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 2.0 in stage 32.0 (TID 117). 801 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 115) in 8 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 2.0 in stage 32.0 (TID 117) in 7 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 1.0 in stage 32.0 (TID 116). 844 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 116) in 11 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:10 INFO DAGScheduler: ResultStage 32 (takeSample at cell44.sc:2) finished in 0,020 s\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 22 finished: takeSample at cell44.sc:2, took 0,028607 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[57] at parallelize at cell44.sc:1\n",
       "\u001b[36mtakeResult\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m8\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m1\u001b[39m)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\n",
    "val takeResult = rdd.takeSample(false, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2729b-1454-4385-b6c5-f94e34c7e608",
   "metadata": {},
   "source": [
    "## TakeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45fba7e9-79bd-4a3d-b6c4-d386d940dd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO SparkContext: Starting job: takeOrdered at cell45.sc:2\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Got job 23 (takeOrdered at cell45.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Final stage: ResultStage 33 (takeOrdered at cell45.sc:2)\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[60] at takeOrdered at cell45.sc:2), which has no missing parents\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 4.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ubuntu:35545 (size: 2.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 33 (MapPartitionsRDD[60] at takeOrdered at cell45.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Adding task set 33.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 118) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 119) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 2.0 in stage 33.0 (TID 120) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO Executor: Running task 2.0 in stage 33.0 (TID 120)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 0.0 in stage 33.0 (TID 118)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 1.0 in stage 33.0 (TID 119)\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 0.0 in stage 33.0 (TID 118). 927 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 1.0 in stage 33.0 (TID 119). 927 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 2.0 in stage 33.0 (TID 120). 927 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 2.0 in stage 33.0 (TID 120) in 19 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 119) in 20 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 118) in 21 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:10 INFO DAGScheduler: ResultStage 33 (takeOrdered at cell45.sc:2) finished in 0,039 s\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 23 finished: takeOrdered at cell45.sc:2, took 0,042580 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[59] at parallelize at cell45.sc:1\n",
       "\u001b[36mtakeResult\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 9, 2, 8, 3, 7, 4, 6, 5), 3)\n",
    "val takeResult = rdd.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c3a4-97e3-49d1-a9aa-506a17478614",
   "metadata": {},
   "source": [
    "## Foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b08c5983-b086-4613-912f-673b2fe5d091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO SparkContext: Starting job: foreach at cell46.sc:2\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Got job 24 (foreach at cell46.sc:2) with 3 output partitions\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Final stage: ResultStage 34 (foreach at cell46.sc:2)\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting ResultStage 34 (ParallelCollectionRDD[61] at parallelize at cell46.sc:1), which has no missing parents\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 3.1 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 1965.0 B, free 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on ubuntu:35545 (size: 1965.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 34 (ParallelCollectionRDD[61] at parallelize at cell46.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Adding task set 34.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 121) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 122) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO TaskSetManager: Starting task 2.0 in stage 34.0 (TID 123) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:10 INFO Executor: Running task 0.0 in stage 34.0 (TID 121)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 1.0 in stage 34.0 (TID 122)\n",
      "24/08/29 15:23:10 INFO Executor: Running task 2.0 in stage 34.0 (TID 123)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next item is 1\n",
      "Next item is 4\n",
      "Next item is 5\n",
      "Next item is 6\n",
      "Next item is 7\n",
      "Next item is 8\n",
      "Next item is 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO Executor: Finished task 1.0 in stage 34.0 (TID 122). 849 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO Executor: Finished task 2.0 in stage 34.0 (TID 123). 849 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next item is 2\n",
      "Next item is 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO Executor: Finished task 0.0 in stage 34.0 (TID 121). 849 bytes result sent to driver\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 122) in 11 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 2.0 in stage 34.0 (TID 123) in 12 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:10 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 121) in 15 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:10 INFO DAGScheduler: ResultStage 34 (foreach at cell46.sc:2) finished in 0,020 s\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "24/08/29 15:23:10 INFO DAGScheduler: Job 24 finished: foreach at cell46.sc:2, took 0,023553 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[61] at parallelize at cell46.sc:1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\n",
    "rdd.foreach { e => println(s\"Next item is $e\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309ef79-22fb-4cdf-b9bd-4057c31b71e4",
   "metadata": {},
   "source": [
    "## SaveAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a34a0f86-e579-4daf-86f0-79f63f06346e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_32_piece0 on ubuntu:35545 in memory (size: 2.2 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_33_piece0 on ubuntu:35545 in memory (size: 2.3 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_30_piece0 on ubuntu:35545 in memory (size: 1794.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_31_piece0 on ubuntu:35545 in memory (size: 1783.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_34_piece0 on ubuntu:35545 in memory (size: 1965.0 B, free: 4.5 GiB)\n",
      "24/08/29 15:23:10 INFO BlockManagerInfo: Removed broadcast_29_piece0 on ubuntu:35545 in memory (size: 1777.0 B, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hadoop.fs._\u001b[39m"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6905bf9e-fdb4-4b30-a201-37573bbbc1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfs\u001b[39m: \u001b[32mFileSystem\u001b[39m = org.apache.hadoop.fs.LocalFileSystem@6bc0d746\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdeleteFile\u001b[39m"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)\n",
    "\n",
    "def deleteFile(fs: FileSystem, file: String): Unit = {\n",
    "    if (fs.exists(new Path(file)))\n",
    "      fs.delete(new Path(file), true)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fecb859b-e2ab-4fad-a38e-2c096517f016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtextFile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"result/txt/textFile\"\u001b[39m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = \"result/txt/textFile\"\n",
    "deleteFile(fs, textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8987cc0b-c9f0-460d-a866-54b0559fe09c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:11 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "24/08/29 15:23:11 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:11 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Got job 25 (runJob at SparkHadoopWriter.scala:83) with 3 output partitions\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Final stage: ResultStage 35 (runJob at SparkHadoopWriter.scala:83)\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[63] at saveAsTextFile at cell50.sc:2), which has no missing parents\n",
      "24/08/29 15:23:11 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 101.2 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:11 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:11 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on ubuntu:35545 (size: 36.5 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:11 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:11 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 35 (MapPartitionsRDD[63] at saveAsTextFile at cell50.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:11 INFO TaskSchedulerImpl: Adding task set 35.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:11 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 124) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:11 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 125) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:11 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 126) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:11 INFO Executor: Running task 0.0 in stage 35.0 (TID 124)\n",
      "24/08/29 15:23:11 INFO Executor: Running task 1.0 in stage 35.0 (TID 125)\n",
      "24/08/29 15:23:11 INFO Executor: Running task 2.0 in stage 35.0 (TID 126)\n",
      "24/08/29 15:23:11 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:11 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:11 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523118495375435276931083_0063_m_000000_0' to file:/home/vadim/workspace/Spark/RDD/result/txt/textFile/_temporary/0/task_202408291523118495375435276931083_0063_m_000000\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523118495375435276931083_0063_m_000001_0' to file:/home/vadim/workspace/Spark/RDD/result/txt/textFile/_temporary/0/task_202408291523118495375435276931083_0063_m_000001\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523118495375435276931083_0063_m_000002_0' to file:/home/vadim/workspace/Spark/RDD/result/txt/textFile/_temporary/0/task_202408291523118495375435276931083_0063_m_000002\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523118495375435276931083_0063_m_000001_0: Committed. Elapsed time: 1 ms.\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523118495375435276931083_0063_m_000002_0: Committed. Elapsed time: 1 ms.\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523118495375435276931083_0063_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 0.0 in stage 35.0 (TID 124). 1170 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 2.0 in stage 35.0 (TID 126). 1213 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 1.0 in stage 35.0 (TID 125). 1213 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 124) in 118 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 126) in 118 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 125) in 119 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:12 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:12 INFO DAGScheduler: ResultStage 35 (runJob at SparkHadoopWriter.scala:83) finished in 0,139 s\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Job 25 finished: runJob at SparkHadoopWriter.scala:83, took 0,142988 s\n",
      "24/08/29 15:23:12 INFO SparkHadoopWriter: Start to commit write Job job_202408291523118495375435276931083_0063.\n",
      "24/08/29 15:23:12 INFO SparkHadoopWriter: Write Job job_202408291523118495375435276931083_0063 committed. Elapsed time: 19 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[62] at parallelize at cell50.sc:1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\n",
    "rdd.saveAsTextFile(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f197ab4c-0731-46e8-abc1-04871bfeba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mobjectFile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"result/object/objectFile\"\u001b[39m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val objectFile = \"result/object/objectFile\"\n",
    "deleteFile(fs, objectFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04201e3f-3668-46f4-8434-1ca2d74dcf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 15:23:12 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)\n",
      "24/08/29 15:23:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:12 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Got job 26 (runJob at SparkHadoopWriter.scala:83) with 3 output partitions\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Final stage: ResultStage 36 (runJob at SparkHadoopWriter.scala:83)\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[66] at saveAsObjectFile at cell52.sc:2), which has no missing parents\n",
      "24/08/29 15:23:12 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 101.7 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:12 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 4.5 GiB)\n",
      "24/08/29 15:23:12 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on ubuntu:35545 (size: 36.7 KiB, free: 4.5 GiB)\n",
      "24/08/29 15:23:12 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1580\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 36 (MapPartitionsRDD[66] at saveAsObjectFile at cell52.sc:2) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/08/29 15:23:12 INFO TaskSchedulerImpl: Adding task set 36.0 with 3 tasks resource profile 0\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 127) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:12 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 128) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:12 INFO TaskSetManager: Starting task 2.0 in stage 36.0 (TID 129) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7576 bytes) \n",
      "24/08/29 15:23:12 INFO Executor: Running task 0.0 in stage 36.0 (TID 127)\n",
      "24/08/29 15:23:12 INFO Executor: Running task 2.0 in stage 36.0 (TID 129)\n",
      "24/08/29 15:23:12 INFO Executor: Running task 1.0 in stage 36.0 (TID 128)\n",
      "24/08/29 15:23:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523129037134906193361948_0066_m_000000_0' to file:/home/vadim/workspace/Spark/RDD/result/object/objectFile/_temporary/0/task_202408291523129037134906193361948_0066_m_000000\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523129037134906193361948_0066_m_000000_0: Committed. Elapsed time: 0 ms.\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523129037134906193361948_0066_m_000002_0' to file:/home/vadim/workspace/Spark/RDD/result/object/objectFile/_temporary/0/task_202408291523129037134906193361948_0066_m_000002\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523129037134906193361948_0066_m_000002_0: Committed. Elapsed time: 0 ms.\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 2.0 in stage 36.0 (TID 129). 1170 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 0.0 in stage 36.0 (TID 127). 1170 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202408291523129037134906193361948_0066_m_000001_0' to file:/home/vadim/workspace/Spark/RDD/result/object/objectFile/_temporary/0/task_202408291523129037134906193361948_0066_m_000001\n",
      "24/08/29 15:23:12 INFO SparkHadoopMapRedUtil: attempt_202408291523129037134906193361948_0066_m_000001_0: Committed. Elapsed time: 2 ms.\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 2.0 in stage 36.0 (TID 129) in 75 ms on ubuntu (executor driver) (1/3)\n",
      "24/08/29 15:23:12 INFO Executor: Finished task 1.0 in stage 36.0 (TID 128). 1170 bytes result sent to driver\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 127) in 78 ms on ubuntu (executor driver) (2/3)\n",
      "24/08/29 15:23:12 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 128) in 79 ms on ubuntu (executor driver) (3/3)\n",
      "24/08/29 15:23:12 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "24/08/29 15:23:12 INFO DAGScheduler: ResultStage 36 (runJob at SparkHadoopWriter.scala:83) finished in 0,098 s\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/29 15:23:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished\n",
      "24/08/29 15:23:12 INFO DAGScheduler: Job 26 finished: runJob at SparkHadoopWriter.scala:83, took 0,103068 s\n",
      "24/08/29 15:23:12 INFO SparkHadoopWriter: Start to commit write Job job_202408291523129037134906193361948_0066.\n",
      "24/08/29 15:23:12 INFO SparkHadoopWriter: Write Job job_202408291523129037134906193361948_0066 committed. Elapsed time: 13 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[64] at parallelize at cell52.sc:1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\n",
    "rdd.saveAsObjectFile(objectFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92ce13-e920-4aba-bae4-b02cd1f419ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
