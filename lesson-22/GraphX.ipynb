{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d410d1-fe5b-4688-836a-7db0858d109e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.5.6`\n",
    "import $ivy.`org.apache.spark::spark-graphx:3.5.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96505ad-2f40-4ba1-b231-6ffa0b601d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f38fb6-b0d8-45b9-9785-8d841ef910cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f031c77-765f-4591-9060-fcadadb1614b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/13 14:29:20 INFO SparkContext: Running Spark version 3.5.6\n",
      "26/01/13 14:29:20 INFO SparkContext: OS info Mac OS X, 26.2, aarch64\n",
      "26/01/13 14:29:20 INFO SparkContext: Java version 11.0.29\n",
      "26/01/13 14:29:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"WARN\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@e2a65fc\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@3713e0b9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf()\n",
    "                .setAppName(\"GraphX\")\n",
    "                .setMaster(\"local[*]\")\n",
    "                .set(\"spark.log.level\", \"WARN\")\n",
    "                \n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba9823-8909-4ae6-905e-0bf3fe3f7904",
   "metadata": {},
   "source": [
    "# Создаём граф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb17e720-5853-47ec-a040-867fb99fd3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[0] at parallelize at cmd5.sc:3\n",
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = ParallelCollectionRDD[1] at parallelize at cmd5.sc:9\n",
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@36edae54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Seq((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\")),\n",
    "                       (4L, (\"peter\", \"student\"))))\n",
    "\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Seq(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\"),\n",
    "                       Edge(4L, 0L, \"student\"),   Edge(5L, 0L, \"colleague\")))\n",
    "\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a74ffb-47d1-4ec7-820b-b35f0e656540",
   "metadata": {},
   "source": [
    "# Операции с графами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d6072-0315-4f4a-9b4c-1edc742913b7",
   "metadata": {},
   "source": [
    "## Information about the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01a797e-ae7e-4bba-a763-20aa69174375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numEdges: 6\n",
      "numVertices: 6\n"
     ]
    }
   ],
   "source": [
    "println(s\"numEdges: ${graph.numEdges}\\nnumVertices: ${graph.numVertices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e459047f-41ff-46f3-a126-070a5de7259f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m0L\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[32m3L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m5L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m7L\u001b[39m, \u001b[32m2\u001b[39m))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.inDegrees.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0756b11f-3007-4130-b58f-578f7f66ae2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m2L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m3L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m4L\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m5L\u001b[39m, \u001b[32m3\u001b[39m))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.outDegrees.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47be36b2-5e90-40d1-838f-df788072b1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m2L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m3L\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m4L\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m5L\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  (\u001b[32m7L\u001b[39m, \u001b[32m2\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.degrees.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea459b4-3e0e-4f0f-b4a9-724dab3f8927",
   "metadata": {},
   "source": [
    "## Views of the graph as collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bfdfb0-3cce-4696-a9db-87a0c57be21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,(John Doe,Missing))\n",
      "(2,(istoica,prof))\n",
      "(3,(rxin,student))\n",
      "(4,(peter,student))\n",
      "(5,(franklin,prof))\n",
      "(7,(jgonzal,postdoc))\n"
     ]
    }
   ],
   "source": [
    "graph.vertices.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5699de92-bc25-4f0b-a1d5-4df7587ffef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)),\n",
       "  (\u001b[32m2L\u001b[39m, (\u001b[32m\"istoica\"\u001b[39m, \u001b[32m\"prof\"\u001b[39m)),\n",
       "  (\u001b[32m3L\u001b[39m, (\u001b[32m\"rxin\"\u001b[39m, \u001b[32m\"student\"\u001b[39m)),\n",
       "  (\u001b[32m4L\u001b[39m, (\u001b[32m\"peter\"\u001b[39m, \u001b[32m\"student\"\u001b[39m)),\n",
       "  (\u001b[32m5L\u001b[39m, (\u001b[32m\"franklin\"\u001b[39m, \u001b[32m\"prof\"\u001b[39m)),\n",
       "  (\u001b[32m7L\u001b[39m, (\u001b[32m\"jgonzal\"\u001b[39m, \u001b[32m\"postdoc\"\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e1cb3e-c57e-4f3c-8991-215142678a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge(3,7,collab)\n",
      "Edge(5,3,advisor)\n",
      "Edge(2,5,colleague)\n",
      "Edge(5,7,pi)\n",
      "Edge(4,0,student)\n",
      "Edge(5,0,colleague)\n"
     ]
    }
   ],
   "source": [
    "graph.edges.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1056fa5d-8115-4bae-8092-5b88401ce4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m3L\u001b[39m, \u001b[32m7L\u001b[39m, \u001b[32m\"collab\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m3L\u001b[39m, \u001b[32m\"advisor\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m2L\u001b[39m, \u001b[32m5L\u001b[39m, \u001b[32m\"colleague\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m7L\u001b[39m, \u001b[32m\"pi\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m4L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m\"student\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m\"colleague\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2322d797-df1b-47aa-a2ff-7c6162a2689f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3,(rxin,student)),(7,(jgonzal,postdoc)),collab)\n",
      "((5,(franklin,prof)),(3,(rxin,student)),advisor)\n",
      "((2,(istoica,prof)),(5,(franklin,prof)),colleague)\n",
      "((5,(franklin,prof)),(7,(jgonzal,postdoc)),pi)\n",
      "((4,(peter,student)),(0,(John Doe,Missing)),student)\n",
      "((5,(franklin,prof)),(0,(John Doe,Missing)),colleague)\n"
     ]
    }
   ],
   "source": [
    "graph.triplets.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473a301d-070a-4f5e-99da-2b64518d0cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mEdgeTriplet\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m3L\u001b[39m, \u001b[32m7L\u001b[39m, \u001b[32m\"collab\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m3L\u001b[39m, \u001b[32m\"advisor\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m2L\u001b[39m, \u001b[32m5L\u001b[39m, \u001b[32m\"colleague\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m7L\u001b[39m, \u001b[32m\"pi\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m4L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m\"student\"\u001b[39m),\n",
       "  \u001b[33mEdge\u001b[39m(\u001b[32m5L\u001b[39m, \u001b[32m0L\u001b[39m, \u001b[32m\"colleague\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.triplets.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be043ed-d47e-4e40-b99a-1c9f3b0dda02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n",
      "peter is the student of John Doe\n",
      "franklin is the colleague of John Doe\n"
     ]
    }
   ],
   "source": [
    "graph.triplets.map(\n",
    "  triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    ").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1928f55-f22a-4234-ae85-6692f552c7cc",
   "metadata": {},
   "source": [
    "## Structural Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885ba909-26a1-4087-8ac2-c427f3f885a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@1fa82dd9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad65066d-c922-4a6e-97f4-a9582d47f2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,(istoica,prof))\n",
      "(3,(rxin,student))\n",
      "(4,(peter,student))\n",
      "(5,(franklin,prof))\n",
      "(7,(jgonzal,postdoc))\n"
     ]
    }
   ],
   "source": [
    "// The valid subgraph will disconnect users 4 and 5 by removing user 0\n",
    "validGraph.vertices.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6199dd6-33f2-47bd-84c2-56bb29e46a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n"
     ]
    }
   ],
   "source": [
    "validGraph.triplets.map(\n",
    "  triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    ").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "033bb587-97e0-48a5-acf5-9df35d2aa972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.EdgeDirection._\u001b[39m\n",
       "\u001b[36mres20_1\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mVertexId\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mVertexId\u001b[39m])] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, \u001b[33mArray\u001b[39m(\u001b[32m4L\u001b[39m, \u001b[32m5L\u001b[39m)),\n",
       "  (\u001b[32m2L\u001b[39m, \u001b[33mArray\u001b[39m()),\n",
       "  (\u001b[32m3L\u001b[39m, \u001b[33mArray\u001b[39m(\u001b[32m5L\u001b[39m)),\n",
       "  (\u001b[32m4L\u001b[39m, \u001b[33mArray\u001b[39m()),\n",
       "  (\u001b[32m5L\u001b[39m, \u001b[33mArray\u001b[39m(\u001b[32m2L\u001b[39m)),\n",
       "  (\u001b[32m7L\u001b[39m, \u001b[33mArray\u001b[39m(\u001b[32m3L\u001b[39m, \u001b[32m5L\u001b[39m))\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.EdgeDirection._\n",
    "\n",
    "graph.collectNeighborIds(In).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d82d5-f98d-470a-9891-4dfcbc6128db",
   "metadata": {},
   "source": [
    "# Graph Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8d872-fac2-4f6e-8300-247944a9ca2a",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "\n",
    "GraphX also includes an example social network dataset that we can run PageRank on. A set of users is given in ``data/graphx/users.txt``, and a set of relationships between users is given in ``data/graphx/followers.txt``. We compute the PageRank of each user as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22c53494-3565-4bf1-ac2b-1146bedd2caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/13 14:29:27 WARN BlockManager: Block rdd_267_1 already exists on this machine; not re-adding it\n",
      "26/01/13 14:29:27 WARN BlockManager: Block rdd_347_1 already exists on this machine; not re-adding it\n",
      "26/01/13 14:29:27 WARN BlockManager: Block rdd_363_1 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,0.15007622780470478)\n",
      "(matei_zaharia,0.7017164142469724)\n",
      "(ladygaga,1.3907556008752426)\n",
      "(BarackObama,1.4596227918476916)\n",
      "(jeresig,0.9998520559494657)\n",
      "(odersky,1.2979769092759237)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.GraphLoader\u001b[39m\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@3dc71223\n",
       "\u001b[36mranks\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = VertexRDDImpl[900] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[909] at map at cmd21.sc:10\n",
       "\u001b[36mranksByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = MapPartitionsRDD[913] at map at cmd21.sc:14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.GraphLoader\n",
    "\n",
    "// Load the edges as a graph\n",
    "val graph = GraphLoader.edgeListFile(sc, \"/opt/spark/data/graphx/followers.txt\")\n",
    "\n",
    "// Run PageRank\n",
    "val ranks = graph.pageRank(0.0001).vertices\n",
    "\n",
    "// Join the ranks with the usernames\n",
    "val users = sc.textFile(\"/opt/spark/data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val ranksByUsername = users.join(ranks).map {\n",
    "  case (id, (username, rank)) => (username, rank)\n",
    "}\n",
    "\n",
    "// Print the result\n",
    "println(ranksByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a9706-5c0f-4d6a-8609-4282dd8b4e9b",
   "metadata": {},
   "source": [
    "## Connected Components\n",
    "\n",
    "GraphX contains an implementation of the algorithm in the ConnectedComponents object, and we compute the connected components of the example social network dataset from the PageRank section as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e93e7d5-8179-47a5-8cb8-eaf1dc7815b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/13 14:29:30 WARN BlockManager: Block rdd_925_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,1)\n",
      "(matei_zaharia,3)\n",
      "(ladygaga,1)\n",
      "(BarackObama,1)\n",
      "(jeresig,3)\n",
      "(odersky,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.GraphLoader\u001b[39m\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@598a14cc\n",
       "\u001b[36mcc\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mVertexId\u001b[39m] = VertexRDDImpl[946] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[963] at map at cmd22.sc:10\n",
       "\u001b[36mccByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mVertexId\u001b[39m)] = MapPartitionsRDD[967] at map at cmd22.sc:14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.GraphLoader\n",
    "\n",
    "// Load the graph as in the PageRank example\n",
    "val graph = GraphLoader.edgeListFile(sc, \"/opt/spark/data/graphx/followers.txt\")\n",
    "\n",
    "// Find the connected components\n",
    "val cc = graph.connectedComponents().vertices\n",
    "\n",
    "// Join the connected components with the usernames\n",
    "val users = sc.textFile(\"/opt/spark/data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val ccByUsername = users.join(cc).map {\n",
    "  case (id, (username, cc)) => (username, cc)\n",
    "}\n",
    "\n",
    "// Print the result\n",
    "println(ccByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee10f6-42cb-48a5-97e7-0a19e41c9980",
   "metadata": {},
   "source": [
    "## Triangle Counting\n",
    "\n",
    "We compute the triangle count of the social network dataset from the PageRank section. Note that TriangleCount requires the edges to be in canonical orientation (srcId < dstId) and the graph to be partitioned using Graph.partitionBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "269fbc0e-61f4-4433-8c31-edf3574b882b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/13 14:29:30 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n",
      "26/01/13 14:29:30 WARN ShippableVertexPartitionOps: Joining two VertexPartitions with different indexes is slow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,0)\n",
      "(matei_zaharia,1)\n",
      "(ladygaga,0)\n",
      "(BarackObama,0)\n",
      "(jeresig,1)\n",
      "(odersky,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.{GraphLoader, PartitionStrategy}\u001b[39m\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6b798187\n",
       "\u001b[36mtriCounts\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mInt\u001b[39m] = VertexRDDImpl[1037] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[1042] at map at cmd23.sc:11\n",
       "\u001b[36mtriCountByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = MapPartitionsRDD[1046] at map at cmd23.sc:15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.{GraphLoader, PartitionStrategy}\n",
    "\n",
    "// Load the edges in canonical order and partition the graph for triangle count\n",
    "val graph = GraphLoader.edgeListFile(sc, \"/opt/spark/data/graphx/followers.txt\", true)\n",
    "  .partitionBy(PartitionStrategy.RandomVertexCut)\n",
    "  \n",
    "// Find the triangle count for each vertex\n",
    "val triCounts = graph.triangleCount().vertices\n",
    "\n",
    "// Join the triangle counts with the usernames\n",
    "val users = sc.textFile(\"/opt/spark/data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val triCountByUsername = users.join(triCounts).map { \n",
    "  case (id, (username, tc)) => (username, tc)\n",
    "}\n",
    "\n",
    "// Print the result\n",
    "println(triCountByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310d155-39cf-43e5-b0ff-a5146f56d381",
   "metadata": {},
   "source": [
    "# Pregel\n",
    "\n",
    "We can use the Pregel operator to express computation such as single source shortest path in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba06f752-5a57-41af-b625-00831e86270d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,2.0)\n",
      "(56,2.0)\n",
      "(16,2.0)\n",
      "(80,2.0)\n",
      "(48,2.0)\n",
      "(32,2.0)\n",
      "(0,2.0)\n",
      "(24,2.0)\n",
      "(64,2.0)\n",
      "(40,2.0)\n",
      "(72,2.0)\n",
      "(8,1.0)\n",
      "(88,2.0)\n",
      "(41,1.0)\n",
      "(81,2.0)\n",
      "(97,2.0)\n",
      "(25,2.0)\n",
      "(65,2.0)\n",
      "(73,2.0)\n",
      "(57,2.0)\n",
      "(33,2.0)\n",
      "(1,2.0)\n",
      "(89,2.0)\n",
      "(17,2.0)\n",
      "(9,2.0)\n",
      "(49,2.0)\n",
      "(34,2.0)\n",
      "(82,1.0)\n",
      "(66,2.0)\n",
      "(98,2.0)\n",
      "(50,2.0)\n",
      "(42,0.0)\n",
      "(74,2.0)\n",
      "(90,1.0)\n",
      "(18,1.0)\n",
      "(58,2.0)\n",
      "(26,2.0)\n",
      "(10,2.0)\n",
      "(2,2.0)\n",
      "(19,2.0)\n",
      "(59,2.0)\n",
      "(11,2.0)\n",
      "(35,1.0)\n",
      "(27,2.0)\n",
      "(75,2.0)\n",
      "(51,2.0)\n",
      "(83,1.0)\n",
      "(67,2.0)\n",
      "(3,2.0)\n",
      "(91,1.0)\n",
      "(43,2.0)\n",
      "(99,2.0)\n",
      "(84,2.0)\n",
      "(52,2.0)\n",
      "(4,2.0)\n",
      "(76,1.0)\n",
      "(28,2.0)\n",
      "(36,2.0)\n",
      "(92,2.0)\n",
      "(20,2.0)\n",
      "(12,2.0)\n",
      "(60,2.0)\n",
      "(44,1.0)\n",
      "(68,2.0)\n",
      "(13,2.0)\n",
      "(61,2.0)\n",
      "(21,2.0)\n",
      "(77,2.0)\n",
      "(53,2.0)\n",
      "(29,2.0)\n",
      "(93,2.0)\n",
      "(37,1.0)\n",
      "(45,2.0)\n",
      "(69,1.0)\n",
      "(85,2.0)\n",
      "(5,2.0)\n",
      "(22,2.0)\n",
      "(54,2.0)\n",
      "(46,2.0)\n",
      "(30,2.0)\n",
      "(14,2.0)\n",
      "(62,2.0)\n",
      "(6,2.0)\n",
      "(70,2.0)\n",
      "(38,2.0)\n",
      "(86,2.0)\n",
      "(78,2.0)\n",
      "(94,1.0)\n",
      "(39,2.0)\n",
      "(15,2.0)\n",
      "(47,1.0)\n",
      "(71,2.0)\n",
      "(55,2.0)\n",
      "(95,2.0)\n",
      "(79,2.0)\n",
      "(23,2.0)\n",
      "(63,2.0)\n",
      "(7,2.0)\n",
      "(31,2.0)\n",
      "(87,2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.{Graph, VertexId}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.util.GraphGenerators\u001b[39m\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@59bdbf0c\n",
       "\u001b[36msourceId\u001b[39m: \u001b[32mVertexId\u001b[39m = \u001b[32m42L\u001b[39m\n",
       "\u001b[36minitialGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5f655399\n",
       "\u001b[36msssp\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@7744eeff"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "// A graph with edge attributes containing distances\n",
    "val graph: Graph[Long, Double] = GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e => e.attr.toDouble)\n",
    "\n",
    "val sourceId: VertexId = 42 // The ultimate source\n",
    "\n",
    "// Initialize the graph such that all vertices except the root have distance infinity.\n",
    "val initialGraph = graph.mapVertices((id, _) => if (id == sourceId) 0.0 else Double.PositiveInfinity)\n",
    "\n",
    "val sssp = initialGraph.pregel(Double.PositiveInfinity)(\n",
    "  (id, dist, newDist) => math.min(dist, newDist), // Vertex Program\n",
    "  triplet => {  // Send Message\n",
    "    if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "      Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  },\n",
    "  (a, b) => math.min(a, b) // Merge Message\n",
    ")\n",
    "\n",
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc437c-ee9d-45e5-b9a9-c1e93125e730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
