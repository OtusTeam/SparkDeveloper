{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// neo4j-spark-connector\n",
    "/*\n",
    "import coursierapi._\n",
    "interp.repositories() ++= Seq(MavenRepository.of(\"https://repos.spark-packages.org/\"))\n",
    "interp.load.ivy((\"neo4j\" % \"neo4j-spark-connector\" % \"5.3.3-s_2.12\"))\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session с подключением к Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"SparkNeo4j\")\n",
    "                .config(\"spark.log.level\", \"WARN\")\n",
    "                .config(\"spark.jars.packages\", \"neo4j:neo4j-spark-connector:5.3.3-s_2.12\")\n",
    "                .config(\"neo4j.url\", \"neo4j://localhost:7687\")\n",
    "                .config(\"neo4j.authentication.type\", \"basic\")\n",
    "                .config(\"neo4j.authentication.basic.username\", \"neo4j\")\n",
    "                .config(\"neo4j.authentication.basic.password\", \"password\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Neo4j into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val movies = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val person = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"labels\", \":Person\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTED_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actedin = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"relationship\", \"ACTED_IN\")\n",
    "                .option(\"relationship.source.labels\", \":Person\")\n",
    "                .option(\"relationship.target.labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actedin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actedin.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame with nodes as map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actedinMap = spark.read\n",
    "                    .format(\"org.neo4j.spark.DataSource\")\n",
    "                    .option(\"relationship.nodes.map\", true)\n",
    "                    .option(\"relationship\", \"ACTED_IN\")\n",
    "                    .option(\"relationship.source.labels\", \":Person\")\n",
    "                    .option(\"relationship.target.labels\", \":Movie\")\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actedinMap.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actedinMap.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val directed = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"relationship\", \"DIRECTED\")\n",
    "                .option(\"relationship.source.labels\", \":Person\")\n",
    "                .option(\"relationship.target.labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read arbitrary data via Cypher query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cypher = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"query\", \"\"\"\n",
    "                    // Extend Tom Hanks co-actors, to find co-co-actors who haven't worked with Tom Hanks\n",
    "                    MATCH (tom:Person {name:\"Tom Hanks\"})-[:ACTED_IN]->(m)<-[:ACTED_IN]-(coActors),\n",
    "                    (coActors)-[:ACTED_IN]->(m2)<-[:ACTED_IN]-(cocoActors)\n",
    "                    WHERE NOT (tom)-[:ACTED_IN]->()<-[:ACTED_IN]-(cocoActors)\n",
    "                        AND tom <> cocoActors\n",
    "                    RETURN cocoActors.name AS Recommended, count(*) AS Strength\n",
    "                    ORDER BY Strength DESC\n",
    "                    \"\"\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all the actors that have also directed a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actorsDirectors = spark.read\n",
    "                        .format(\"org.neo4j.spark.DataSource\")\n",
    "                        .option(\"query\", \"\"\"\n",
    "                                MATCH (p:Person)\n",
    "                                MATCH (p)-[:ACTED_IN]->(m:Movie)\n",
    "                                MATCH (p)-[:DIRECTED]->(m1:Movie)\n",
    "                                RETURN p.name AS name, collect(m.title) AS acted_in, collect(m1.title) AS directed\n",
    "                                \"\"\")\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorsDirectors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorsDirectors.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data from Spark to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val products = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"inferSchema\", true)\n",
    "                .load(\"desktop-csv-import/products.csv\")\n",
    "                .withColumnsRenamed(Map(\"_c0\" -> \"id\", \"_c1\" -> \"name\", \"_c2\" -> \"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"inferSchema\", true)\n",
    "                .option(\"header\", true)\n",
    "                .load(\"desktop-csv-import/orders.csv\")\n",
    "                .selectExpr(\"orderID AS id\", \"CAST(orderDate AS TIMESTAMP) AS date\", \"shipCountry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write nodes via label option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"labels\", \":Product\")\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write\n",
    "      .format(\"org.neo4j.spark.DataSource\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"labels\", \":Order\")\n",
    "      .option(\"schema.optimization.type\", \"NODE_CONSTRAINTS\")\n",
    "// this is necessary in order to specify what is the constraint field\n",
    "      .option(\"node.keys\", \"id\")\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write relationships via relationship option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orderDetails = spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"inferSchema\", true)\n",
    "                    .option(\"header\", true)\n",
    "                    .load(\"desktop-csv-import/order-details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDetails.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDetails.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDetails.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDetails.write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"relationship\", \"CONTAINS\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.source.labels\", \":Product\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.source.node.keys\", \"productID:id\")\n",
    "            .option(\"relationship.target.labels\", \":Order\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.node.keys\", \"orderID:id\")\n",
    "            .option(\"relationship.properties\", \"quantity:quantityOrdered\")\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write custom graphs via Cypher Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actorOrders = Seq(\n",
    "  (\"Cuba Gooding Jr.\", 1, Array(11, 42, 72), Array(1, 2, 3), \"2022-06-07 00:00:00\"),\n",
    "  (\"Tom Hanks\", 2, Array(24, 55, 75), Array(3, 2, 1), \"2022-06-06 00:00:00\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actorOrdersDF = spark\n",
    "                    .createDataFrame(actorOrders)\n",
    "                    .withColumnsRenamed(\n",
    "                        Map(\n",
    "                            \"_1\" -> \"actor_name\",\n",
    "                            \"_2\" -> \"order_id\",\n",
    "                            \"_3\" -> \"products\",\n",
    "                            \"_4\" -> \"quantities\",\n",
    "                            \"_5\" -> \"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorOrdersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorOrdersDF.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorOrdersDF.write\n",
    "             .format(\"org.neo4j.spark.DataSource\")\n",
    "             .mode(\"overwrite\")\n",
    "             .option(\"query\", \"\"\"\n",
    "                     MATCH (person:Person {name: event.actor_name})\n",
    "                     MERGE (order:Order {id: event.order_id, date: datetime(replace(event.order_date, ' ', 'T'))})\n",
    "                     MERGE (person)-[:CREATED]->(order)\n",
    "                     WITH event, order\n",
    "                     UNWIND range(0, size(event.products) - 1) AS index\n",
    "                     MATCH (product:Product {id: event.products[index]})\n",
    "                     MERGE (product)-[:CONTAINS{quantityOrdered: event.quantities[index]}]->(order)\n",
    "                    \"\"\")\n",
    "             .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
