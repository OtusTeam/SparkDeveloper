{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:12:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on Air.local:51188 in memory (size: 34.1 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:12:00 INFO BlockManagerInfo: Removed broadcast_44_piece0 on Air.local:51188 in memory (size: 11.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:12:00 INFO BlockManagerInfo: Removed broadcast_42_piece0 on Air.local:51188 in memory (size: 7.4 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// neo4j-spark-connector\n",
    "/*\n",
    "import coursierapi._\n",
    "interp.repositories() ++= Seq(MavenRepository.of(\"https://repos.spark-packages.org/\"))\n",
    "interp.load.ivy((\"neo4j\" % \"neo4j-spark-connector\" % \"5.3.3-s_2.12\"))\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session с подключением к Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:12:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1307135\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"SparkNeo4j\")\n",
    "                .config(\"spark.jars.packages\", \"neo4j:neo4j-spark-connector:5.3.3-s_2.12\")\n",
    "                .config(\"neo4j.url\", \"neo4j://localhost:7687\")\n",
    "                .config(\"neo4j.authentication.type\", \"basic\")\n",
    "                .config(\"neo4j.authentication.basic.username\", \"neo4j\")\n",
    "                .config(\"neo4j.authentication.basic.password\", \"password\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Neo4j into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:12:09 INFO DriverFactory: Routing driver instance 208498087 created for server address localhost:7687\n",
      "25/02/02 23:12:09 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmovies\u001b[39m: \u001b[32mDataFrame\u001b[39m = [<id>: bigint, <labels>: array<string> ... 3 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <id>: long (nullable = false)\n",
      " |-- <labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- released: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:04 INFO InternalDriver: Closing driver instance 208498087\n",
      "25/02/02 23:13:04 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:04 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:04 INFO V2ScanRelationPushDown: \n",
      "Output: <id>#558L, <labels>#559, title#560, tagline#561, released#562L\n",
      "         \n",
      "25/02/02 23:13:04 INFO DriverFactory: Routing driver instance 692435674 created for server address localhost:7687\n",
      "25/02/02 23:13:04 INFO SparkContext: Starting job: show at cmd54.sc:1\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Got job 28 (show at cmd54.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Final stage: ResultStage 31 (show at cmd54.sc:1)\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[110] at show at cmd54.sc:1), which has no missing parents\n",
      "25/02/02 23:13:04 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 18.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:04 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:04 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on Air.local:51188 (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:04 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[110] at show at cmd54.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:04 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:04 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 29) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:04 INFO Executor: Running task 0.0 in stage 31.0 (TID 29)\n",
      "25/02/02 23:13:04 INFO Neo4jPartitionReader: Running the following query on Neo4j: MATCH (n:`Movie`) RETURN id(n) AS `<id>`, labels(n) AS `<labels>`, n.title AS title, n.tagline AS tagline, n.released AS released LIMIT 11\n",
      "25/02/02 23:13:04 INFO InternalDriver: Closing driver instance 692435674\n",
      "25/02/02 23:13:04 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:04 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:04 INFO Executor: Finished task 0.0 in stage 31.0 (TID 29). 2561 bytes result sent to driver\n",
      "25/02/02 23:13:04 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 29) in 322 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:04 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:04 INFO DAGScheduler: ResultStage 31 (show at cmd54.sc:1) finished in 0,330 s\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "25/02/02 23:13:04 INFO DAGScheduler: Job 28 finished: show at cmd54.sc:1, took 0,333737 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|<id>|<labels>|title                 |tagline                                                                                                                                                                      |released|\n",
      "+----+--------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|0   |[Movie] |The Matrix            |Welcome to the Real World                                                                                                                                                    |1999    |\n",
      "|9   |[Movie] |The Matrix Reloaded   |Free your mind                                                                                                                                                               |2003    |\n",
      "|10  |[Movie] |The Matrix Revolutions|Everything that has a beginning has an end                                                                                                                                   |2003    |\n",
      "|11  |[Movie] |The Devil's Advocate  |Evil has its winning ways                                                                                                                                                    |1997    |\n",
      "|15  |[Movie] |A Few Good Men        |In the heart of the nation's capital, in a courthouse of the U.S. government, one man will stop at nothing to keep his honor, and one will stop at nothing to find the truth.|1992    |\n",
      "|29  |[Movie] |Top Gun               |I feel the need, the need for speed.                                                                                                                                         |1986    |\n",
      "|37  |[Movie] |Jerry Maguire         |The rest of his life begins now.                                                                                                                                             |2000    |\n",
      "|46  |[Movie] |Stand By Me           |For some, it's the last real taste of innocence, and the first real taste of life. But for everyone, it's the time that memories are made of.                                |1986    |\n",
      "|52  |[Movie] |As Good as It Gets    |A comedy from the heart that goes for the throat.                                                                                                                            |1997    |\n",
      "|56  |[Movie] |What Dreams May Come  |After life there is more. The end is just the beginning.                                                                                                                     |1998    |\n",
      "+----+--------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:04 INFO BlockManagerInfo: Removed broadcast_45_piece0 on Air.local:51188 in memory (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:04 INFO DriverFactory: Routing driver instance 185520045 created for server address localhost:7687\n",
      "25/02/02 23:13:04 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mperson\u001b[39m: \u001b[32mDataFrame\u001b[39m = [<id>: bigint, <labels>: array<string> ... 2 more fields]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"labels\", \":Person\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <id>: long (nullable = false)\n",
      " |-- <labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- born: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:05 INFO InternalDriver: Closing driver instance 185520045\n",
      "25/02/02 23:13:05 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:05 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:05 INFO V2ScanRelationPushDown: \n",
      "Output: <id>#594L, <labels>#595, name#596, born#597L\n",
      "         \n",
      "25/02/02 23:13:05 INFO DriverFactory: Routing driver instance 481946499 created for server address localhost:7687\n",
      "25/02/02 23:13:05 INFO SparkContext: Starting job: show at cmd57.sc:1\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Got job 29 (show at cmd57.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Final stage: ResultStage 32 (show at cmd57.sc:1)\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[114] at show at cmd57.sc:1), which has no missing parents\n",
      "25/02/02 23:13:05 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 18.0 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:05 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:05 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on Air.local:51188 (size: 8.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:05 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[114] at show at cmd57.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:05 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:05 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 30) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:05 INFO Executor: Running task 0.0 in stage 32.0 (TID 30)\n",
      "25/02/02 23:13:05 INFO Neo4jPartitionReader: Running the following query on Neo4j: MATCH (n:`Person`) RETURN id(n) AS `<id>`, labels(n) AS `<labels>`, n.name AS name, n.born AS born LIMIT 11\n",
      "25/02/02 23:13:05 INFO InternalDriver: Closing driver instance 481946499\n",
      "25/02/02 23:13:05 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:05 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:05 INFO Executor: Finished task 0.0 in stage 32.0 (TID 30). 1907 bytes result sent to driver\n",
      "25/02/02 23:13:05 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 30) in 262 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:05 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:05 INFO DAGScheduler: ResultStage 32 (show at cmd57.sc:1) finished in 0,265 s\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "25/02/02 23:13:05 INFO DAGScheduler: Job 29 finished: show at cmd57.sc:1, took 0,268181 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+----+\n",
      "|<id>|<labels>|name              |born|\n",
      "+----+--------+------------------+----+\n",
      "|1   |[Person]|Keanu Reeves      |1964|\n",
      "|2   |[Person]|Carrie-Anne Moss  |1967|\n",
      "|3   |[Person]|Laurence Fishburne|1961|\n",
      "|4   |[Person]|Hugo Weaving      |1960|\n",
      "|5   |[Person]|Lilly Wachowski   |1967|\n",
      "|6   |[Person]|Lana Wachowski    |1965|\n",
      "|7   |[Person]|Joel Silver       |1952|\n",
      "|8   |[Person]|Emil Eifrem       |1978|\n",
      "|12  |[Person]|Charlize Theron   |1975|\n",
      "|13  |[Person]|Al Pacino         |1940|\n",
      "+----+--------+------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTED_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:05 INFO BlockManagerInfo: Removed broadcast_46_piece0 on Air.local:51188 in memory (size: 8.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:05 INFO DriverFactory: Routing driver instance 1848875431 created for server address localhost:7687\n",
      "25/02/02 23:13:05 WARN SchemaService: Switching to query schema resolution\n",
      "25/02/02 23:13:05 WARN SchemaService: Switching to query schema resolution\n",
      "25/02/02 23:13:05 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mactedin\u001b[39m: \u001b[32mDataFrame\u001b[39m = [<rel.id>: bigint, <rel.type>: string ... 10 more fields]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actedin = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"relationship\", \"ACTED_IN\")\n",
    "                .option(\"relationship.source.labels\", \":Person\")\n",
    "                .option(\"relationship.target.labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <rel.id>: long (nullable = false)\n",
      " |-- <rel.type>: string (nullable = false)\n",
      " |-- <source.id>: long (nullable = false)\n",
      " |-- <source.labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- source.name: string (nullable = true)\n",
      " |-- source.born: long (nullable = true)\n",
      " |-- <target.id>: long (nullable = false)\n",
      " |-- <target.labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- target.title: string (nullable = true)\n",
      " |-- target.tagline: string (nullable = true)\n",
      " |-- target.released: long (nullable = true)\n",
      " |-- rel.roles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actedin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:06 INFO InternalDriver: Closing driver instance 1848875431\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:06 INFO V2ScanRelationPushDown: \n",
      "Output: <rel.id>#623L, <rel.type>#624, <source.id>#625L, <source.labels>#626, source.name#627, source.born#628L, <target.id>#629L, <target.labels>#630, target.title#631, target.tagline#632, target.released#633L, rel.roles#634\n",
      "         \n",
      "25/02/02 23:13:06 INFO DriverFactory: Routing driver instance 850522817 created for server address localhost:7687\n",
      "25/02/02 23:13:06 INFO SparkContext: Starting job: show at cmd60.sc:1\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Got job 30 (show at cmd60.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Final stage: ResultStage 33 (show at cmd60.sc:1)\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[118] at show at cmd60.sc:1), which has no missing parents\n",
      "25/02/02 23:13:06 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 23.1 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:06 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:06 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on Air.local:51188 (size: 9.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:06 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[118] at show at cmd60.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:06 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:06 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 31) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:06 INFO Executor: Running task 0.0 in stage 33.0 (TID 31)\n",
      "25/02/02 23:13:06 INFO Neo4jPartitionReader: Running the following query on Neo4j: MATCH (source:`Person`) MATCH (target:`Movie`) MATCH (source)-[rel:`ACTED_IN`]->(target) RETURN id(rel) AS `<rel.id>`, type(rel) AS `<rel.type>`, id(source) AS `<source.id>`, labels(source) AS `<source.labels>`, source.name AS `source.name`, source.born AS `source.born`, id(target) AS `<target.id>`, labels(target) AS `<target.labels>`, target.title AS `target.title`, target.tagline AS `target.tagline`, target.released AS `target.released`, rel.roles AS `rel.roles` LIMIT 11\n",
      "25/02/02 23:13:06 INFO InternalDriver: Closing driver instance 850522817\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:06 INFO Executor: Finished task 0.0 in stage 33.0 (TID 31). 2437 bytes result sent to driver\n",
      "25/02/02 23:13:06 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 31) in 275 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:06 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:06 INFO DAGScheduler: ResultStage 33 (show at cmd60.sc:1) finished in 0,279 s\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "25/02/02 23:13:06 INFO DAGScheduler: Job 30 finished: show at cmd60.sc:1, took 0,282173 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+------------------+-----------+-----------+---------------+----------------------+------------------------------------------+---------------+-------------+\n",
      "|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.name       |source.born|<target.id>|<target.labels>|target.title          |target.tagline                            |target.released|rel.roles    |\n",
      "+--------+----------+-----------+---------------+------------------+-----------+-----------+---------------+----------------------+------------------------------------------+---------------+-------------+\n",
      "|7       |ACTED_IN  |8          |[Person]       |Emil Eifrem       |1978       |0          |[Movie]        |The Matrix            |Welcome to the Real World                 |1999           |[Emil]       |\n",
      "|3       |ACTED_IN  |4          |[Person]       |Hugo Weaving      |1960       |0          |[Movie]        |The Matrix            |Welcome to the Real World                 |1999           |[Agent Smith]|\n",
      "|2       |ACTED_IN  |3          |[Person]       |Laurence Fishburne|1961       |0          |[Movie]        |The Matrix            |Welcome to the Real World                 |1999           |[Morpheus]   |\n",
      "|1       |ACTED_IN  |2          |[Person]       |Carrie-Anne Moss  |1967       |0          |[Movie]        |The Matrix            |Welcome to the Real World                 |1999           |[Trinity]    |\n",
      "|0       |ACTED_IN  |1          |[Person]       |Keanu Reeves      |1964       |0          |[Movie]        |The Matrix            |Welcome to the Real World                 |1999           |[Neo]        |\n",
      "|11      |ACTED_IN  |4          |[Person]       |Hugo Weaving      |1960       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                            |2003           |[Agent Smith]|\n",
      "|10      |ACTED_IN  |3          |[Person]       |Laurence Fishburne|1961       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                            |2003           |[Morpheus]   |\n",
      "|9       |ACTED_IN  |2          |[Person]       |Carrie-Anne Moss  |1967       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                            |2003           |[Trinity]    |\n",
      "|8       |ACTED_IN  |1          |[Person]       |Keanu Reeves      |1964       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                            |2003           |[Neo]        |\n",
      "|18      |ACTED_IN  |4          |[Person]       |Hugo Weaving      |1960       |10         |[Movie]        |The Matrix Revolutions|Everything that has a beginning has an end|2003           |[Agent Smith]|\n",
      "+--------+----------+-----------+---------------+------------------+-----------+-----------+---------------+----------------------+------------------------------------------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actedin.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame with nodes as map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:06 INFO BlockManagerInfo: Removed broadcast_47_piece0 on Air.local:51188 in memory (size: 9.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:06 INFO DriverFactory: Routing driver instance 1513353525 created for server address localhost:7687\n",
      "25/02/02 23:13:06 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mactedinMap\u001b[39m: \u001b[32mDataFrame\u001b[39m = [<rel.id>: bigint, <rel.type>: string ... 3 more fields]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actedinMap = spark.read\n",
    "                    .format(\"org.neo4j.spark.DataSource\")\n",
    "                    .option(\"relationship.nodes.map\", true)\n",
    "                    .option(\"relationship\", \"ACTED_IN\")\n",
    "                    .option(\"relationship.source.labels\", \":Person\")\n",
    "                    .option(\"relationship.target.labels\", \":Movie\")\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <rel.id>: long (nullable = false)\n",
      " |-- <rel.type>: string (nullable = false)\n",
      " |-- <source>: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- <target>: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- rel.roles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actedinMap.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:06 INFO InternalDriver: Closing driver instance 1513353525\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:06 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:07 INFO V2ScanRelationPushDown: \n",
      "Output: <rel.id>#708L, <rel.type>#709, <source>#710, <target>#711, rel.roles#712\n",
      "         \n",
      "25/02/02 23:13:07 INFO DriverFactory: Routing driver instance 2077710876 created for server address localhost:7687\n",
      "25/02/02 23:13:07 INFO SparkContext: Starting job: show at cmd63.sc:1\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Got job 31 (show at cmd63.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Final stage: ResultStage 34 (show at cmd63.sc:1)\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[122] at show at cmd63.sc:1), which has no missing parents\n",
      "25/02/02 23:13:07 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 21.0 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:07 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:07 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on Air.local:51188 (size: 9.2 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:07 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[122] at show at cmd63.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:07 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:07 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 32) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:07 INFO Executor: Running task 0.0 in stage 34.0 (TID 32)\n",
      "25/02/02 23:13:07 INFO Neo4jPartitionReader: Running the following query on Neo4j: MATCH (source:`Person`) MATCH (target:`Movie`) MATCH (source)-[rel:`ACTED_IN`]->(target) RETURN rel, source AS source, target AS target LIMIT 11\n",
      "25/02/02 23:13:07 INFO InternalDriver: Closing driver instance 2077710876\n",
      "25/02/02 23:13:07 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:07 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:07 INFO Executor: Finished task 0.0 in stage 34.0 (TID 32). 2239 bytes result sent to driver\n",
      "25/02/02 23:13:07 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 32) in 314 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:07 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:07 INFO DAGScheduler: ResultStage 34 (show at cmd63.sc:1) finished in 0,321 s\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "25/02/02 23:13:07 INFO DAGScheduler: Job 31 finished: show at cmd63.sc:1, took 0,322801 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "|<rel.id>|<rel.type>|<source>                                                                       |<target>                                                                                                                                         |rel.roles    |\n",
      "+--------+----------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "|7       |ACTED_IN  |{name -> \"Emil Eifrem\", born -> 1978, <labels> -> [\"Person\"], <id> -> 8}       |{title -> \"The Matrix\", released -> 1999, <labels> -> [\"Movie\"], <id> -> 0, tagline -> \"Welcome to the Real World\"}                              |[Emil]       |\n",
      "|3       |ACTED_IN  |{name -> \"Hugo Weaving\", born -> 1960, <labels> -> [\"Person\"], <id> -> 4}      |{title -> \"The Matrix\", released -> 1999, <labels> -> [\"Movie\"], <id> -> 0, tagline -> \"Welcome to the Real World\"}                              |[Agent Smith]|\n",
      "|2       |ACTED_IN  |{name -> \"Laurence Fishburne\", born -> 1961, <labels> -> [\"Person\"], <id> -> 3}|{title -> \"The Matrix\", released -> 1999, <labels> -> [\"Movie\"], <id> -> 0, tagline -> \"Welcome to the Real World\"}                              |[Morpheus]   |\n",
      "|1       |ACTED_IN  |{name -> \"Carrie-Anne Moss\", born -> 1967, <labels> -> [\"Person\"], <id> -> 2}  |{title -> \"The Matrix\", released -> 1999, <labels> -> [\"Movie\"], <id> -> 0, tagline -> \"Welcome to the Real World\"}                              |[Trinity]    |\n",
      "|0       |ACTED_IN  |{name -> \"Keanu Reeves\", born -> 1964, <labels> -> [\"Person\"], <id> -> 1}      |{title -> \"The Matrix\", released -> 1999, <labels> -> [\"Movie\"], <id> -> 0, tagline -> \"Welcome to the Real World\"}                              |[Neo]        |\n",
      "|11      |ACTED_IN  |{name -> \"Hugo Weaving\", born -> 1960, <labels> -> [\"Person\"], <id> -> 4}      |{title -> \"The Matrix Reloaded\", released -> 2003, <labels> -> [\"Movie\"], <id> -> 9, tagline -> \"Free your mind\"}                                |[Agent Smith]|\n",
      "|10      |ACTED_IN  |{name -> \"Laurence Fishburne\", born -> 1961, <labels> -> [\"Person\"], <id> -> 3}|{title -> \"The Matrix Reloaded\", released -> 2003, <labels> -> [\"Movie\"], <id> -> 9, tagline -> \"Free your mind\"}                                |[Morpheus]   |\n",
      "|9       |ACTED_IN  |{name -> \"Carrie-Anne Moss\", born -> 1967, <labels> -> [\"Person\"], <id> -> 2}  |{title -> \"The Matrix Reloaded\", released -> 2003, <labels> -> [\"Movie\"], <id> -> 9, tagline -> \"Free your mind\"}                                |[Trinity]    |\n",
      "|8       |ACTED_IN  |{name -> \"Keanu Reeves\", born -> 1964, <labels> -> [\"Person\"], <id> -> 1}      |{title -> \"The Matrix Reloaded\", released -> 2003, <labels> -> [\"Movie\"], <id> -> 9, tagline -> \"Free your mind\"}                                |[Neo]        |\n",
      "|18      |ACTED_IN  |{name -> \"Hugo Weaving\", born -> 1960, <labels> -> [\"Person\"], <id> -> 4}      |{title -> \"The Matrix Revolutions\", released -> 2003, <labels> -> [\"Movie\"], <id> -> 10, tagline -> \"Everything that has a beginning has an end\"}|[Agent Smith]|\n",
      "+--------+----------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actedinMap.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:07 INFO BlockManagerInfo: Removed broadcast_48_piece0 on Air.local:51188 in memory (size: 9.2 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:07 INFO DriverFactory: Routing driver instance 1050517317 created for server address localhost:7687\n",
      "25/02/02 23:13:07 WARN SchemaService: Switching to query schema resolution\n",
      "25/02/02 23:13:07 WARN SchemaService: Switching to query schema resolution\n",
      "25/02/02 23:13:07 WARN SchemaService: Switching to query schema resolution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdirected\u001b[39m: \u001b[32mDataFrame\u001b[39m = [<rel.id>: bigint, <rel.type>: string ... 9 more fields]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val directed = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"relationship\", \"DIRECTED\")\n",
    "                .option(\"relationship.source.labels\", \":Person\")\n",
    "                .option(\"relationship.target.labels\", \":Movie\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <rel.id>: long (nullable = false)\n",
      " |-- <rel.type>: string (nullable = false)\n",
      " |-- <source.id>: long (nullable = false)\n",
      " |-- <source.labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- source.name: string (nullable = true)\n",
      " |-- source.born: long (nullable = true)\n",
      " |-- <target.id>: long (nullable = false)\n",
      " |-- <target.labels>: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- target.title: string (nullable = true)\n",
      " |-- target.tagline: string (nullable = true)\n",
      " |-- target.released: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:08 INFO InternalDriver: Closing driver instance 1050517317\n",
      "25/02/02 23:13:08 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:08 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:08 INFO V2ScanRelationPushDown: \n",
      "Output: <rel.id>#744L, <rel.type>#745, <source.id>#746L, <source.labels>#747, source.name#748, source.born#749L, <target.id>#750L, <target.labels>#751, target.title#752, target.tagline#753, target.released#754L\n",
      "         \n",
      "25/02/02 23:13:08 INFO DriverFactory: Routing driver instance 1072211576 created for server address localhost:7687\n",
      "25/02/02 23:13:08 INFO SparkContext: Starting job: show at cmd66.sc:1\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Got job 32 (show at cmd66.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Final stage: ResultStage 35 (show at cmd66.sc:1)\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[126] at show at cmd66.sc:1), which has no missing parents\n",
      "25/02/02 23:13:08 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 21.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:08 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:08 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on Air.local:51188 (size: 9.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:08 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[126] at show at cmd66.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:08 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:08 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 33) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:08 INFO Executor: Running task 0.0 in stage 35.0 (TID 33)\n",
      "25/02/02 23:13:08 INFO Neo4jPartitionReader: Running the following query on Neo4j: MATCH (source:`Person`) MATCH (target:`Movie`) MATCH (source)-[rel:`DIRECTED`]->(target) RETURN id(rel) AS `<rel.id>`, type(rel) AS `<rel.type>`, id(source) AS `<source.id>`, labels(source) AS `<source.labels>`, source.name AS `source.name`, source.born AS `source.born`, id(target) AS `<target.id>`, labels(target) AS `<target.labels>`, target.title AS `target.title`, target.tagline AS `target.tagline`, target.released AS `target.released` LIMIT 11\n",
      "25/02/02 23:13:08 INFO InternalDriver: Closing driver instance 1072211576\n",
      "25/02/02 23:13:08 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:08 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:08 INFO Executor: Finished task 0.0 in stage 35.0 (TID 33). 2692 bytes result sent to driver\n",
      "25/02/02 23:13:08 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 33) in 275 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:08 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:08 INFO DAGScheduler: ResultStage 35 (show at cmd66.sc:1) finished in 0,280 s\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
      "25/02/02 23:13:08 INFO DAGScheduler: Job 32 finished: show at cmd66.sc:1, took 0,282027 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+---------------+-----------+-----------+---------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.name    |source.born|<target.id>|<target.labels>|target.title          |target.tagline                                                                                                                                                               |target.released|\n",
      "+--------+----------+-----------+---------------+---------------+-----------+-----------+---------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|5       |DIRECTED  |6          |[Person]       |Lana Wachowski |1965       |0          |[Movie]        |The Matrix            |Welcome to the Real World                                                                                                                                                    |1999           |\n",
      "|4       |DIRECTED  |5          |[Person]       |Lilly Wachowski|1967       |0          |[Movie]        |The Matrix            |Welcome to the Real World                                                                                                                                                    |1999           |\n",
      "|13      |DIRECTED  |6          |[Person]       |Lana Wachowski |1965       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                                                                                                                                                               |2003           |\n",
      "|12      |DIRECTED  |5          |[Person]       |Lilly Wachowski|1967       |9          |[Movie]        |The Matrix Reloaded   |Free your mind                                                                                                                                                               |2003           |\n",
      "|20      |DIRECTED  |6          |[Person]       |Lana Wachowski |1965       |10         |[Movie]        |The Matrix Revolutions|Everything that has a beginning has an end                                                                                                                                   |2003           |\n",
      "|19      |DIRECTED  |5          |[Person]       |Lilly Wachowski|1967       |10         |[Movie]        |The Matrix Revolutions|Everything that has a beginning has an end                                                                                                                                   |2003           |\n",
      "|25      |DIRECTED  |14         |[Person]       |Taylor Hackford|1944       |11         |[Movie]        |The Devil's Advocate  |Evil has its winning ways                                                                                                                                                    |1997           |\n",
      "|38      |DIRECTED  |27         |[Person]       |Rob Reiner     |1947       |15         |[Movie]        |A Few Good Men        |In the heart of the nation's capital, in a courthouse of the U.S. government, one man will stop at nothing to keep his honor, and one will stop at nothing to find the truth.|1992           |\n",
      "|46      |DIRECTED  |35         |[Person]       |Tony Scott     |1944       |29         |[Movie]        |Top Gun               |I feel the need, the need for speed.                                                                                                                                         |1986           |\n",
      "|57      |DIRECTED  |45         |[Person]       |Cameron Crowe  |1957       |37         |[Movie]        |Jerry Maguire         |The rest of his life begins now.                                                                                                                                             |2000           |\n",
      "+--------+----------+-----------+---------------+---------------+-----------+-----------+---------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directed.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read arbitrary data via Cypher query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:08 INFO BlockManagerInfo: Removed broadcast_49_piece0 on Air.local:51188 in memory (size: 9.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:08 INFO DriverFactory: Routing driver instance 1517916703 created for server address localhost:7687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcypher\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Recommended: string, Strength: bigint]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cypher = spark.read\n",
    "                .format(\"org.neo4j.spark.DataSource\")\n",
    "                .option(\"query\", \"\"\"\n",
    "                    // Extend Tom Hanks co-actors, to find co-co-actors who haven't worked with Tom Hanks\n",
    "                    MATCH (tom:Person {name:\"Tom Hanks\"})-[:ACTED_IN]->(m)<-[:ACTED_IN]-(coActors),\n",
    "                    (coActors)-[:ACTED_IN]->(m2)<-[:ACTED_IN]-(cocoActors)\n",
    "                    WHERE NOT (tom)-[:ACTED_IN]->()<-[:ACTED_IN]-(cocoActors)\n",
    "                        AND tom <> cocoActors\n",
    "                    RETURN cocoActors.name AS Recommended, count(*) AS Strength\n",
    "                    ORDER BY Strength DESC\n",
    "                    \"\"\")\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Recommended: string (nullable = true)\n",
      " |-- Strength: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cypher.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:09 INFO InternalDriver: Closing driver instance 1517916703\n",
      "25/02/02 23:13:09 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:09 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:09 INFO V2ScanRelationPushDown: \n",
      "Output: Recommended#822, Strength#823L\n",
      "         \n",
      "25/02/02 23:13:09 INFO DriverFactory: Routing driver instance 674566131 created for server address localhost:7687\n",
      "25/02/02 23:13:09 INFO SparkContext: Starting job: show at cmd69.sc:1\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Got job 33 (show at cmd69.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Final stage: ResultStage 36 (show at cmd69.sc:1)\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[130] at show at cmd69.sc:1), which has no missing parents\n",
      "25/02/02 23:13:09 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 17.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:09 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:09 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on Air.local:51188 (size: 8.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:09 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[130] at show at cmd69.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:09 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:09 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 34) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:09 INFO Executor: Running task 0.0 in stage 36.0 (TID 34)\n",
      "25/02/02 23:13:09 INFO Neo4jPartitionReader: Running the following query on Neo4j: WITH $scriptResult AS scriptResult // Extend Tom Hanks co-actors, to find co-co-actors who haven't worked with Tom Hanks\n",
      "                    MATCH (tom:Person {name:\"Tom Hanks\"})-[:ACTED_IN]->(m)<-[:ACTED_IN]-(coActors),\n",
      "                    (coActors)-[:ACTED_IN]->(m2)<-[:ACTED_IN]-(cocoActors)\n",
      "                    WHERE NOT (tom)-[:ACTED_IN]->()<-[:ACTED_IN]-(cocoActors)\n",
      "                        AND tom <> cocoActors\n",
      "                    RETURN cocoActors.name AS Recommended, count(*) AS Strength\n",
      "                    ORDER BY Strength DESC SKIP 0 LIMIT 11\n",
      "25/02/02 23:13:09 INFO InternalDriver: Closing driver instance 674566131\n",
      "25/02/02 23:13:09 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:09 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:10 INFO Executor: Finished task 0.0 in stage 36.0 (TID 34). 1734 bytes result sent to driver\n",
      "25/02/02 23:13:10 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 34) in 334 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:10 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:10 INFO DAGScheduler: ResultStage 36 (show at cmd69.sc:1) finished in 0,341 s\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Job 33 finished: show at cmd69.sc:1, took 0,342217 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|Recommended     |Strength|\n",
      "+----------------+--------+\n",
      "|Zach Grenier    |5       |\n",
      "|Tom Cruise      |5       |\n",
      "|Cuba Gooding Jr.|4       |\n",
      "|Keanu Reeves    |4       |\n",
      "|Kelly McGillis  |3       |\n",
      "|Tom Skerritt    |3       |\n",
      "|Billy Crystal   |3       |\n",
      "|Jack Nicholson  |3       |\n",
      "|Val Kilmer      |3       |\n",
      "|Anthony Edwards |3       |\n",
      "+----------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cypher.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all the actors that have also directed a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:10 INFO BlockManagerInfo: Removed broadcast_50_piece0 on Air.local:51188 in memory (size: 8.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:10 INFO DriverFactory: Routing driver instance 83322656 created for server address localhost:7687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mactorsDirectors\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, acted_in: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actorsDirectors = spark.read\n",
    "                        .format(\"org.neo4j.spark.DataSource\")\n",
    "                        .option(\"query\", \"\"\"\n",
    "                                MATCH (p:Person)\n",
    "                                MATCH (p)-[:ACTED_IN]->(m:Movie)\n",
    "                                MATCH (p)-[:DIRECTED]->(m1:Movie)\n",
    "                                RETURN p.name AS name, collect(m.title) AS acted_in, collect(m1.title) AS directed\n",
    "                                \"\"\")\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- acted_in: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- directed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorsDirectors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:10 INFO InternalDriver: Closing driver instance 83322656\n",
      "25/02/02 23:13:10 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:10 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:10 INFO V2ScanRelationPushDown: \n",
      "Output: name#837, acted_in#838, directed#839\n",
      "         \n",
      "25/02/02 23:13:10 INFO DriverFactory: Routing driver instance 1346247445 created for server address localhost:7687\n",
      "25/02/02 23:13:10 INFO SparkContext: Starting job: show at cmd72.sc:1\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Got job 34 (show at cmd72.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Final stage: ResultStage 37 (show at cmd72.sc:1)\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[134] at show at cmd72.sc:1), which has no missing parents\n",
      "25/02/02 23:13:10 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 19.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:10 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:10 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on Air.local:51188 (size: 8.8 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:10 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[134] at show at cmd72.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:10 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:10 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 35) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9464 bytes) \n",
      "25/02/02 23:13:10 INFO Executor: Running task 0.0 in stage 37.0 (TID 35)\n",
      "25/02/02 23:13:11 INFO Neo4jPartitionReader: Running the following query on Neo4j: WITH $scriptResult AS scriptResult MATCH (p:Person)\n",
      "                                MATCH (p)-[:ACTED_IN]->(m:Movie)\n",
      "                                MATCH (p)-[:DIRECTED]->(m1:Movie)\n",
      "                                RETURN p.name AS name, collect(m.title) AS acted_in, collect(m1.title) AS directed SKIP 0 LIMIT 11\n",
      "25/02/02 23:13:11 INFO InternalDriver: Closing driver instance 1346247445\n",
      "25/02/02 23:13:11 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:11 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:11 INFO Executor: Finished task 0.0 in stage 37.0 (TID 35). 2034 bytes result sent to driver\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 35) in 273 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:11 INFO DAGScheduler: ResultStage 37 (show at cmd72.sc:1) finished in 0,280 s\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 34 finished: show at cmd72.sc:1, took 0,282141 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|name          |acted_in                                                                                                                                                                                                                |directed                                                                                                                                                                                                                            |\n",
      "+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tom Hanks     |[Apollo 13, You've Got Mail, A League of Their Own, Joe Versus the Volcano, That Thing You Do, The Da Vinci Code, Cloud Atlas, Cast Away, The Green Mile, Sleepless in Seattle, The Polar Express, Charlie Wilson's War]|[That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do, That Thing You Do]|\n",
      "|Werner Herzog |[What Dreams May Come]                                                                                                                                                                                                  |[RescueDawn]                                                                                                                                                                                                                        |\n",
      "|Clint Eastwood|[Unforgiven]                                                                                                                                                                                                            |[Unforgiven]                                                                                                                                                                                                                        |\n",
      "|James Marshall|[A Few Good Men, A Few Good Men]                                                                                                                                                                                        |[V for Vendetta, Ninja Assassin]                                                                                                                                                                                                    |\n",
      "|Danny DeVito  |[Hoffa, One Flew Over the Cuckoo's Nest]                                                                                                                                                                                |[Hoffa, Hoffa]                                                                                                                                                                                                                      |\n",
      "+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorsDirectors.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data from Spark to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#859, None)) > 0)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 52 from load at cmd73.sc:4\n",
      "25/02/02 23:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:11 INFO SparkContext: Starting job: load at cmd73.sc:4\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Got job 35 (load at cmd73.sc:4) with 1 output partitions\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Final stage: ResultStage 38 (load at cmd73.sc:4)\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[138] at load at cmd73.sc:4), which has no missing parents\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 13.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on Air.local:51188 (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[138] at load at cmd73.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 36) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9629 bytes) \n",
      "25/02/02 23:13:11 INFO Executor: Running task 0.0 in stage 38.0 (TID 36)\n",
      "25/02/02 23:13:11 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/products.csv, range: 0-1966, partition values: [empty row]\n",
      "25/02/02 23:13:11 INFO Executor: Finished task 0.0 in stage 38.0 (TID 36). 1585 bytes result sent to driver\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 36) in 7 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:11 INFO DAGScheduler: ResultStage 38 (load at cmd73.sc:4) finished in 0,011 s\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 35 finished: load at cmd73.sc:4, took 0,013862 s\n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 54 from load at cmd73.sc:4\n",
      "25/02/02 23:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:11 INFO SparkContext: Starting job: load at cmd73.sc:4\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Got job 36 (load at cmd73.sc:4) with 1 output partitions\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Final stage: ResultStage 39 (load at cmd73.sc:4)\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[144] at load at cmd73.sc:4), which has no missing parents\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 27.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on Air.local:51188 (size: 12.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[144] at load at cmd73.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 37) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9629 bytes) \n",
      "25/02/02 23:13:11 INFO Executor: Running task 0.0 in stage 39.0 (TID 37)\n",
      "25/02/02 23:13:11 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/products.csv, range: 0-1966, partition values: [empty row]\n",
      "25/02/02 23:13:11 INFO Executor: Finished task 0.0 in stage 39.0 (TID 37). 1623 bytes result sent to driver\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 37) in 9 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:11 INFO DAGScheduler: ResultStage 39 (load at cmd73.sc:4) finished in 0,019 s\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 36 finished: load at cmd73.sc:4, took 0,021321 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mproducts\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val products = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"inferSchema\", true)\n",
    "                .load(\"desktop-csv-import/products.csv\")\n",
    "                .withColumnsRenamed(Map(\"_c0\" -> \"id\", \"_c1\" -> \"name\", \"_c2\" -> \"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:11 INFO BlockManagerInfo: Removed broadcast_54_piece0 on Air.local:51188 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Removed broadcast_53_piece0 on Air.local:51188 in memory (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Removed broadcast_51_piece0 on Air.local:51188 in memory (size: 8.8 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Removed broadcast_52_piece0 on Air.local:51188 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Removed broadcast_55_piece0 on Air.local:51188 in memory (size: 12.7 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 56 from show at cmd75.sc:1\n",
      "25/02/02 23:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:11 INFO SparkContext: Starting job: show at cmd75.sc:1\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Got job 37 (show at cmd75.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Final stage: ResultStage 40 (show at cmd75.sc:1)\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[148] at show at cmd75.sc:1), which has no missing parents\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 15.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on Air.local:51188 (size: 7.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:11 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[148] at show at cmd75.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 38) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9629 bytes) \n",
      "25/02/02 23:13:11 INFO Executor: Running task 0.0 in stage 40.0 (TID 38)\n",
      "25/02/02 23:13:11 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/products.csv, range: 0-1966, partition values: [empty row]\n",
      "25/02/02 23:13:11 INFO Executor: Finished task 0.0 in stage 40.0 (TID 38). 1976 bytes result sent to driver\n",
      "25/02/02 23:13:11 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 38) in 7 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:11 INFO DAGScheduler: ResultStage 40 (show at cmd75.sc:1) finished in 0,011 s\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "25/02/02 23:13:11 INFO DAGScheduler: Job 37 finished: show at cmd75.sc:1, took 0,012985 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------+-----+\n",
      "|id |name                           |price|\n",
      "+---+-------------------------------+-----+\n",
      "|1  |Chai                           |18.0 |\n",
      "|2  |Chang                          |19.0 |\n",
      "|3  |Aniseed Syrup                  |10.0 |\n",
      "|4  |Chef Anton's Cajun Seasoning   |22.0 |\n",
      "|5  |Chef Anton's Gumbo Mix         |21.35|\n",
      "|6  |Grandma's Boysenberry Spread   |25.0 |\n",
      "|7  |Uncle Bob's Organic Dried Pears|30.0 |\n",
      "|8  |Northwoods Cranberry Sauce     |40.0 |\n",
      "|9  |Mishi Kobe Niku                |97.0 |\n",
      "|10 |Ikura                          |31.0 |\n",
      "+---+-------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:11 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 58 from count at cmd76.sc:1\n",
      "25/02/02 23:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Registering RDD 152 (count at cmd76.sc:1) as input to shuffle 3\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got map stage job 38 (count at cmd76.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (count at cmd76.sc:1)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[152] at count at cmd76.sc:1), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 17.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on Air.local:51188 (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[152] at count at cmd76.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 39) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9618 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 41.0 (TID 39)\n",
      "25/02/02 23:13:12 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/products.csv, range: 0-1966, partition values: [empty row]\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 41.0 (TID 39). 1925 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 39) in 48 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ShuffleMapStage 41 (count at cmd76.sc:1) finished in 0,054 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/02 23:13:12 INFO DAGScheduler: running: Set()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: waiting: Set()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: failed: Set()\n",
      "25/02/02 23:13:12 INFO SparkContext: Starting job: count at cmd76.sc:1\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got job 39 (count at cmd76.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ResultStage 43 (count at cmd76.sc:1)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[155] at count at cmd76.sc:1), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 12.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on Air.local:51188 (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[155] at count at cmd76.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 40) (Air.local, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 43.0 (TID 40)\n",
      "25/02/02 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/02 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 43.0 (TID 40). 3995 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 40) in 5 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ResultStage 43 (count at cmd76.sc:1) finished in 0,009 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 39 finished: count at cmd76.sc:1, took 0,011738 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres76\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m77L\u001b[39m"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_57_piece0 on Air.local:51188 in memory (size: 7.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_59_piece0 on Air.local:51188 in memory (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#913, None)) > 0)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 61 from load at cmd77.sc:5\n",
      "25/02/02 23:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:12 INFO SparkContext: Starting job: load at cmd77.sc:5\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got job 40 (load at cmd77.sc:5) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ResultStage 44 (load at cmd77.sc:5)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[159] at load at cmd77.sc:5), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 13.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on Air.local:51188 (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[159] at load at cmd77.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 41) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9627 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 44.0 (TID 41)\n",
      "25/02/02 23:13:12 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/orders.csv, range: 0-31470, partition values: [empty row]\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 44.0 (TID 41). 1600 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 41) in 9 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ResultStage 44 (load at cmd77.sc:5) finished in 0,012 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 40 finished: load at cmd77.sc:5, took 0,013828 s\n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 63 from load at cmd77.sc:5\n",
      "25/02/02 23:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:12 INFO SparkContext: Starting job: load at cmd77.sc:5\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got job 41 (load at cmd77.sc:5) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ResultStage 45 (load at cmd77.sc:5)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[165] at load at cmd77.sc:5), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 27.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on Air.local:51188 (size: 12.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[165] at load at cmd77.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 42) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9627 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 45.0 (TID 42)\n",
      "25/02/02 23:13:12 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/orders.csv, range: 0-31470, partition values: [empty row]\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 45.0 (TID 42). 1626 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 42) in 8 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ResultStage 45 (load at cmd77.sc:5) finished in 0,017 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 41 finished: load at cmd77.sc:5, took 0,018027 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36morders\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, date: timestamp ... 1 more field]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"inferSchema\", true)\n",
    "                .option(\"header\", true)\n",
    "                .load(\"desktop-csv-import/orders.csv\")\n",
    "                .selectExpr(\"orderID AS id\", \"CAST(orderDate AS TIMESTAMP) AS date\", \"shipCountry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- shipCountry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_61_piece0 on Air.local:51188 in memory (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_62_piece0 on Air.local:51188 in memory (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_63_piece0 on Air.local:51188 in memory (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_64_piece0 on Air.local:51188 in memory (size: 12.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 65 from show at cmd79.sc:1\n",
      "25/02/02 23:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:12 INFO SparkContext: Starting job: show at cmd79.sc:1\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got job 42 (show at cmd79.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ResultStage 46 (show at cmd79.sc:1)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[169] at show at cmd79.sc:1), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 16.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on Air.local:51188 (size: 8.0 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[169] at show at cmd79.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 43) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9627 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 46.0 (TID 43)\n",
      "25/02/02 23:13:12 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/orders.csv, range: 0-31470, partition values: [empty row]\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 46.0 (TID 43). 1789 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 43) in 7 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ResultStage 46 (show at cmd79.sc:1) finished in 0,012 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 42 finished: show at cmd79.sc:1, took 0,015491 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+\n",
      "|id   |date               |shipCountry|\n",
      "+-----+-------------------+-----------+\n",
      "|10248|1996-07-04 00:00:00|France     |\n",
      "|10249|1996-07-05 00:00:00|Germany    |\n",
      "|10250|1996-07-08 00:00:00|Brazil     |\n",
      "|10251|1996-07-08 00:00:00|France     |\n",
      "|10252|1996-07-09 00:00:00|Belgium    |\n",
      "|10253|1996-07-10 00:00:00|Brazil     |\n",
      "|10254|1996-07-11 00:00:00|Switzerland|\n",
      "|10255|1996-07-12 00:00:00|Switzerland|\n",
      "|10256|1996-07-15 00:00:00|Brazil     |\n",
      "|10257|1996-07-16 00:00:00|Venezuela  |\n",
      "+-----+-------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 67 from count at cmd80.sc:1\n",
      "25/02/02 23:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Registering RDD 173 (count at cmd80.sc:1) as input to shuffle 4\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got map stage job 43 (count at cmd80.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ShuffleMapStage 47 (count at cmd80.sc:1)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[173] at count at cmd80.sc:1), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 17.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on Air.local:51188 (size: 8.8 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[173] at count at cmd80.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 44) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9616 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 47.0 (TID 44)\n",
      "25/02/02 23:13:12 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/orders.csv, range: 0-31470, partition values: [empty row]\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 47.0 (TID 44). 1925 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 44) in 10 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ShuffleMapStage 47 (count at cmd80.sc:1) finished in 0,014 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/02 23:13:12 INFO DAGScheduler: running: Set()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: waiting: Set()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: failed: Set()\n",
      "25/02/02 23:13:12 INFO SparkContext: Starting job: count at cmd80.sc:1\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Got job 44 (count at cmd80.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Final stage: ResultStage 49 (count at cmd80.sc:1)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[176] at count at cmd80.sc:1), which has no missing parents\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 12.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on Air.local:51188 (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[176] at count at cmd80.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 45) (Air.local, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/02/02 23:13:12 INFO Executor: Running task 0.0 in stage 49.0 (TID 45)\n",
      "25/02/02 23:13:12 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/02 23:13:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/02/02 23:13:12 INFO Executor: Finished task 0.0 in stage 49.0 (TID 45). 3995 bytes result sent to driver\n",
      "25/02/02 23:13:12 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 45) in 5 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:12 INFO DAGScheduler: ResultStage 49 (count at cmd80.sc:1) finished in 0,010 s\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "25/02/02 23:13:12 INFO DAGScheduler: Job 44 finished: count at cmd80.sc:1, took 0,011494 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres80\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m830L\u001b[39m"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write nodes via label option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_66_piece0 on Air.local:51188 in memory (size: 8.0 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_69_piece0 on Air.local:51188 in memory (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_68_piece0 on Air.local:51188 in memory (size: 8.8 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:12 INFO BlockManagerInfo: Removed broadcast_65_piece0 on Air.local:51188 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:13 INFO DriverFactory: Routing driver instance 30853224 created for server address localhost:7687\n",
      "25/02/02 23:13:13 INFO InternalDriver: Closing driver instance 30853224\n",
      "25/02/02 23:13:13 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO SparkContext: Created broadcast 70 from save at cmd81.sc:4\n",
      "25/02/02 23:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:13 INFO DriverFactory: Routing driver instance 864229234 created for server address localhost:7687\n",
      "25/02/02 23:13:13 INFO AppendDataExec: Start processing data source write support: org.neo4j.spark.writer.Neo4jBatchWriter@b1843cb. The input RDD has 1 partitions.\n",
      "25/02/02 23:13:13 INFO SparkContext: Starting job: save at cmd81.sc:4\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Got job 45 (save at cmd81.sc:4) with 1 output partitions\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Final stage: ResultStage 50 (save at cmd81.sc:4)\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[179] at save at cmd81.sc:4), which has no missing parents\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 23.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 11.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on Air.local:51188 (size: 11.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[179] at save at cmd81.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:13 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:13 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 46) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9629 bytes) \n",
      "25/02/02 23:13:13 INFO Executor: Running task 0.0 in stage 50.0 (TID 46)\n",
      "25/02/02 23:13:13 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/products.csv, range: 0-1966, partition values: [empty row]\n",
      "25/02/02 23:13:13 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 46, attempt 0, stage 50.0)\n",
      "25/02/02 23:13:13 INFO Neo4jDataWriter: Writing a batch of 77 elements to Neo4j,\n",
      "for jobId=5c6f33e4-3dbb-47a8-97d7-31801faa82a6 and partitionId=0\n",
      "with query: UNWIND $events AS event\n",
      "CREATE (node:Product )\n",
      "SET node += event.properties\n",
      "\n",
      "\n",
      "25/02/02 23:13:13 INFO DataWritingSparkTask: Committed partition 0 (task 46, attempt 0, stage 50.0)\n",
      "25/02/02 23:13:13 INFO Executor: Finished task 0.0 in stage 50.0 (TID 46). 2510 bytes result sent to driver\n",
      "25/02/02 23:13:13 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 46) in 85 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:13 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:13 INFO DAGScheduler: ResultStage 50 (save at cmd81.sc:4) finished in 0,090 s\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished\n",
      "25/02/02 23:13:13 INFO DAGScheduler: Job 45 finished: save at cmd81.sc:4, took 0,091901 s\n",
      "25/02/02 23:13:13 INFO AppendDataExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@b1843cb is committing.\n",
      "25/02/02 23:13:13 INFO InternalDriver: Closing driver instance 864229234\n",
      "25/02/02 23:13:13 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:13 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:13 INFO AppendDataExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@b1843cb committed.\n"
     ]
    }
   ],
   "source": [
    "products.write\n",
    "        .format(\"org.neo4j.spark.DataSource\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"labels\", \":Product\")\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:13 WARN Neo4jOptions: \n",
      "Option `schema.optimization.type` is deprecated and will be removed in future implementations,\n",
      "please move to one of the following depending on your use case:\n",
      "- `schema.optimization.node.keys`\n",
      "- `schema.optimization.relationship.keys`\n",
      "\n",
      "25/02/02 23:13:13 WARN Neo4jOptions: \n",
      "Option `schema.optimization.type` is deprecated and will be removed in future implementations,\n",
      "please move to one of the following depending on your use case:\n",
      "- `schema.optimization.node.keys`\n",
      "- `schema.optimization.relationship.keys`\n",
      "\n",
      "25/02/02 23:13:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:13 INFO DriverFactory: Routing driver instance 199570512 created for server address localhost:7687\n",
      "25/02/02 23:13:13 INFO InternalDriver: Closing driver instance 199570512\n",
      "25/02/02 23:13:13 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:13 INFO SparkContext: Created broadcast 72 from save at cmd82.sc:7\n",
      "25/02/02 23:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:13 INFO DriverFactory: Routing driver instance 1728372664 created for server address localhost:7687\n",
      "25/02/02 23:13:14 INFO SchemaService: Performing the following schema query: CREATE CONSTRAINT spark_NODE_CONSTRAINTS_Order_id FOR (n:Order) REQUIRE (n.id) IS UNIQUE\n",
      "25/02/02 23:13:14 INFO SchemaService: Status for NODE_CONSTRAINTS named with label Order and props n.id is: CREATED\n",
      "25/02/02 23:13:14 INFO OverwriteByExpressionExec: Start processing data source write support: org.neo4j.spark.writer.Neo4jBatchWriter@32535a44. The input RDD has 1 partitions.\n",
      "25/02/02 23:13:14 INFO SparkContext: Starting job: save at cmd82.sc:7\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Got job 46 (save at cmd82.sc:7) with 1 output partitions\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Final stage: ResultStage 51 (save at cmd82.sc:7)\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[182] at save at cmd82.sc:7), which has no missing parents\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 23.6 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on Air.local:51188 (size: 11.5 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[182] at save at cmd82.sc:7) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 47) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9627 bytes) \n",
      "25/02/02 23:13:14 INFO Executor: Running task 0.0 in stage 51.0 (TID 47)\n",
      "25/02/02 23:13:14 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/orders.csv, range: 0-31470, partition values: [empty row]\n",
      "25/02/02 23:13:14 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 47, attempt 0, stage 51.0)\n",
      "25/02/02 23:13:14 INFO Neo4jDataWriter: Writing a batch of 830 elements to Neo4j,\n",
      "for jobId=1400c029-7abf-483b-894d-75e4f362f806 and partitionId=0\n",
      "with query: UNWIND $events AS event\n",
      "MERGE (node:Order {id: event.keys.id})\n",
      "SET node += event.properties\n",
      "\n",
      "\n",
      "25/02/02 23:13:14 INFO DataWritingSparkTask: Committed partition 0 (task 47, attempt 0, stage 51.0)\n",
      "25/02/02 23:13:14 INFO Executor: Finished task 0.0 in stage 51.0 (TID 47). 2510 bytes result sent to driver\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 47) in 219 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:14 INFO DAGScheduler: ResultStage 51 (save at cmd82.sc:7) finished in 0,224 s\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 46 finished: save at cmd82.sc:7, took 0,226780 s\n",
      "25/02/02 23:13:14 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@32535a44 is committing.\n",
      "25/02/02 23:13:14 INFO InternalDriver: Closing driver instance 1728372664\n",
      "25/02/02 23:13:14 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:14 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:14 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@32535a44 committed.\n"
     ]
    }
   ],
   "source": [
    "orders.write\n",
    "      .format(\"org.neo4j.spark.DataSource\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"labels\", \":Order\")\n",
    "      .option(\"schema.optimization.type\", \"NODE_CONSTRAINTS\")\n",
    "// this is necessary in order to specify what is the constraint field\n",
    "      .option(\"node.keys\", \"id\")\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write relationships via relationship option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_70_piece0 on Air.local:51188 in memory (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_71_piece0 on Air.local:51188 in memory (size: 11.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_73_piece0 on Air.local:51188 in memory (size: 11.5 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:14 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/02/02 23:13:14 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:14 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#984, None)) > 0)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO SparkContext: Created broadcast 74 from load at cmd83.sc:5\n",
      "25/02/02 23:13:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:14 INFO SparkContext: Starting job: load at cmd83.sc:5\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Got job 47 (load at cmd83.sc:5) with 1 output partitions\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Final stage: ResultStage 52 (load at cmd83.sc:5)\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[186] at load at cmd83.sc:5), which has no missing parents\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 13.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on Air.local:51188 (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[186] at load at cmd83.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 48) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9634 bytes) \n",
      "25/02/02 23:13:14 INFO Executor: Running task 0.0 in stage 52.0 (TID 48)\n",
      "25/02/02 23:13:14 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/order-details.csv, range: 0-27454, partition values: [empty row]\n",
      "25/02/02 23:13:14 INFO Executor: Finished task 0.0 in stage 52.0 (TID 48). 1601 bytes result sent to driver\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 48) in 6 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:14 INFO DAGScheduler: ResultStage 52 (load at cmd83.sc:5) finished in 0,010 s\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 47 finished: load at cmd83.sc:5, took 0,013114 s\n",
      "25/02/02 23:13:14 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 199.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on Air.local:51188 (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO SparkContext: Created broadcast 76 from load at cmd83.sc:5\n",
      "25/02/02 23:13:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:14 INFO SparkContext: Starting job: load at cmd83.sc:5\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Got job 48 (load at cmd83.sc:5) with 1 output partitions\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Final stage: ResultStage 53 (load at cmd83.sc:5)\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[192] at load at cmd83.sc:5), which has no missing parents\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 27.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on Air.local:51188 (size: 12.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[192] at load at cmd83.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 49) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9634 bytes) \n",
      "25/02/02 23:13:14 INFO Executor: Running task 0.0 in stage 53.0 (TID 49)\n",
      "25/02/02 23:13:14 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/order-details.csv, range: 0-27454, partition values: [empty row]\n",
      "25/02/02 23:13:14 INFO Executor: Finished task 0.0 in stage 53.0 (TID 49). 1523 bytes result sent to driver\n",
      "25/02/02 23:13:14 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 49) in 12 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:14 INFO DAGScheduler: ResultStage 53 (load at cmd83.sc:5) finished in 0,018 s\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "25/02/02 23:13:14 INFO DAGScheduler: Job 48 finished: load at cmd83.sc:5, took 0,019864 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36morderDetails\u001b[39m: \u001b[32mDataFrame\u001b[39m = [orderID: int, productID: int ... 1 more field]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderDetails = spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"inferSchema\", true)\n",
    "                    .option(\"header\", true)\n",
    "                    .load(\"desktop-csv-import/order-details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: integer (nullable = true)\n",
      " |-- productID: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDetails.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_77_piece0 on Air.local:51188 in memory (size: 12.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_74_piece0 on Air.local:51188 in memory (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_76_piece0 on Air.local:51188 in memory (size: 34.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:14 INFO BlockManagerInfo: Removed broadcast_75_piece0 on Air.local:51188 in memory (size: 6.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 78 from show at cmd85.sc:1\n",
      "25/02/02 23:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:15 INFO SparkContext: Starting job: show at cmd85.sc:1\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Got job 49 (show at cmd85.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Final stage: ResultStage 54 (show at cmd85.sc:1)\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[196] at show at cmd85.sc:1), which has no missing parents\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 15.8 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on Air.local:51188 (size: 7.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[196] at show at cmd85.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 50) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9634 bytes) \n",
      "25/02/02 23:13:15 INFO Executor: Running task 0.0 in stage 54.0 (TID 50)\n",
      "25/02/02 23:13:15 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/order-details.csv, range: 0-27454, partition values: [empty row]\n",
      "25/02/02 23:13:15 INFO Executor: Finished task 0.0 in stage 54.0 (TID 50). 1724 bytes result sent to driver\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 50) in 43 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:15 INFO DAGScheduler: ResultStage 54 (show at cmd85.sc:1) finished in 0,048 s\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Job 49 finished: show at cmd85.sc:1, took 0,050394 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n",
      "|orderID|productID|quantity|\n",
      "+-------+---------+--------+\n",
      "|10248  |11       |12      |\n",
      "|10248  |42       |10      |\n",
      "|10248  |72       |5       |\n",
      "|10249  |14       |9       |\n",
      "|10249  |51       |40      |\n",
      "|10250  |41       |10      |\n",
      "|10250  |51       |35      |\n",
      "|10250  |65       |15      |\n",
      "|10251  |22       |6       |\n",
      "|10251  |57       |15      |\n",
      "+-------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDetails.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 80 from count at cmd86.sc:1\n",
      "25/02/02 23:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Registering RDD 200 (count at cmd86.sc:1) as input to shuffle 5\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Got map stage job 50 (count at cmd86.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Final stage: ShuffleMapStage 55 (count at cmd86.sc:1)\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[200] at count at cmd86.sc:1), which has no missing parents\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 17.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on Air.local:51188 (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[200] at count at cmd86.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 51) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9623 bytes) \n",
      "25/02/02 23:13:15 INFO Executor: Running task 0.0 in stage 55.0 (TID 51)\n",
      "25/02/02 23:13:15 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/order-details.csv, range: 0-27454, partition values: [empty row]\n",
      "25/02/02 23:13:15 INFO Executor: Finished task 0.0 in stage 55.0 (TID 51). 1925 bytes result sent to driver\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 51) in 13 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:15 INFO DAGScheduler: ShuffleMapStage 55 (count at cmd86.sc:1) finished in 0,018 s\n",
      "25/02/02 23:13:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/02 23:13:15 INFO DAGScheduler: running: Set()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: waiting: Set()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: failed: Set()\n",
      "25/02/02 23:13:15 INFO SparkContext: Starting job: count at cmd86.sc:1\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Got job 51 (count at cmd86.sc:1) with 1 output partitions\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Final stage: ResultStage 57 (count at cmd86.sc:1)\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 56)\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[203] at count at cmd86.sc:1), which has no missing parents\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 12.5 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on Air.local:51188 (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[203] at count at cmd86.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 52) (Air.local, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/02/02 23:13:15 INFO Executor: Running task 0.0 in stage 57.0 (TID 52)\n",
      "25/02/02 23:13:15 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/02 23:13:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/02/02 23:13:15 INFO Executor: Finished task 0.0 in stage 57.0 (TID 52). 3995 bytes result sent to driver\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 52) in 7 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:15 INFO DAGScheduler: ResultStage 57 (count at cmd86.sc:1) finished in 0,011 s\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Job 51 finished: count at cmd86.sc:1, took 0,013653 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres86\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2155L\u001b[39m"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderDetails.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:15 INFO BlockManagerInfo: Removed broadcast_79_piece0 on Air.local:51188 in memory (size: 7.6 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Removed broadcast_78_piece0 on Air.local:51188 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Removed broadcast_82_piece0 on Air.local:51188 in memory (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Removed broadcast_81_piece0 on Air.local:51188 in memory (size: 8.7 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/02 23:13:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/02 23:13:15 INFO DriverFactory: Routing driver instance 1359325672 created for server address localhost:7687\n",
      "25/02/02 23:13:15 INFO InternalDriver: Closing driver instance 1359325672\n",
      "25/02/02 23:13:15 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 199.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on Air.local:51188 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 83 from save at cmd87.sc:12\n",
      "25/02/02 23:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/02 23:13:15 INFO DriverFactory: Routing driver instance 902490260 created for server address localhost:7687\n",
      "25/02/02 23:13:15 INFO OverwriteByExpressionExec: Start processing data source write support: org.neo4j.spark.writer.Neo4jBatchWriter@2b6bb50. The input RDD has 1 partitions.\n",
      "25/02/02 23:13:15 INFO SparkContext: Starting job: save at cmd87.sc:12\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Got job 52 (save at cmd87.sc:12) with 1 output partitions\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Final stage: ResultStage 58 (save at cmd87.sc:12)\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[205] at save at cmd87.sc:12), which has no missing parents\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 20.2 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on Air.local:51188 (size: 10.1 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:15 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[205] at save at cmd87.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/02 23:13:15 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
      "25/02/02 23:13:15 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 53) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9634 bytes) \n",
      "25/02/02 23:13:15 INFO Executor: Running task 0.0 in stage 58.0 (TID 53)\n",
      "25/02/02 23:13:15 INFO FileScanRDD: Reading File path: file:///Users/vadim/workspace/Neo4j/desktop-csv-import/order-details.csv, range: 0-27454, partition values: [empty row]\n",
      "25/02/02 23:13:15 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 53, attempt 0, stage 58.0)\n",
      "25/02/02 23:13:15 INFO Neo4jDataWriter: Writing a batch of 2155 elements to Neo4j,\n",
      "for jobId=8105907d-48da-40d3-a339-54e5fd2069fd and partitionId=0\n",
      "with query: UNWIND $events AS event\n",
      "MATCH (source:Product {id: event.source.keys.id})\n",
      "MATCH (target:Order {id: event.target.keys.id})\n",
      "MERGE (source)-[rel:CONTAINS]->(target)\n",
      "SET rel += event.rel.properties\n",
      "\n",
      "\n",
      "25/02/02 23:13:16 INFO DataWritingSparkTask: Committed partition 0 (task 53, attempt 0, stage 58.0)\n",
      "25/02/02 23:13:16 INFO Executor: Finished task 0.0 in stage 58.0 (TID 53). 2438 bytes result sent to driver\n",
      "25/02/02 23:13:16 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 53) in 480 ms on Air.local (executor driver) (1/1)\n",
      "25/02/02 23:13:16 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:16 INFO DAGScheduler: ResultStage 58 (save at cmd87.sc:12) finished in 0,484 s\n",
      "25/02/02 23:13:16 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished\n",
      "25/02/02 23:13:16 INFO DAGScheduler: Job 52 finished: save at cmd87.sc:12, took 0,485885 s\n",
      "25/02/02 23:13:16 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@2b6bb50 is committing.\n",
      "25/02/02 23:13:16 INFO InternalDriver: Closing driver instance 902490260\n",
      "25/02/02 23:13:16 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:16 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:16 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@2b6bb50 committed.\n"
     ]
    }
   ],
   "source": [
    "orderDetails.write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"relationship\", \"CONTAINS\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.source.labels\", \":Product\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.source.node.keys\", \"productID:id\")\n",
    "            .option(\"relationship.target.labels\", \":Order\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.node.keys\", \"orderID:id\")\n",
    "            .option(\"relationship.properties\", \"quantity:quantityOrdered\")\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write custom graphs via Cypher Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mactorOrders\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m], \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m], \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m\"Cuba Gooding Jr.\"\u001b[39m,\n",
       "    \u001b[32m1\u001b[39m,\n",
       "    \u001b[33mArray\u001b[39m(\u001b[32m11\u001b[39m, \u001b[32m42\u001b[39m, \u001b[32m72\u001b[39m),\n",
       "    \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "    \u001b[32m\"2022-06-07 00:00:00\"\u001b[39m\n",
       "  ),\n",
       "  (\u001b[32m\"Tom Hanks\"\u001b[39m, \u001b[32m2\u001b[39m, \u001b[33mArray\u001b[39m(\u001b[32m24\u001b[39m, \u001b[32m55\u001b[39m, \u001b[32m75\u001b[39m), \u001b[33mArray\u001b[39m(\u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m\"2022-06-06 00:00:00\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actorOrders = Seq(\n",
    "  (\"Cuba Gooding Jr.\", 1, Array(11, 42, 72), Array(1, 2, 3), \"2022-06-07 00:00:00\"),\n",
    "  (\"Tom Hanks\", 2, Array(24, 55, 75), Array(3, 2, 1), \"2022-06-06 00:00:00\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:16 INFO BlockManagerInfo: Removed broadcast_84_piece0 on Air.local:51188 in memory (size: 10.1 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mactorOrdersDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [actor_name: string, order_id: int ... 3 more fields]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val actorOrdersDF = spark\n",
    "                    .createDataFrame(actorOrders)\n",
    "                    .withColumnsRenamed(\n",
    "                        Map(\n",
    "                            \"_1\" -> \"actor_name\",\n",
    "                            \"_2\" -> \"order_id\",\n",
    "                            \"_3\" -> \"products\",\n",
    "                            \"_4\" -> \"quantities\",\n",
    "                            \"_5\" -> \"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_name: string (nullable = true)\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- products: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- quantities: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- order_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorOrdersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+------------+----------+-------------------+\n",
      "|actor_name      |order_id|products    |quantities|order_date         |\n",
      "+----------------+--------+------------+----------+-------------------+\n",
      "|Cuba Gooding Jr.|1       |[11, 42, 72]|[1, 2, 3] |2022-06-07 00:00:00|\n",
      "|Tom Hanks       |2       |[24, 55, 75]|[3, 2, 1] |2022-06-06 00:00:00|\n",
      "+----------------+--------+------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actorOrdersDF.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 23:13:17 INFO DriverFactory: Routing driver instance 884824584 created for server address localhost:7687\n",
      "25/02/02 23:13:17 INFO InternalDriver: Closing driver instance 884824584\n",
      "25/02/02 23:13:17 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:17 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:17 INFO DriverFactory: Routing driver instance 117436635 created for server address localhost:7687\n",
      "25/02/02 23:13:17 INFO OverwriteByExpressionExec: Start processing data source write support: org.neo4j.spark.writer.Neo4jBatchWriter@62d4b0ca. The input RDD has 2 partitions.\n",
      "25/02/02 23:13:17 INFO SparkContext: Starting job: save at cmd92.sc:4\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Got job 53 (save at cmd92.sc:4) with 2 output partitions\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Final stage: ResultStage 59 (save at cmd92.sc:4)\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[207] at save at cmd92.sc:4), which has no missing parents\n",
      "25/02/02 23:13:17 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 15.3 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:17 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 2.2 GiB)\n",
      "25/02/02 23:13:17 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on Air.local:51188 (size: 7.4 KiB, free: 2.2 GiB)\n",
      "25/02/02 23:13:17 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[207] at save at cmd92.sc:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/02/02 23:13:17 INFO TaskSchedulerImpl: Adding task set 59.0 with 2 tasks resource profile 0\n",
      "25/02/02 23:13:17 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 54) (Air.local, executor driver, partition 0, PROCESS_LOCAL, 9307 bytes) \n",
      "25/02/02 23:13:17 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 55) (Air.local, executor driver, partition 1, PROCESS_LOCAL, 9307 bytes) \n",
      "25/02/02 23:13:17 INFO Executor: Running task 0.0 in stage 59.0 (TID 54)\n",
      "25/02/02 23:13:17 INFO Executor: Running task 1.0 in stage 59.0 (TID 55)\n",
      "25/02/02 23:13:17 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 55, attempt 0, stage 59.0)\n",
      "25/02/02 23:13:17 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 54, attempt 0, stage 59.0)\n",
      "25/02/02 23:13:17 INFO Neo4jDataWriter: Writing a batch of 1 elements to Neo4j,\n",
      "for jobId=4307f9a3-ef38-4262-983d-2b88030d04a5 and partitionId=1\n",
      "with query: WITH $scriptResult AS scriptResult\n",
      "UNWIND $events AS event\n",
      "MATCH (person:Person {name: event.actor_name})\n",
      "                     MERGE (order:Order {id: event.order_id, date: datetime(replace(event.order_date, ' ', 'T'))})\n",
      "                     MERGE (person)-[:CREATED]->(order)\n",
      "                     WITH event, order\n",
      "                     UNWIND range(0, size(event.products) - 1) AS index\n",
      "                     MATCH (product:Product {id: event.products[index]})\n",
      "                     MERGE (product)-[:CONTAINS{quantityOrdered: event.quantities[index]}]->(order)\n",
      "\n",
      "\n",
      "25/02/02 23:13:17 INFO Neo4jDataWriter: Writing a batch of 1 elements to Neo4j,\n",
      "for jobId=4307f9a3-ef38-4262-983d-2b88030d04a5 and partitionId=0\n",
      "with query: WITH $scriptResult AS scriptResult\n",
      "UNWIND $events AS event\n",
      "MATCH (person:Person {name: event.actor_name})\n",
      "                     MERGE (order:Order {id: event.order_id, date: datetime(replace(event.order_date, ' ', 'T'))})\n",
      "                     MERGE (person)-[:CREATED]->(order)\n",
      "                     WITH event, order\n",
      "                     UNWIND range(0, size(event.products) - 1) AS index\n",
      "                     MATCH (product:Product {id: event.products[index]})\n",
      "                     MERGE (product)-[:CONTAINS{quantityOrdered: event.quantities[index]}]->(order)\n",
      "\n",
      "\n",
      "25/02/02 23:13:17 INFO DataWritingSparkTask: Committed partition 0 (task 54, attempt 0, stage 59.0)\n",
      "25/02/02 23:13:17 INFO DataWritingSparkTask: Committed partition 1 (task 55, attempt 0, stage 59.0)\n",
      "25/02/02 23:13:17 INFO Executor: Finished task 0.0 in stage 59.0 (TID 54). 2352 bytes result sent to driver\n",
      "25/02/02 23:13:17 INFO Executor: Finished task 1.0 in stage 59.0 (TID 55). 2352 bytes result sent to driver\n",
      "25/02/02 23:13:17 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 54) in 164 ms on Air.local (executor driver) (1/2)\n",
      "25/02/02 23:13:17 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 55) in 164 ms on Air.local (executor driver) (2/2)\n",
      "25/02/02 23:13:17 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "25/02/02 23:13:17 INFO DAGScheduler: ResultStage 59 (save at cmd92.sc:4) finished in 0,170 s\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/02 23:13:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished\n",
      "25/02/02 23:13:17 INFO DAGScheduler: Job 53 finished: save at cmd92.sc:4, took 0,172228 s\n",
      "25/02/02 23:13:17 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@62d4b0ca is committing.\n",
      "25/02/02 23:13:17 INFO InternalDriver: Closing driver instance 117436635\n",
      "25/02/02 23:13:17 INFO ConnectionPoolImpl: Closing connection pool towards localhost(127.0.0.1):7687\n",
      "25/02/02 23:13:17 INFO ConnectionPoolImpl: Closing connection pool towards localhost:7687\n",
      "25/02/02 23:13:17 INFO OverwriteByExpressionExec: Data source write support org.neo4j.spark.writer.Neo4jBatchWriter@62d4b0ca committed.\n"
     ]
    }
   ],
   "source": [
    "actorOrdersDF.write\n",
    "             .format(\"org.neo4j.spark.DataSource\")\n",
    "             .mode(\"overwrite\")\n",
    "             .option(\"query\", \"\"\"\n",
    "                     MATCH (person:Person {name: event.actor_name})\n",
    "                     MERGE (order:Order {id: event.order_id, date: datetime(replace(event.order_date, ' ', 'T'))})\n",
    "                     MERGE (person)-[:CREATED]->(order)\n",
    "                     WITH event, order\n",
    "                     UNWIND range(0, size(event.products) - 1) AS index\n",
    "                     MATCH (product:Product {id: event.products[index]})\n",
    "                     MERGE (product)-[:CONTAINS{quantityOrdered: event.quantities[index]}]->(order)\n",
    "                    \"\"\")\n",
    "             .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
